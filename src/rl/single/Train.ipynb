{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from customenv import StockTradingEnv\n",
        "from custommodels import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data_single.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "11\n",
            "Stock Dimension: 1, State Space: 11\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(len(INDICATORS))\n",
        "print(state_space)\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 10,\n",
        "    \"initial_amount\": 100000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    # \"print_verbosity\": 1\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 663           |\n",
            "|    iterations         | 100           |\n",
            "|    time_elapsed       | 0             |\n",
            "|    total_timesteps    | 500           |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.43         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 99            |\n",
            "|    policy_loss        | 0.267         |\n",
            "|    reward             | 1.9528252e-05 |\n",
            "|    std                | 1.01          |\n",
            "|    value_loss         | 1.96e-08      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 616           |\n",
            "|    iterations         | 200           |\n",
            "|    time_elapsed       | 1             |\n",
            "|    total_timesteps    | 1000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.44         |\n",
            "|    explained_variance | 5.96e-08      |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 199           |\n",
            "|    policy_loss        | -0.0354       |\n",
            "|    reward             | -0.0006376806 |\n",
            "|    std                | 1.02          |\n",
            "|    value_loss         | 5.76e-05      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 606         |\n",
            "|    iterations         | 300         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 1500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.45       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 299         |\n",
            "|    policy_loss        | 0.165       |\n",
            "|    reward             | 0.009861949 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.000402    |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 600           |\n",
            "|    iterations         | 400           |\n",
            "|    time_elapsed       | 3             |\n",
            "|    total_timesteps    | 2000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.46         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 399           |\n",
            "|    policy_loss        | 0.0333        |\n",
            "|    reward             | -0.0037799762 |\n",
            "|    std                | 1.04          |\n",
            "|    value_loss         | 0.000192      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 610         |\n",
            "|    iterations         | 500         |\n",
            "|    time_elapsed       | 4           |\n",
            "|    total_timesteps    | 2500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.46       |\n",
            "|    explained_variance | -7.53       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 499         |\n",
            "|    policy_loss        | 0.175       |\n",
            "|    reward             | -0.04936305 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 0.0162      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 614          |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.46        |\n",
            "|    explained_variance | 0.102        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | 0.0291       |\n",
            "|    reward             | 0.0121861715 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 0.00927      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 612          |\n",
            "|    iterations         | 700          |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 3500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.46        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 699          |\n",
            "|    policy_loss        | 0.196        |\n",
            "|    reward             | 8.013031e-06 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 4.66e-06     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 615          |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 6            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.45        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -0.206       |\n",
            "|    reward             | 0.0003179885 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 1.43e-07     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 618           |\n",
            "|    iterations         | 900           |\n",
            "|    time_elapsed       | 7             |\n",
            "|    total_timesteps    | 4500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.44         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 899           |\n",
            "|    policy_loss        | 0.206         |\n",
            "|    reward             | 0.00078448845 |\n",
            "|    std                | 1.03          |\n",
            "|    value_loss         | 8.19e-05      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 620         |\n",
            "|    iterations         | 1000        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 5000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.44       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 999         |\n",
            "|    policy_loss        | 0.0735      |\n",
            "|    reward             | 0.007643367 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.000162    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 623         |\n",
            "|    iterations         | 1100        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 5500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.44       |\n",
            "|    explained_variance | -2.64       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1099        |\n",
            "|    policy_loss        | 0.134       |\n",
            "|    reward             | 0.011614017 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.00209     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 625           |\n",
            "|    iterations         | 1200          |\n",
            "|    time_elapsed       | 9             |\n",
            "|    total_timesteps    | 6000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.45         |\n",
            "|    explained_variance | -0.383        |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1199          |\n",
            "|    policy_loss        | 0.263         |\n",
            "|    reward             | 0.00038218044 |\n",
            "|    std                | 1.03          |\n",
            "|    value_loss         | 0.00338       |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 626        |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.44      |\n",
            "|    explained_variance | -1.62      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 0.114      |\n",
            "|    reward             | 0.04226168 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.00751    |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 627           |\n",
            "|    iterations         | 1400          |\n",
            "|    time_elapsed       | 11            |\n",
            "|    total_timesteps    | 7000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.46         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1399          |\n",
            "|    policy_loss        | 0.0356        |\n",
            "|    reward             | -0.0013736516 |\n",
            "|    std                | 1.04          |\n",
            "|    value_loss         | 1.08e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 629          |\n",
            "|    iterations         | 1500         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 7500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.45        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1499         |\n",
            "|    policy_loss        | 0.0615       |\n",
            "|    reward             | -0.003300422 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 0.000367     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 631          |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.48        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | 0.0415       |\n",
            "|    reward             | 0.0043809297 |\n",
            "|    std                | 1.06         |\n",
            "|    value_loss         | 2.56e-05     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 632          |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.47        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | -0.709       |\n",
            "|    reward             | 0.0019069061 |\n",
            "|    std                | 1.05         |\n",
            "|    value_loss         | 0.000154     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 634           |\n",
            "|    iterations         | 1800          |\n",
            "|    time_elapsed       | 14            |\n",
            "|    total_timesteps    | 9000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1799          |\n",
            "|    policy_loss        | -0.0817       |\n",
            "|    reward             | -0.0010688895 |\n",
            "|    std                | 1.06          |\n",
            "|    value_loss         | 0.000223      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 635         |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | -0.915      |\n",
            "|    reward             | 0.019885449 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.000271    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 634         |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | 0.548       |\n",
            "|    reward             | -0.11484961 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.0226      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 635           |\n",
            "|    iterations         | 2100          |\n",
            "|    time_elapsed       | 16            |\n",
            "|    total_timesteps    | 10500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.49         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2099          |\n",
            "|    policy_loss        | -0.292        |\n",
            "|    reward             | 0.00010051155 |\n",
            "|    std                | 1.07          |\n",
            "|    value_loss         | 1.62e-06      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 636           |\n",
            "|    iterations         | 2200          |\n",
            "|    time_elapsed       | 17            |\n",
            "|    total_timesteps    | 11000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2199          |\n",
            "|    policy_loss        | -0.0103       |\n",
            "|    reward             | 5.0121864e-17 |\n",
            "|    std                | 1.06          |\n",
            "|    value_loss         | 3.81e-11      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 637           |\n",
            "|    iterations         | 2300          |\n",
            "|    time_elapsed       | 18            |\n",
            "|    total_timesteps    | 11500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2299          |\n",
            "|    policy_loss        | -0.0166       |\n",
            "|    reward             | 3.3894285e-05 |\n",
            "|    std                | 1.06          |\n",
            "|    value_loss         | 1.28e-09      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 636           |\n",
            "|    iterations         | 2400          |\n",
            "|    time_elapsed       | 18            |\n",
            "|    total_timesteps    | 12000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.47         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2399          |\n",
            "|    policy_loss        | -0.0427       |\n",
            "|    reward             | 0.00031740454 |\n",
            "|    std                | 1.05          |\n",
            "|    value_loss         | 1.88e-06      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 636            |\n",
            "|    iterations         | 2500           |\n",
            "|    time_elapsed       | 19             |\n",
            "|    total_timesteps    | 12500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.48          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 2499           |\n",
            "|    policy_loss        | -0.455         |\n",
            "|    reward             | -3.7161713e-05 |\n",
            "|    std                | 1.06           |\n",
            "|    value_loss         | 8.11e-08       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 636           |\n",
            "|    iterations         | 2600          |\n",
            "|    time_elapsed       | 20            |\n",
            "|    total_timesteps    | 13000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.48         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2599          |\n",
            "|    policy_loss        | -0.0282       |\n",
            "|    reward             | -0.0129489675 |\n",
            "|    std                | 1.07          |\n",
            "|    value_loss         | 0.00145       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 635         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | -1.01       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | -0.146      |\n",
            "|    reward             | 0.027518682 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.0134      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 635           |\n",
            "|    iterations         | 2800          |\n",
            "|    time_elapsed       | 22            |\n",
            "|    total_timesteps    | 14000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.5          |\n",
            "|    explained_variance | 5.96e-08      |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2799          |\n",
            "|    policy_loss        | -0.0759       |\n",
            "|    reward             | -0.0010071929 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 5.07e-06      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 635            |\n",
            "|    iterations         | 2900           |\n",
            "|    time_elapsed       | 22             |\n",
            "|    total_timesteps    | 14500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.49          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 2899           |\n",
            "|    policy_loss        | -0.351         |\n",
            "|    reward             | -0.00073870126 |\n",
            "|    std                | 1.08           |\n",
            "|    value_loss         | 4.15e-05       |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 636         |\n",
            "|    iterations         | 3000        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 15000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | -9.38       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2999        |\n",
            "|    policy_loss        | -0.0208     |\n",
            "|    reward             | 0.014278559 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.00734     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 635         |\n",
            "|    iterations         | 3100        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 15500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | -0.00103    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3099        |\n",
            "|    policy_loss        | -0.0722     |\n",
            "|    reward             | 0.011526039 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.00731     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 634         |\n",
            "|    iterations         | 3200        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 16000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3199        |\n",
            "|    policy_loss        | 0.141       |\n",
            "|    reward             | 0.080680795 |\n",
            "|    std                | 1.08        |\n",
            "|    value_loss         | 0.000648    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 634        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.5       |\n",
            "|    explained_variance | 0.201      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | -0.0196    |\n",
            "|    reward             | 0.10061582 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 0.00365    |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 634           |\n",
            "|    iterations         | 3400          |\n",
            "|    time_elapsed       | 26            |\n",
            "|    total_timesteps    | 17000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.5          |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 3399          |\n",
            "|    policy_loss        | -0.0046       |\n",
            "|    reward             | 3.0617208e-05 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 3.14e-05      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 635           |\n",
            "|    iterations         | 3500          |\n",
            "|    time_elapsed       | 27            |\n",
            "|    total_timesteps    | 17500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.51         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 3499          |\n",
            "|    policy_loss        | 0.179         |\n",
            "|    reward             | -0.0013904925 |\n",
            "|    std                | 1.09          |\n",
            "|    value_loss         | 1.58e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 635          |\n",
            "|    iterations         | 3600         |\n",
            "|    time_elapsed       | 28           |\n",
            "|    total_timesteps    | 18000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.5         |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3599         |\n",
            "|    policy_loss        | 0.0482       |\n",
            "|    reward             | -0.023977043 |\n",
            "|    std                | 1.09         |\n",
            "|    value_loss         | 0.000429     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 635          |\n",
            "|    iterations         | 3700         |\n",
            "|    time_elapsed       | 29           |\n",
            "|    total_timesteps    | 18500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.5         |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3699         |\n",
            "|    policy_loss        | -0.078       |\n",
            "|    reward             | 0.0017847089 |\n",
            "|    std                | 1.08         |\n",
            "|    value_loss         | 0.00298      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 635           |\n",
            "|    iterations         | 3800          |\n",
            "|    time_elapsed       | 29            |\n",
            "|    total_timesteps    | 19000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.5          |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 3799          |\n",
            "|    policy_loss        | 0.0787        |\n",
            "|    reward             | -0.0020646607 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 5.3e-05       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 635         |\n",
            "|    iterations         | 3900        |\n",
            "|    time_elapsed       | 30          |\n",
            "|    total_timesteps    | 19500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3899        |\n",
            "|    policy_loss        | -0.121      |\n",
            "|    reward             | 0.024723187 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.0436      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 634          |\n",
            "|    iterations         | 4000         |\n",
            "|    time_elapsed       | 31           |\n",
            "|    total_timesteps    | 20000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.49        |\n",
            "|    explained_variance | -0.335       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3999         |\n",
            "|    policy_loss        | 0.0501       |\n",
            "|    reward             | -0.061483044 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 0.0807       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 634           |\n",
            "|    iterations         | 4100          |\n",
            "|    time_elapsed       | 32            |\n",
            "|    total_timesteps    | 20500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.49         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 4099          |\n",
            "|    policy_loss        | -0.104        |\n",
            "|    reward             | -0.0002545076 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 3.19e-06      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 634           |\n",
            "|    iterations         | 4200          |\n",
            "|    time_elapsed       | 33            |\n",
            "|    total_timesteps    | 21000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.49         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 4199          |\n",
            "|    policy_loss        | -0.0895       |\n",
            "|    reward             | -0.0013820294 |\n",
            "|    std                | 1.08          |\n",
            "|    value_loss         | 6.08e-05      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 628         |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 34          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | -0.422      |\n",
            "|    reward             | 0.007532571 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.000297    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 627          |\n",
            "|    iterations         | 4400         |\n",
            "|    time_elapsed       | 35           |\n",
            "|    total_timesteps    | 22000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.49        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4399         |\n",
            "|    policy_loss        | -0.0492      |\n",
            "|    reward             | -0.006126573 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 0.000778     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 626         |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 35          |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | -0.0634     |\n",
            "|    reward             | 0.038044337 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 4.73e-05    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 622         |\n",
            "|    iterations         | 4600        |\n",
            "|    time_elapsed       | 36          |\n",
            "|    total_timesteps    | 23000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.49       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4599        |\n",
            "|    policy_loss        | 0.0834      |\n",
            "|    reward             | 0.015895199 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.0107      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 621        |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 37         |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.49      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | 0.465      |\n",
            "|    reward             | 0.11753231 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.014      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 620          |\n",
            "|    iterations         | 4800         |\n",
            "|    time_elapsed       | 38           |\n",
            "|    total_timesteps    | 24000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.48        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4799         |\n",
            "|    policy_loss        | -0.0647      |\n",
            "|    reward             | 0.0002008715 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 1.24e-05     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 620         |\n",
            "|    iterations         | 4900        |\n",
            "|    time_elapsed       | 39          |\n",
            "|    total_timesteps    | 24500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | -1.28       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4899        |\n",
            "|    policy_loss        | 0.159       |\n",
            "|    reward             | -0.01820563 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 0.000185    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 619          |\n",
            "|    iterations         | 5000         |\n",
            "|    time_elapsed       | 40           |\n",
            "|    total_timesteps    | 25000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.48        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4999         |\n",
            "|    policy_loss        | 0.956        |\n",
            "|    reward             | -0.022199987 |\n",
            "|    std                | 1.07         |\n",
            "|    value_loss         | 0.000872     |\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=25000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 3396, episode: 10\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 1238482.54\n",
            "total_reward: 1138482.54\n",
            "total_cost: 101.04\n",
            "total_trades: 3384\n",
            "Sharpe: 0.863\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 216         |\n",
            "|    time_elapsed    | 62          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -220        |\n",
            "|    critic_loss     | 157         |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 10191       |\n",
            "|    reward          | -0.21202786 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=20000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 881          |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 2            |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | 4.171881e-05 |\n",
            "-------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 855          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063955104 |\n",
            "|    clip_fraction        | 0.074        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0167      |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00164     |\n",
            "|    reward               | 0.0002710044 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.000583     |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 851            |\n",
            "|    iterations           | 3              |\n",
            "|    time_elapsed         | 7              |\n",
            "|    total_timesteps      | 6144           |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0016679068   |\n",
            "|    clip_fraction        | 0.00122        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.44          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0169        |\n",
            "|    n_updates            | 20             |\n",
            "|    policy_gradient_loss | 0.000199       |\n",
            "|    reward               | -0.00011443271 |\n",
            "|    std                  | 1.02           |\n",
            "|    value_loss           | 0.0024         |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 858           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0032540858  |\n",
            "|    clip_fraction        | 0.0118        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.45         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.015        |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000431     |\n",
            "|    reward               | -3.703935e-07 |\n",
            "|    std                  | 1.04          |\n",
            "|    value_loss           | 0.000468      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 847           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029224902 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.46         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00824      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | 0.000185      |\n",
            "|    reward               | -9.787573e-07 |\n",
            "|    std                  | 1.05          |\n",
            "|    value_loss           | 0.00141       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 829           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 14            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0031645293  |\n",
            "|    clip_fraction        | 0.00425       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.47         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.024        |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.000647     |\n",
            "|    reward               | 5.0036843e-17 |\n",
            "|    std                  | 1.05          |\n",
            "|    value_loss           | 0.000771      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 40\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 102351.09\n",
            "total_reward: 2351.09\n",
            "total_cost: 729.74\n",
            "total_trades: 2612\n",
            "Sharpe: 0.340\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 823           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.008532228   |\n",
            "|    clip_fraction        | 0.0822        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0119       |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000869     |\n",
            "|    reward               | 0.00015322663 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 0.000148      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 822           |\n",
            "|    iterations           | 8             |\n",
            "|    time_elapsed         | 19            |\n",
            "|    total_timesteps      | 16384         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0063980557  |\n",
            "|    clip_fraction        | 0.0219        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00147      |\n",
            "|    n_updates            | 70            |\n",
            "|    policy_gradient_loss | -0.000872     |\n",
            "|    reward               | 1.1231314e-05 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 0.000368      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 822           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0041091596  |\n",
            "|    clip_fraction        | 0.017         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0151       |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -0.00108      |\n",
            "|    reward               | 5.0081088e-17 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 6.15e-05      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 823           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.002522597   |\n",
            "|    clip_fraction        | 0.00151       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0137       |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | 0.000134      |\n",
            "|    reward               | 1.3591898e-16 |\n",
            "|    std                  | 1.09          |\n",
            "|    value_loss           | 0.000184      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 821          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071124937 |\n",
            "|    clip_fraction        | 0.0155       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.51        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00869     |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000187    |\n",
            "|    reward               | 0.0001157743 |\n",
            "|    std                  | 1.09         |\n",
            "|    value_loss           | 0.00011      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 818           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 30            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.009113748   |\n",
            "|    clip_fraction        | 0.124         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0358       |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -0.00334      |\n",
            "|    reward               | 5.0013097e-17 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 2.42e-05      |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 818            |\n",
            "|    iterations           | 13             |\n",
            "|    time_elapsed         | 32             |\n",
            "|    total_timesteps      | 26624          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.005874298    |\n",
            "|    clip_fraction        | 0.069          |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.5           |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0129        |\n",
            "|    n_updates            | 120            |\n",
            "|    policy_gradient_loss | -0.000868      |\n",
            "|    reward               | -6.2277572e-06 |\n",
            "|    std                  | 1.08           |\n",
            "|    value_loss           | 7.11e-05       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 817           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0070354315  |\n",
            "|    clip_fraction        | 0.0764        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.027        |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.00252      |\n",
            "|    reward               | -1.040395e-05 |\n",
            "|    std                  | 1.09          |\n",
            "|    value_loss           | 1.46e-05      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 816           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.007427687   |\n",
            "|    clip_fraction        | 0.0824        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0308       |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00328      |\n",
            "|    reward               | -3.897282e-07 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 2.58e-05      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 814           |\n",
            "|    iterations           | 16            |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 32768         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0031473446  |\n",
            "|    clip_fraction        | 0.0242        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0232       |\n",
            "|    n_updates            | 150           |\n",
            "|    policy_gradient_loss | -0.000925     |\n",
            "|    reward               | 1.3587384e-16 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 1.75e-05      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 812           |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 42            |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00930066    |\n",
            "|    clip_fraction        | 0.0868        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0302       |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.00283      |\n",
            "|    reward               | 4.9999498e-17 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 5.2e-06       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 811           |\n",
            "|    iterations           | 18            |\n",
            "|    time_elapsed         | 45            |\n",
            "|    total_timesteps      | 36864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00045722115 |\n",
            "|    clip_fraction        | 0.00166       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0131       |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.000133     |\n",
            "|    reward               | 4.998218e-17  |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 8.48e-06      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 810          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 47           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006652409  |\n",
            "|    clip_fraction        | 0.0213       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.49        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.02        |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.000722    |\n",
            "|    reward               | 4.999931e-17 |\n",
            "|    std                  | 1.08         |\n",
            "|    value_loss           | 1.48e-06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 808           |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 50            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0032706354  |\n",
            "|    clip_fraction        | 0.0477        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0209       |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -0.00146      |\n",
            "|    reward               | 4.9999964e-17 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 3.31e-06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 807           |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 53            |\n",
            "|    total_timesteps      | 43008         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0048475917  |\n",
            "|    clip_fraction        | 0.00845       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.00103       |\n",
            "|    n_updates            | 200           |\n",
            "|    policy_gradient_loss | -0.000211     |\n",
            "|    reward               | 5.0005192e-17 |\n",
            "|    std                  | 1.09          |\n",
            "|    value_loss           | 3.85e-06      |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 808            |\n",
            "|    iterations           | 22             |\n",
            "|    time_elapsed         | 55             |\n",
            "|    total_timesteps      | 45056          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.004753189    |\n",
            "|    clip_fraction        | 0.0794         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.51          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0151        |\n",
            "|    n_updates            | 210            |\n",
            "|    policy_gradient_loss | -0.00129       |\n",
            "|    reward               | -1.5025486e-06 |\n",
            "|    std                  | 1.1            |\n",
            "|    value_loss           | 4.69e-07       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 810           |\n",
            "|    iterations           | 23            |\n",
            "|    time_elapsed         | 58            |\n",
            "|    total_timesteps      | 47104         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00656844    |\n",
            "|    clip_fraction        | 0.0437        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.000532      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -0.00149      |\n",
            "|    reward               | 4.9964995e-17 |\n",
            "|    std                  | 1.08          |\n",
            "|    value_loss           | 1.06e-06      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 50\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 99765.60\n",
            "total_reward: -234.40\n",
            "total_cost: 132.54\n",
            "total_trades: 519\n",
            "Sharpe: -0.420\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 811           |\n",
            "|    iterations           | 24            |\n",
            "|    time_elapsed         | 60            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0007420202  |\n",
            "|    clip_fraction        | 0.0181        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0107       |\n",
            "|    n_updates            | 230           |\n",
            "|    policy_gradient_loss | -5.66e-05     |\n",
            "|    reward               | 5.0002337e-17 |\n",
            "|    std                  | 1.09          |\n",
            "|    value_loss           | 5.58e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 810           |\n",
            "|    iterations           | 25            |\n",
            "|    time_elapsed         | 63            |\n",
            "|    total_timesteps      | 51200         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0061792918  |\n",
            "|    clip_fraction        | 0.0637        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.5          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0189       |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | -0.00278      |\n",
            "|    reward               | 5.0001288e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 6.08e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 809           |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 65            |\n",
            "|    total_timesteps      | 53248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0023260992  |\n",
            "|    clip_fraction        | 0.00537       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0136       |\n",
            "|    n_updates            | 250           |\n",
            "|    policy_gradient_loss | -0.000265     |\n",
            "|    reward               | 4.9990452e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 4.39e-07      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 810          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 68           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.007255613  |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0136      |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00115     |\n",
            "|    reward               | 4.999962e-17 |\n",
            "|    std                  | 1.07         |\n",
            "|    value_loss           | 6.72e-08     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 812           |\n",
            "|    iterations           | 28            |\n",
            "|    time_elapsed         | 70            |\n",
            "|    total_timesteps      | 57344         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0019588885  |\n",
            "|    clip_fraction        | 0.0061        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0219       |\n",
            "|    n_updates            | 270           |\n",
            "|    policy_gradient_loss | -0.000214     |\n",
            "|    reward               | 5.0041363e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 3.55e-06      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 812           |\n",
            "|    iterations           | 29            |\n",
            "|    time_elapsed         | 73            |\n",
            "|    total_timesteps      | 59392         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0033808132  |\n",
            "|    clip_fraction        | 0.0147        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0222       |\n",
            "|    n_updates            | 280           |\n",
            "|    policy_gradient_loss | -0.00147      |\n",
            "|    reward               | 2.9768074e-05 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 7.67e-07      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 813          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 75           |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007998253 |\n",
            "|    clip_fraction        | 0.0128       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0173      |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | 1.9e-05      |\n",
            "|    reward               | 4.999825e-17 |\n",
            "|    std                  | 1.06         |\n",
            "|    value_loss           | 1.49e-07     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 815          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018079998 |\n",
            "|    clip_fraction        | 0.0116       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0244      |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00047     |\n",
            "|    reward               | 4.997365e-17 |\n",
            "|    std                  | 1.07         |\n",
            "|    value_loss           | 2.22e-06     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 818          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014983732 |\n",
            "|    clip_fraction        | 0.0164       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0139      |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.000763    |\n",
            "|    reward               | 4.999252e-17 |\n",
            "|    std                  | 1.06         |\n",
            "|    value_loss           | 1.12e-06     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 821           |\n",
            "|    iterations           | 33            |\n",
            "|    time_elapsed         | 82            |\n",
            "|    total_timesteps      | 67584         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0028543693  |\n",
            "|    clip_fraction        | 0.00352       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0207       |\n",
            "|    n_updates            | 320           |\n",
            "|    policy_gradient_loss | 0.00016       |\n",
            "|    reward               | 4.9943875e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 1.93e-05      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 824           |\n",
            "|    iterations           | 34            |\n",
            "|    time_elapsed         | 84            |\n",
            "|    total_timesteps      | 69632         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0018869939  |\n",
            "|    clip_fraction        | 0.0083        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0113       |\n",
            "|    n_updates            | 330           |\n",
            "|    policy_gradient_loss | -0.000542     |\n",
            "|    reward               | 4.9998985e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 7.78e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 825           |\n",
            "|    iterations           | 35            |\n",
            "|    time_elapsed         | 86            |\n",
            "|    total_timesteps      | 71680         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0033055786  |\n",
            "|    clip_fraction        | 0.002         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0258       |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | 7.47e-05      |\n",
            "|    reward               | 5.0001566e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 5.08e-08      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 827          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 89           |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004909822  |\n",
            "|    clip_fraction        | 0.0106       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.49        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0187      |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.000162    |\n",
            "|    reward               | 4.998993e-17 |\n",
            "|    std                  | 1.08         |\n",
            "|    value_loss           | 1.4e-05      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 828           |\n",
            "|    iterations           | 37            |\n",
            "|    time_elapsed         | 91            |\n",
            "|    total_timesteps      | 75776         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0065279603  |\n",
            "|    clip_fraction        | 0.0312        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.011        |\n",
            "|    n_updates            | 360           |\n",
            "|    policy_gradient_loss | -0.00267      |\n",
            "|    reward               | 4.9990935e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 4.2e-05       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 831           |\n",
            "|    iterations           | 38            |\n",
            "|    time_elapsed         | 93            |\n",
            "|    total_timesteps      | 77824         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0016856052  |\n",
            "|    clip_fraction        | 0.0152        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0203       |\n",
            "|    n_updates            | 370           |\n",
            "|    policy_gradient_loss | -0.00093      |\n",
            "|    reward               | 5.0020075e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 4.61e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 833           |\n",
            "|    iterations           | 39            |\n",
            "|    time_elapsed         | 95            |\n",
            "|    total_timesteps      | 79872         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0047080633  |\n",
            "|    clip_fraction        | 0.0282        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0236       |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -0.00244      |\n",
            "|    reward               | 4.9989638e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 7.78e-08      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 60\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 100093.73\n",
            "total_reward: 93.73\n",
            "total_cost: 91.70\n",
            "total_trades: 419\n",
            "Sharpe: 0.228\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 835          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 98           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024653054 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0156      |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.0011      |\n",
            "|    reward               | 5.000003e-17 |\n",
            "|    std                  | 1.06         |\n",
            "|    value_loss           | 7.06e-09     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 837           |\n",
            "|    iterations           | 41            |\n",
            "|    time_elapsed         | 100           |\n",
            "|    total_timesteps      | 83968         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0032225852  |\n",
            "|    clip_fraction        | 0.025         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0207       |\n",
            "|    n_updates            | 400           |\n",
            "|    policy_gradient_loss | -0.00085      |\n",
            "|    reward               | 5.0006575e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 1.01e-07      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 838          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 102          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017457822 |\n",
            "|    clip_fraction        | 0.0085       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00632     |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.000176    |\n",
            "|    reward               | 2.71422e-06  |\n",
            "|    std                  | 1.07         |\n",
            "|    value_loss           | 3.36e-07     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 840           |\n",
            "|    iterations           | 43            |\n",
            "|    time_elapsed         | 104           |\n",
            "|    total_timesteps      | 88064         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081139756 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0158       |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | 0.000329      |\n",
            "|    reward               | 4.9966976e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 0.000149      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 841           |\n",
            "|    iterations           | 44            |\n",
            "|    time_elapsed         | 107           |\n",
            "|    total_timesteps      | 90112         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.005218884   |\n",
            "|    clip_fraction        | 0.0354        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.49         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0229       |\n",
            "|    n_updates            | 430           |\n",
            "|    policy_gradient_loss | -0.000574     |\n",
            "|    reward               | 5.0000087e-17 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 2.15e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 843           |\n",
            "|    iterations           | 45            |\n",
            "|    time_elapsed         | 109           |\n",
            "|    total_timesteps      | 92160         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.005142438   |\n",
            "|    clip_fraction        | 0.0361        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0211       |\n",
            "|    n_updates            | 440           |\n",
            "|    policy_gradient_loss | -0.00188      |\n",
            "|    reward               | 4.9998806e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 9.53e-09      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 845           |\n",
            "|    iterations           | 46            |\n",
            "|    time_elapsed         | 111           |\n",
            "|    total_timesteps      | 94208         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.44051e-05   |\n",
            "|    clip_fraction        | 0.000928      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0143       |\n",
            "|    n_updates            | 450           |\n",
            "|    policy_gradient_loss | -0.0002       |\n",
            "|    reward               | 4.9985955e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 1.63e-07      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 847           |\n",
            "|    iterations           | 47            |\n",
            "|    time_elapsed         | 113           |\n",
            "|    total_timesteps      | 96256         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00026965677 |\n",
            "|    clip_fraction        | 0.000635      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.018        |\n",
            "|    n_updates            | 460           |\n",
            "|    policy_gradient_loss | 0.000336      |\n",
            "|    reward               | 5.0003002e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 9.23e-09      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 848           |\n",
            "|    iterations           | 48            |\n",
            "|    time_elapsed         | 115           |\n",
            "|    total_timesteps      | 98304         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.004886898   |\n",
            "|    clip_fraction        | 0.014         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00765      |\n",
            "|    n_updates            | 470           |\n",
            "|    policy_gradient_loss | -8.7e-05      |\n",
            "|    reward               | 5.0037694e-17 |\n",
            "|    std                  | 1.06          |\n",
            "|    value_loss           | 6.63e-09      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 849          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018125561 |\n",
            "|    clip_fraction        | 0.00654      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0254      |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    reward               | 5.000002e-17 |\n",
            "|    std                  | 1.07         |\n",
            "|    value_loss           | 1.5e-09      |\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=100000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 205      |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 13588    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 921      |\n",
            "|    critic_loss     | 6.17e+03 |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 10191    |\n",
            "|    reward          | 5e-17    |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=20000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 139         |\n",
            "|    time_elapsed    | 97          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 72.1        |\n",
            "|    critic_loss     | 3.98        |\n",
            "|    ent_coef        | 0.339       |\n",
            "|    ent_coef_loss   | 10.1        |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.21202786 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=20000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
