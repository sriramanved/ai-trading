{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data_single.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 1, State Space: 11\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 10,\n",
        "    \"initial_amount\": 10000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1724         |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 0            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.55        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -0.000771    |\n",
            "|    reward             | 0.0004179001 |\n",
            "|    std                | 1.14         |\n",
            "|    value_loss         | 9.34e-07     |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1695           |\n",
            "|    iterations         | 200            |\n",
            "|    time_elapsed       | 0              |\n",
            "|    total_timesteps    | 1000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.63          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 199            |\n",
            "|    policy_loss        | 0.000126       |\n",
            "|    reward             | -0.00015342745 |\n",
            "|    std                | 1.24           |\n",
            "|    value_loss         | 6.2e-09        |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1609      |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.7      |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -0.000591 |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 1.33      |\n",
            "|    value_loss         | 1.81e-07  |\n",
            "-------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1654           |\n",
            "|    iterations         | 400            |\n",
            "|    time_elapsed       | 1              |\n",
            "|    total_timesteps    | 2000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.77          |\n",
            "|    explained_variance | 5.96e-08       |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 399            |\n",
            "|    policy_loss        | 0.000332       |\n",
            "|    reward             | -0.00041308065 |\n",
            "|    std                | 1.43           |\n",
            "|    value_loss         | 1.6e-07        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1674           |\n",
            "|    iterations         | 500            |\n",
            "|    time_elapsed       | 1              |\n",
            "|    total_timesteps    | 2500           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.84          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 499            |\n",
            "|    policy_loss        | 0.00186        |\n",
            "|    reward             | -1.6748445e-05 |\n",
            "|    std                | 1.53           |\n",
            "|    value_loss         | 1.8e-06        |\n",
            "------------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1693      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.91     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -0.00405  |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 1.64      |\n",
            "|    value_loss         | 5.37e-06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1706      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 2         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.98     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -6.47e-05 |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 1.75      |\n",
            "|    value_loss         | 2.08e-09  |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1718         |\n",
            "|    iterations         | 800          |\n",
            "|    time_elapsed       | 2            |\n",
            "|    total_timesteps    | 4000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.05        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 799          |\n",
            "|    policy_loss        | -8.88e-06    |\n",
            "|    reward             | 6.436687e-05 |\n",
            "|    std                | 1.88         |\n",
            "|    value_loss         | 3e-11        |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1729           |\n",
            "|    iterations         | 900            |\n",
            "|    time_elapsed       | 2              |\n",
            "|    total_timesteps    | 4500           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -2.12          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 899            |\n",
            "|    policy_loss        | -0.00253       |\n",
            "|    reward             | -1.3578929e-05 |\n",
            "|    std                | 2.01           |\n",
            "|    value_loss         | 1.36e-06       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1736          |\n",
            "|    iterations         | 1000          |\n",
            "|    time_elapsed       | 2             |\n",
            "|    total_timesteps    | 5000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.19         |\n",
            "|    explained_variance | 1.19e-07      |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 999           |\n",
            "|    policy_loss        | 0.00139       |\n",
            "|    reward             | 0.00070687523 |\n",
            "|    std                | 2.16          |\n",
            "|    value_loss         | 2.46e-07      |\n",
            "-----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1743      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 3         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.26     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -0.000178 |\n",
            "|    reward             | 0.0       |\n",
            "|    std                | 2.31      |\n",
            "|    value_loss         | 6.42e-09  |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1749         |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.32        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | 0.00173      |\n",
            "|    reward             | -1.10292e-05 |\n",
            "|    std                | 2.47         |\n",
            "|    value_loss         | 4.22e-06     |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1751     |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.39    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | 0.00795  |\n",
            "|    reward             | 0.0      |\n",
            "|    std                | 2.63     |\n",
            "|    value_loss         | 9.3e-06  |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1749          |\n",
            "|    iterations         | 1400          |\n",
            "|    time_elapsed       | 4             |\n",
            "|    total_timesteps    | 7000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.43         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1399          |\n",
            "|    policy_loss        | -0.000157     |\n",
            "|    reward             | -0.0002669436 |\n",
            "|    std                | 2.75          |\n",
            "|    value_loss         | 9.28e-09      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1745          |\n",
            "|    iterations         | 1500          |\n",
            "|    time_elapsed       | 4             |\n",
            "|    total_timesteps    | 7500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.48         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1499          |\n",
            "|    policy_loss        | 0.00467       |\n",
            "|    reward             | -0.0003122883 |\n",
            "|    std                | 2.9           |\n",
            "|    value_loss         | 7.67e-06      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1747         |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.55        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | 0.00162      |\n",
            "|    reward             | 0.0005534784 |\n",
            "|    std                | 3.09         |\n",
            "|    value_loss         | 7.33e-07     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1746          |\n",
            "|    iterations         | 1700          |\n",
            "|    time_elapsed       | 4             |\n",
            "|    total_timesteps    | 8500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.61         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1699          |\n",
            "|    policy_loss        | 0.0146        |\n",
            "|    reward             | 0.00014246743 |\n",
            "|    std                | 3.3           |\n",
            "|    value_loss         | 3.39e-05      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1746           |\n",
            "|    iterations         | 1800           |\n",
            "|    time_elapsed       | 5              |\n",
            "|    total_timesteps    | 9000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -2.68          |\n",
            "|    explained_variance | 1.19e-07       |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 1799           |\n",
            "|    policy_loss        | 0.0108         |\n",
            "|    reward             | -0.00014239529 |\n",
            "|    std                | 3.53           |\n",
            "|    value_loss         | 1.82e-05       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1746          |\n",
            "|    iterations         | 1900          |\n",
            "|    time_elapsed       | 5             |\n",
            "|    total_timesteps    | 9500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.74         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1899          |\n",
            "|    policy_loss        | -0.00101      |\n",
            "|    reward             | 0.00057842105 |\n",
            "|    std                | 3.76          |\n",
            "|    value_loss         | 2.62e-07      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1746         |\n",
            "|    iterations         | 2000         |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 10000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.8         |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1999         |\n",
            "|    policy_loss        | -0.029       |\n",
            "|    reward             | -0.010378107 |\n",
            "|    std                | 3.99         |\n",
            "|    value_loss         | 0.000135     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1744         |\n",
            "|    iterations         | 2100         |\n",
            "|    time_elapsed       | 6            |\n",
            "|    total_timesteps    | 10500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.85        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2099         |\n",
            "|    policy_loss        | -0.00675     |\n",
            "|    reward             | 0.0031029044 |\n",
            "|    std                | 4.18         |\n",
            "|    value_loss         | 8.44e-06     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1744        |\n",
            "|    iterations         | 2200        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 11000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.91       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2199        |\n",
            "|    policy_loss        | 0.108       |\n",
            "|    reward             | 0.008783431 |\n",
            "|    std                | 4.44        |\n",
            "|    value_loss         | 0.00181     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1738        |\n",
            "|    iterations         | 2300        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 11500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.92       |\n",
            "|    explained_variance | -1.7e+03    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2299        |\n",
            "|    policy_loss        | 0.0929      |\n",
            "|    reward             | 0.012884776 |\n",
            "|    std                | 4.49        |\n",
            "|    value_loss         | 0.00821     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1739        |\n",
            "|    iterations         | 2400        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 12000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.93       |\n",
            "|    explained_variance | -12.7       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2399        |\n",
            "|    policy_loss        | -0.0491     |\n",
            "|    reward             | 0.020279128 |\n",
            "|    std                | 4.54        |\n",
            "|    value_loss         | 0.017       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1741         |\n",
            "|    iterations         | 2500         |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 12500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.94        |\n",
            "|    explained_variance | 0.373        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2499         |\n",
            "|    policy_loss        | 0.0854       |\n",
            "|    reward             | -0.032039624 |\n",
            "|    std                | 4.59         |\n",
            "|    value_loss         | 0.00276      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1742        |\n",
            "|    iterations         | 2600        |\n",
            "|    time_elapsed       | 7           |\n",
            "|    total_timesteps    | 13000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.95       |\n",
            "|    explained_variance | -1.21       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2599        |\n",
            "|    policy_loss        | -2.5        |\n",
            "|    reward             | -0.18518578 |\n",
            "|    std                | 4.63        |\n",
            "|    value_loss         | 0.688       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1741       |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.94      |\n",
            "|    explained_variance | 0.528      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | -0.0111    |\n",
            "|    reward             | 0.21023509 |\n",
            "|    std                | 4.59       |\n",
            "|    value_loss         | 0.0265     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1741         |\n",
            "|    iterations         | 2800         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 14000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.96        |\n",
            "|    explained_variance | 0.574        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2799         |\n",
            "|    policy_loss        | 0.0388       |\n",
            "|    reward             | -0.020871716 |\n",
            "|    std                | 4.65         |\n",
            "|    value_loss         | 0.000785     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1740          |\n",
            "|    iterations         | 2900          |\n",
            "|    time_elapsed       | 8             |\n",
            "|    total_timesteps    | 14500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.97         |\n",
            "|    explained_variance | -1.39         |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2899          |\n",
            "|    policy_loss        | -0.00809      |\n",
            "|    reward             | -0.0045939344 |\n",
            "|    std                | 4.7           |\n",
            "|    value_loss         | 0.00303       |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1740       |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 8          |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.98      |\n",
            "|    explained_variance | 0.352      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | -0.183     |\n",
            "|    reward             | 0.04819385 |\n",
            "|    std                | 4.76       |\n",
            "|    value_loss         | 0.00476    |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1741        |\n",
            "|    iterations         | 3100        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 15500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.99       |\n",
            "|    explained_variance | 0.0648      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3099        |\n",
            "|    policy_loss        | 0.616       |\n",
            "|    reward             | 0.034911096 |\n",
            "|    std                | 4.8         |\n",
            "|    value_loss         | 0.0574      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1742       |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3         |\n",
            "|    explained_variance | 0.75       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | 0.0233     |\n",
            "|    reward             | 0.23809813 |\n",
            "|    std                | 4.84       |\n",
            "|    value_loss         | 0.000669   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1742       |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3         |\n",
            "|    explained_variance | 0.0534     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 0.104      |\n",
            "|    reward             | 0.29043928 |\n",
            "|    std                | 4.87       |\n",
            "|    value_loss         | 0.0168     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1743         |\n",
            "|    iterations         | 3400         |\n",
            "|    time_elapsed       | 9            |\n",
            "|    total_timesteps    | 17000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.01        |\n",
            "|    explained_variance | 0.000656     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3399         |\n",
            "|    policy_loss        | -0.0724      |\n",
            "|    reward             | 0.0002140583 |\n",
            "|    std                | 4.92         |\n",
            "|    value_loss         | 0.000734     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1744         |\n",
            "|    iterations         | 3500         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 17500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.03        |\n",
            "|    explained_variance | 0.26         |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3499         |\n",
            "|    policy_loss        | 0.0225       |\n",
            "|    reward             | -0.009581649 |\n",
            "|    std                | 5            |\n",
            "|    value_loss         | 0.000209     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1744         |\n",
            "|    iterations         | 3600         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 18000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.05        |\n",
            "|    explained_variance | 0.11         |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3599         |\n",
            "|    policy_loss        | -0.145       |\n",
            "|    reward             | -0.085775405 |\n",
            "|    std                | 5.1          |\n",
            "|    value_loss         | 0.00508      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1743        |\n",
            "|    iterations         | 3700        |\n",
            "|    time_elapsed       | 10          |\n",
            "|    total_timesteps    | 18500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.07       |\n",
            "|    explained_variance | 0.379       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3699        |\n",
            "|    policy_loss        | -0.335      |\n",
            "|    reward             | 0.004723591 |\n",
            "|    std                | 5.23        |\n",
            "|    value_loss         | 0.0157      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1743         |\n",
            "|    iterations         | 3800         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 19000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.1         |\n",
            "|    explained_variance | 0.225        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3799         |\n",
            "|    policy_loss        | 0.0929       |\n",
            "|    reward             | -0.005428285 |\n",
            "|    std                | 5.37         |\n",
            "|    value_loss         | 0.000927     |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1745      |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.11     |\n",
            "|    explained_variance | 0.0703    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -1.49     |\n",
            "|    reward             | 0.0638681 |\n",
            "|    std                | 5.42      |\n",
            "|    value_loss         | 0.262     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1746        |\n",
            "|    iterations         | 4000        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 20000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.11       |\n",
            "|    explained_variance | 0.118       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3999        |\n",
            "|    policy_loss        | 2.04        |\n",
            "|    reward             | -0.16129856 |\n",
            "|    std                | 5.44        |\n",
            "|    value_loss         | 0.559       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1747         |\n",
            "|    iterations         | 4100         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 20500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.11        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4099         |\n",
            "|    policy_loss        | -0.0356      |\n",
            "|    reward             | -0.008730912 |\n",
            "|    std                | 5.45         |\n",
            "|    value_loss         | 0.000211     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1749         |\n",
            "|    iterations         | 4200         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 21000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.13        |\n",
            "|    explained_variance | -0.724       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4199         |\n",
            "|    policy_loss        | -0.135       |\n",
            "|    reward             | -0.009211381 |\n",
            "|    std                | 5.52         |\n",
            "|    value_loss         | 0.00428      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1750        |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.15       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | -0.231      |\n",
            "|    reward             | 0.021921871 |\n",
            "|    std                | 5.65        |\n",
            "|    value_loss         | 0.00482     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1751         |\n",
            "|    iterations         | 4400         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 22000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.18        |\n",
            "|    explained_variance | 0.306        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4399         |\n",
            "|    policy_loss        | -0.0342      |\n",
            "|    reward             | -0.016272524 |\n",
            "|    std                | 5.81         |\n",
            "|    value_loss         | 0.00119      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1751       |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.19      |\n",
            "|    explained_variance | -1.15      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | 0.0142     |\n",
            "|    reward             | 0.09991428 |\n",
            "|    std                | 5.9        |\n",
            "|    value_loss         | 0.000738   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1750        |\n",
            "|    iterations         | 4600        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 23000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.19       |\n",
            "|    explained_variance | 0.151       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4599        |\n",
            "|    policy_loss        | -0.803      |\n",
            "|    reward             | 0.041721143 |\n",
            "|    std                | 5.9         |\n",
            "|    value_loss         | 0.0817      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1750       |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 13         |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.2       |\n",
            "|    explained_variance | -0.033     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | 0.268      |\n",
            "|    reward             | 0.30568978 |\n",
            "|    std                | 5.95       |\n",
            "|    value_loss         | 0.111      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1750         |\n",
            "|    iterations         | 4800         |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 24000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.2         |\n",
            "|    explained_variance | -14.4        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4799         |\n",
            "|    policy_loss        | 0.00563      |\n",
            "|    reward             | 0.0013629922 |\n",
            "|    std                | 5.96         |\n",
            "|    value_loss         | 0.00142      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1751         |\n",
            "|    iterations         | 4900         |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 24500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.22        |\n",
            "|    explained_variance | -1.8         |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4899         |\n",
            "|    policy_loss        | -0.0916      |\n",
            "|    reward             | -0.056284573 |\n",
            "|    std                | 6.05         |\n",
            "|    value_loss         | 0.00329      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1752        |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.23       |\n",
            "|    explained_variance | -0.266      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | -0.115      |\n",
            "|    reward             | -0.04332755 |\n",
            "|    std                | 6.14        |\n",
            "|    value_loss         | 0.00268     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1753        |\n",
            "|    iterations         | 5100        |\n",
            "|    time_elapsed       | 14          |\n",
            "|    total_timesteps    | 25500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.24       |\n",
            "|    explained_variance | -0.0123     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5099        |\n",
            "|    policy_loss        | -0.138      |\n",
            "|    reward             | 0.052890535 |\n",
            "|    std                | 6.16        |\n",
            "|    value_loss         | 0.00727     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1746       |\n",
            "|    iterations         | 5200       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 26000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.24      |\n",
            "|    explained_variance | 0.0246     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5199       |\n",
            "|    policy_loss        | -0.323     |\n",
            "|    reward             | 0.22021832 |\n",
            "|    std                | 6.2        |\n",
            "|    value_loss         | 0.0181     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1741        |\n",
            "|    iterations         | 5300        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 26500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.24       |\n",
            "|    explained_variance | 0.101       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5299        |\n",
            "|    policy_loss        | 0.00801     |\n",
            "|    reward             | 0.016471146 |\n",
            "|    std                | 6.17        |\n",
            "|    value_loss         | 0.00644     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1741       |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 15         |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.26      |\n",
            "|    explained_variance | -0.245     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -0.934     |\n",
            "|    reward             | 0.36855537 |\n",
            "|    std                | 6.28       |\n",
            "|    value_loss         | 0.062      |\n",
            "--------------------------------------\n",
            "day: 3396, episode: 90\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 229814.36\n",
            "total_reward: 219814.36\n",
            "total_cost: 1298.95\n",
            "total_trades: 3362\n",
            "Sharpe: 1.023\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1742          |\n",
            "|    iterations         | 5500          |\n",
            "|    time_elapsed       | 15            |\n",
            "|    total_timesteps    | 27500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.27         |\n",
            "|    explained_variance | 0.0793        |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 5499          |\n",
            "|    policy_loss        | -0.109        |\n",
            "|    reward             | -0.0017889463 |\n",
            "|    std                | 6.34          |\n",
            "|    value_loss         | 0.00123       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1742         |\n",
            "|    iterations         | 5600         |\n",
            "|    time_elapsed       | 16           |\n",
            "|    total_timesteps    | 28000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.27        |\n",
            "|    explained_variance | 0.229        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5599         |\n",
            "|    policy_loss        | 0.176        |\n",
            "|    reward             | -0.011959119 |\n",
            "|    std                | 6.36         |\n",
            "|    value_loss         | 0.00451      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1743           |\n",
            "|    iterations         | 5700           |\n",
            "|    time_elapsed       | 16             |\n",
            "|    total_timesteps    | 28500          |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -3.29          |\n",
            "|    explained_variance | -0.151         |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 5699           |\n",
            "|    policy_loss        | 0.0161         |\n",
            "|    reward             | -0.00017476184 |\n",
            "|    std                | 6.47           |\n",
            "|    value_loss         | 0.000449       |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1744        |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 16          |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.29       |\n",
            "|    explained_variance | 0.357       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -0.0995     |\n",
            "|    reward             | 0.041992642 |\n",
            "|    std                | 6.51        |\n",
            "|    value_loss         | 0.00102     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1746        |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 16          |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.3        |\n",
            "|    explained_variance | -0.492      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | -0.702      |\n",
            "|    reward             | -0.07200021 |\n",
            "|    std                | 6.54        |\n",
            "|    value_loss         | 0.0501      |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1747      |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.29     |\n",
            "|    explained_variance | 0.195     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 0.0785    |\n",
            "|    reward             | 0.8577976 |\n",
            "|    std                | 6.5       |\n",
            "|    value_loss         | 0.346     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1748       |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.29      |\n",
            "|    explained_variance | 0.00591    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -2.11      |\n",
            "|    reward             | 0.16002727 |\n",
            "|    std                | 6.48       |\n",
            "|    value_loss         | 0.563      |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1746         |\n",
            "|    iterations         | 6200         |\n",
            "|    time_elapsed       | 17           |\n",
            "|    total_timesteps    | 31000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.29        |\n",
            "|    explained_variance | 0.145        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 6199         |\n",
            "|    policy_loss        | 0.0797       |\n",
            "|    reward             | 0.0044632996 |\n",
            "|    std                | 6.53         |\n",
            "|    value_loss         | 0.00119      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1747        |\n",
            "|    iterations         | 6300        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 31500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.31       |\n",
            "|    explained_variance | -1.43e-06   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6299        |\n",
            "|    policy_loss        | 0.0168      |\n",
            "|    reward             | 0.027564323 |\n",
            "|    std                | 6.65        |\n",
            "|    value_loss         | 0.000498    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1747         |\n",
            "|    iterations         | 6400         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 32000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.33        |\n",
            "|    explained_variance | 0.196        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 6399         |\n",
            "|    policy_loss        | 0.0801       |\n",
            "|    reward             | -0.027485523 |\n",
            "|    std                | 6.73         |\n",
            "|    value_loss         | 0.000891     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1748          |\n",
            "|    iterations         | 6500          |\n",
            "|    time_elapsed       | 18            |\n",
            "|    total_timesteps    | 32500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.34         |\n",
            "|    explained_variance | -5.88         |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 6499          |\n",
            "|    policy_loss        | -0.101        |\n",
            "|    reward             | -0.0043355925 |\n",
            "|    std                | 6.81          |\n",
            "|    value_loss         | 0.00141       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1749        |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 18          |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.35       |\n",
            "|    explained_variance | -0.102      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | 0.356       |\n",
            "|    reward             | 0.016478872 |\n",
            "|    std                | 6.87        |\n",
            "|    value_loss         | 0.0131      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1750        |\n",
            "|    iterations         | 6700        |\n",
            "|    time_elapsed       | 19          |\n",
            "|    total_timesteps    | 33500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.35       |\n",
            "|    explained_variance | 0.06        |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6699        |\n",
            "|    policy_loss        | 0.386       |\n",
            "|    reward             | 0.022075258 |\n",
            "|    std                | 6.86        |\n",
            "|    value_loss         | 0.0801      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1751          |\n",
            "|    iterations         | 6800          |\n",
            "|    time_elapsed       | 19            |\n",
            "|    total_timesteps    | 34000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.36         |\n",
            "|    explained_variance | 1.19e-07      |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 6799          |\n",
            "|    policy_loss        | 0.0123        |\n",
            "|    reward             | -0.0023408884 |\n",
            "|    std                | 6.95          |\n",
            "|    value_loss         | 1.68e-05      |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1750       |\n",
            "|    iterations         | 6900       |\n",
            "|    time_elapsed       | 19         |\n",
            "|    total_timesteps    | 34500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.37      |\n",
            "|    explained_variance | -0.0984    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6899       |\n",
            "|    policy_loss        | -0.0301    |\n",
            "|    reward             | 0.01731562 |\n",
            "|    std                | 7.05       |\n",
            "|    value_loss         | 0.000614   |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1750          |\n",
            "|    iterations         | 7000          |\n",
            "|    time_elapsed       | 19            |\n",
            "|    total_timesteps    | 35000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.39         |\n",
            "|    explained_variance | 0.172         |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 6999          |\n",
            "|    policy_loss        | -0.0507       |\n",
            "|    reward             | -0.0028671543 |\n",
            "|    std                | 7.18          |\n",
            "|    value_loss         | 0.0214        |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1751       |\n",
            "|    iterations         | 7100       |\n",
            "|    time_elapsed       | 20         |\n",
            "|    total_timesteps    | 35500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.41      |\n",
            "|    explained_variance | 0.26       |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7099       |\n",
            "|    policy_loss        | 0.423      |\n",
            "|    reward             | 0.06315258 |\n",
            "|    std                | 7.32       |\n",
            "|    value_loss         | 0.0205     |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1752         |\n",
            "|    iterations         | 7200         |\n",
            "|    time_elapsed       | 20           |\n",
            "|    total_timesteps    | 36000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.43        |\n",
            "|    explained_variance | -0.472       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 7199         |\n",
            "|    policy_loss        | -0.0358      |\n",
            "|    reward             | 0.0026152187 |\n",
            "|    std                | 7.44         |\n",
            "|    value_loss         | 0.000483     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1752        |\n",
            "|    iterations         | 7300        |\n",
            "|    time_elapsed       | 20          |\n",
            "|    total_timesteps    | 36500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.44       |\n",
            "|    explained_variance | -0.336      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7299        |\n",
            "|    policy_loss        | 0.0598      |\n",
            "|    reward             | 0.028415343 |\n",
            "|    std                | 7.56        |\n",
            "|    value_loss         | 0.00696     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1753        |\n",
            "|    iterations         | 7400        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 37000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.45       |\n",
            "|    explained_variance | 0.238       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7399        |\n",
            "|    policy_loss        | 1.05        |\n",
            "|    reward             | -0.33621126 |\n",
            "|    std                | 7.58        |\n",
            "|    value_loss         | 0.246       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1753        |\n",
            "|    iterations         | 7500        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 37500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.45       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7499        |\n",
            "|    policy_loss        | -0.0265     |\n",
            "|    reward             | 0.012373233 |\n",
            "|    std                | 7.59        |\n",
            "|    value_loss         | 0.000131    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1754        |\n",
            "|    iterations         | 7600        |\n",
            "|    time_elapsed       | 21          |\n",
            "|    total_timesteps    | 38000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.46       |\n",
            "|    explained_variance | -0.161      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7599        |\n",
            "|    policy_loss        | 0.0205      |\n",
            "|    reward             | 0.015747286 |\n",
            "|    std                | 7.7         |\n",
            "|    value_loss         | 0.000322    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1754         |\n",
            "|    iterations         | 7700         |\n",
            "|    time_elapsed       | 21           |\n",
            "|    total_timesteps    | 38500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.48        |\n",
            "|    explained_variance | -0.186       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 7699         |\n",
            "|    policy_loss        | -0.206       |\n",
            "|    reward             | -0.021659812 |\n",
            "|    std                | 7.84         |\n",
            "|    value_loss         | 0.00654      |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1755       |\n",
            "|    iterations         | 7800       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 39000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.49      |\n",
            "|    explained_variance | 0.145      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7799       |\n",
            "|    policy_loss        | -0.127     |\n",
            "|    reward             | 0.04186782 |\n",
            "|    std                | 7.93       |\n",
            "|    value_loss         | 0.00174    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1756       |\n",
            "|    iterations         | 7900       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 39500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.49      |\n",
            "|    explained_variance | -0.317     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7899       |\n",
            "|    policy_loss        | -0.68      |\n",
            "|    reward             | 0.03145032 |\n",
            "|    std                | 7.97       |\n",
            "|    value_loss         | 0.0512     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1756       |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 22         |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.51      |\n",
            "|    explained_variance | -0.0581    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | 0.344      |\n",
            "|    reward             | 0.07920967 |\n",
            "|    std                | 8.09       |\n",
            "|    value_loss         | 0.0407     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1757        |\n",
            "|    iterations         | 8100        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 40500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.5        |\n",
            "|    explained_variance | -0.337      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8099        |\n",
            "|    policy_loss        | 1.2         |\n",
            "|    reward             | -0.16997992 |\n",
            "|    std                | 8.05        |\n",
            "|    value_loss         | 0.108       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1757        |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.51       |\n",
            "|    explained_variance | -1.47       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | -0.423      |\n",
            "|    reward             | 0.041258503 |\n",
            "|    std                | 8.07        |\n",
            "|    value_loss         | 0.0134      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1758        |\n",
            "|    iterations         | 8300        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 41500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.53       |\n",
            "|    explained_variance | 0.105       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8299        |\n",
            "|    policy_loss        | -0.209      |\n",
            "|    reward             | 0.011367636 |\n",
            "|    std                | 8.23        |\n",
            "|    value_loss         | 0.00494     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1758        |\n",
            "|    iterations         | 8400        |\n",
            "|    time_elapsed       | 23          |\n",
            "|    total_timesteps    | 42000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.53       |\n",
            "|    explained_variance | -0.132      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8399        |\n",
            "|    policy_loss        | 0.158       |\n",
            "|    reward             | 0.064887874 |\n",
            "|    std                | 8.29        |\n",
            "|    value_loss         | 0.00363     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1758        |\n",
            "|    iterations         | 8500        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 42500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.54       |\n",
            "|    explained_variance | -0.744      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8499        |\n",
            "|    policy_loss        | -0.593      |\n",
            "|    reward             | 0.053014975 |\n",
            "|    std                | 8.36        |\n",
            "|    value_loss         | 0.0347      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1759        |\n",
            "|    iterations         | 8600        |\n",
            "|    time_elapsed       | 24          |\n",
            "|    total_timesteps    | 43000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.56       |\n",
            "|    explained_variance | 0.284       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8599        |\n",
            "|    policy_loss        | -0.0149     |\n",
            "|    reward             | -0.04699013 |\n",
            "|    std                | 8.54        |\n",
            "|    value_loss         | 0.0021      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1759       |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 24         |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.56      |\n",
            "|    explained_variance | -0.0446    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | 0.648      |\n",
            "|    reward             | -0.0932618 |\n",
            "|    std                | 8.48       |\n",
            "|    value_loss         | 0.0667     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1759       |\n",
            "|    iterations         | 8800       |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 44000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.57      |\n",
            "|    explained_variance | -0.148     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8799       |\n",
            "|    policy_loss        | -0.0833    |\n",
            "|    reward             | 0.07616804 |\n",
            "|    std                | 8.56       |\n",
            "|    value_loss         | 0.094      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1760       |\n",
            "|    iterations         | 8900       |\n",
            "|    time_elapsed       | 25         |\n",
            "|    total_timesteps    | 44500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.58      |\n",
            "|    explained_variance | -0.559     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8899       |\n",
            "|    policy_loss        | 0.157      |\n",
            "|    reward             | 0.11313576 |\n",
            "|    std                | 8.69       |\n",
            "|    value_loss         | 0.0044     |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1760        |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 25          |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.6        |\n",
            "|    explained_variance | -1.37       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 0.633       |\n",
            "|    reward             | 0.023239452 |\n",
            "|    std                | 8.86        |\n",
            "|    value_loss         | 0.0583      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1761         |\n",
            "|    iterations         | 9100         |\n",
            "|    time_elapsed       | 25           |\n",
            "|    total_timesteps    | 45500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.62        |\n",
            "|    explained_variance | -0.189       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9099         |\n",
            "|    policy_loss        | -0.0293      |\n",
            "|    reward             | -0.014160977 |\n",
            "|    std                | 9.07         |\n",
            "|    value_loss         | 0.00128      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1761         |\n",
            "|    iterations         | 9200         |\n",
            "|    time_elapsed       | 26           |\n",
            "|    total_timesteps    | 46000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.63        |\n",
            "|    explained_variance | 0.139        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9199         |\n",
            "|    policy_loss        | -0.49        |\n",
            "|    reward             | -0.026885822 |\n",
            "|    std                | 9.15         |\n",
            "|    value_loss         | 0.0227       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1761        |\n",
            "|    iterations         | 9300        |\n",
            "|    time_elapsed       | 26          |\n",
            "|    total_timesteps    | 46500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.64       |\n",
            "|    explained_variance | 0.118       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9299        |\n",
            "|    policy_loss        | 0.373       |\n",
            "|    reward             | -0.23231302 |\n",
            "|    std                | 9.25        |\n",
            "|    value_loss         | 0.0192      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1761        |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 26          |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.64       |\n",
            "|    explained_variance | 0.0608      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | 1.35        |\n",
            "|    reward             | -0.12413266 |\n",
            "|    std                | 9.25        |\n",
            "|    value_loss         | 0.324       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1761        |\n",
            "|    iterations         | 9500        |\n",
            "|    time_elapsed       | 26          |\n",
            "|    total_timesteps    | 47500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.65       |\n",
            "|    explained_variance | 0.0743      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9499        |\n",
            "|    policy_loss        | -1.03       |\n",
            "|    reward             | 0.058675814 |\n",
            "|    std                | 9.29        |\n",
            "|    value_loss         | 0.187       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1762        |\n",
            "|    iterations         | 9600        |\n",
            "|    time_elapsed       | 27          |\n",
            "|    total_timesteps    | 48000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.65       |\n",
            "|    explained_variance | 0.291       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9599        |\n",
            "|    policy_loss        | -0.122      |\n",
            "|    reward             | 0.057636734 |\n",
            "|    std                | 9.31        |\n",
            "|    value_loss         | 0.00109     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1762       |\n",
            "|    iterations         | 9700       |\n",
            "|    time_elapsed       | 27         |\n",
            "|    total_timesteps    | 48500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.67      |\n",
            "|    explained_variance | 0.683      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9699       |\n",
            "|    policy_loss        | -0.0642    |\n",
            "|    reward             | 0.09481358 |\n",
            "|    std                | 9.48       |\n",
            "|    value_loss         | 0.000815   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1762        |\n",
            "|    iterations         | 9800        |\n",
            "|    time_elapsed       | 27          |\n",
            "|    total_timesteps    | 49000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.68       |\n",
            "|    explained_variance | -0.882      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9799        |\n",
            "|    policy_loss        | -0.0456     |\n",
            "|    reward             | -0.16269393 |\n",
            "|    std                | 9.63        |\n",
            "|    value_loss         | 0.000742    |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1763          |\n",
            "|    iterations         | 9900          |\n",
            "|    time_elapsed       | 28            |\n",
            "|    total_timesteps    | 49500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.7          |\n",
            "|    explained_variance | -0.22         |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 9899          |\n",
            "|    policy_loss        | 0.192         |\n",
            "|    reward             | -0.0004384554 |\n",
            "|    std                | 9.81          |\n",
            "|    value_loss         | 0.0123        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1763        |\n",
            "|    iterations         | 10000       |\n",
            "|    time_elapsed       | 28          |\n",
            "|    total_timesteps    | 50000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.72       |\n",
            "|    explained_variance | -0.0471     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9999        |\n",
            "|    policy_loss        | -1.04       |\n",
            "|    reward             | -0.11243387 |\n",
            "|    std                | 9.95        |\n",
            "|    value_loss         | 0.08        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1763       |\n",
            "|    iterations         | 10100      |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 50500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.73      |\n",
            "|    explained_variance | -0.0792    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 10099      |\n",
            "|    policy_loss        | 0.249      |\n",
            "|    reward             | -0.6864058 |\n",
            "|    std                | 10.1       |\n",
            "|    value_loss         | 0.392      |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1763        |\n",
            "|    iterations         | 10200       |\n",
            "|    time_elapsed       | 28          |\n",
            "|    total_timesteps    | 51000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.73       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10199       |\n",
            "|    policy_loss        | -0.0122     |\n",
            "|    reward             | 0.005204817 |\n",
            "|    std                | 10.1        |\n",
            "|    value_loss         | 1.62e-05    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1764        |\n",
            "|    iterations         | 10300       |\n",
            "|    time_elapsed       | 29          |\n",
            "|    total_timesteps    | 51500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.74       |\n",
            "|    explained_variance | 0.473       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10299       |\n",
            "|    policy_loss        | -0.224      |\n",
            "|    reward             | 0.042506315 |\n",
            "|    std                | 10.2        |\n",
            "|    value_loss         | 0.0057      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1764         |\n",
            "|    iterations         | 10400        |\n",
            "|    time_elapsed       | 29           |\n",
            "|    total_timesteps    | 52000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.75        |\n",
            "|    explained_variance | 0.125        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 10399        |\n",
            "|    policy_loss        | -0.349       |\n",
            "|    reward             | -0.018669114 |\n",
            "|    std                | 10.3         |\n",
            "|    value_loss         | 0.0102       |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1765        |\n",
            "|    iterations         | 10500       |\n",
            "|    time_elapsed       | 29          |\n",
            "|    total_timesteps    | 52500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.78       |\n",
            "|    explained_variance | -0.117      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10499       |\n",
            "|    policy_loss        | 0.44        |\n",
            "|    reward             | -0.03579628 |\n",
            "|    std                | 10.6        |\n",
            "|    value_loss         | 0.0297      |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1766         |\n",
            "|    iterations         | 10600        |\n",
            "|    time_elapsed       | 30           |\n",
            "|    total_timesteps    | 53000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.79        |\n",
            "|    explained_variance | -0.146       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 10599        |\n",
            "|    policy_loss        | 0.429        |\n",
            "|    reward             | -0.007203775 |\n",
            "|    std                | 10.7         |\n",
            "|    value_loss         | 0.0149       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1766         |\n",
            "|    iterations         | 10700        |\n",
            "|    time_elapsed       | 30           |\n",
            "|    total_timesteps    | 53500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -3.81        |\n",
            "|    explained_variance | -0.0232      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 10699        |\n",
            "|    policy_loss        | 0.797        |\n",
            "|    reward             | -0.032825924 |\n",
            "|    std                | 10.9         |\n",
            "|    value_loss         | 0.074        |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1766        |\n",
            "|    iterations         | 10800       |\n",
            "|    time_elapsed       | 30          |\n",
            "|    total_timesteps    | 54000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.81       |\n",
            "|    explained_variance | 0.00263     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 10799       |\n",
            "|    policy_loss        | -3.3        |\n",
            "|    reward             | 0.028063504 |\n",
            "|    std                | 11          |\n",
            "|    value_loss         | 1.73        |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1767          |\n",
            "|    iterations         | 10900         |\n",
            "|    time_elapsed       | 30            |\n",
            "|    total_timesteps    | 54500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.82         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 10899         |\n",
            "|    policy_loss        | -0.0445       |\n",
            "|    reward             | -0.0011437663 |\n",
            "|    std                | 11            |\n",
            "|    value_loss         | 0.000124      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1767          |\n",
            "|    iterations         | 11000         |\n",
            "|    time_elapsed       | 31            |\n",
            "|    total_timesteps    | 55000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.83         |\n",
            "|    explained_variance | -0.824        |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 10999         |\n",
            "|    policy_loss        | 0.279         |\n",
            "|    reward             | -0.0077656773 |\n",
            "|    std                | 11.1          |\n",
            "|    value_loss         | 0.00687       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1767        |\n",
            "|    iterations         | 11100       |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 55500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.85       |\n",
            "|    explained_variance | -0.0893     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11099       |\n",
            "|    policy_loss        | -0.0885     |\n",
            "|    reward             | 0.014943601 |\n",
            "|    std                | 11.4        |\n",
            "|    value_loss         | 0.000665    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1768        |\n",
            "|    iterations         | 11200       |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 56000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.87       |\n",
            "|    explained_variance | 0.266       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11199       |\n",
            "|    policy_loss        | 0.468       |\n",
            "|    reward             | -0.23938867 |\n",
            "|    std                | 11.6        |\n",
            "|    value_loss         | 0.0226      |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1768        |\n",
            "|    iterations         | 11300       |\n",
            "|    time_elapsed       | 31          |\n",
            "|    total_timesteps    | 56500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.89       |\n",
            "|    explained_variance | -1.76       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11299       |\n",
            "|    policy_loss        | 0.0648      |\n",
            "|    reward             | 0.023213875 |\n",
            "|    std                | 11.9        |\n",
            "|    value_loss         | 0.00224     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1769       |\n",
            "|    iterations         | 11400      |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 57000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.91      |\n",
            "|    explained_variance | 0.112      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11399      |\n",
            "|    policy_loss        | 0.264      |\n",
            "|    reward             | 0.07136482 |\n",
            "|    std                | 12         |\n",
            "|    value_loss         | 0.0126     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1769       |\n",
            "|    iterations         | 11500      |\n",
            "|    time_elapsed       | 32         |\n",
            "|    total_timesteps    | 57500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.91      |\n",
            "|    explained_variance | -0.711     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11499      |\n",
            "|    policy_loss        | 1.3        |\n",
            "|    reward             | 0.47435585 |\n",
            "|    std                | 12         |\n",
            "|    value_loss         | 0.11       |\n",
            "--------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1770          |\n",
            "|    iterations         | 11600         |\n",
            "|    time_elapsed       | 32            |\n",
            "|    total_timesteps    | 58000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -3.91         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 11599         |\n",
            "|    policy_loss        | -0.129        |\n",
            "|    reward             | -0.0037123396 |\n",
            "|    std                | 12.1          |\n",
            "|    value_loss         | 0.00126       |\n",
            "-----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1770       |\n",
            "|    iterations         | 11700      |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 58500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.93      |\n",
            "|    explained_variance | 0.781      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11699      |\n",
            "|    policy_loss        | -0.0484    |\n",
            "|    reward             | 0.02679832 |\n",
            "|    std                | 12.3       |\n",
            "|    value_loss         | 0.000509   |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1770        |\n",
            "|    iterations         | 11800       |\n",
            "|    time_elapsed       | 33          |\n",
            "|    total_timesteps    | 59000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -3.93       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 11799       |\n",
            "|    policy_loss        | -0.276      |\n",
            "|    reward             | 0.026518892 |\n",
            "|    std                | 12.4        |\n",
            "|    value_loss         | 0.00363     |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1770       |\n",
            "|    iterations         | 11900      |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 59500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.94      |\n",
            "|    explained_variance | 0.345      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11899      |\n",
            "|    policy_loss        | -0.244     |\n",
            "|    reward             | 0.03574361 |\n",
            "|    std                | 12.5       |\n",
            "|    value_loss         | 0.00645    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1771       |\n",
            "|    iterations         | 12000      |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 60000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -3.95      |\n",
            "|    explained_variance | -0.108     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 11999      |\n",
            "|    policy_loss        | -0.474     |\n",
            "|    reward             | 0.14238758 |\n",
            "|    std                | 12.6       |\n",
            "|    value_loss         | 0.0197     |\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=60000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 242         |\n",
            "|    time_elapsed    | 55          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -28.9       |\n",
            "|    critic_loss     | 0.0756      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 242         |\n",
            "|    time_elapsed    | 111         |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -14.4       |\n",
            "|    critic_loss     | 0.056       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 110\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 236         |\n",
            "|    time_elapsed    | 172         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -8.23       |\n",
            "|    critic_loss     | 0.0772      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 240         |\n",
            "|    time_elapsed    | 225         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -4.83       |\n",
            "|    critic_loss     | 0.0471      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 120\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 20          |\n",
            "|    fps             | 244         |\n",
            "|    time_elapsed    | 278         |\n",
            "|    total_timesteps | 67940       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -2.93       |\n",
            "|    critic_loss     | 0.029       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 67839       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=80000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 2889         |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 0            |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | 0.0012088026 |\n",
            "-------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2504         |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 1            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.005105222  |\n",
            "|    clip_fraction        | 0.0406       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -4.15        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0248      |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00229     |\n",
            "|    reward               | 0.0005436136 |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 0.000958     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2456         |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 2            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034112213 |\n",
            "|    clip_fraction        | 0.0191       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -0.683       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00124     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00141     |\n",
            "|    reward               | -0.009009346 |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 0.00056      |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2436           |\n",
            "|    iterations           | 4              |\n",
            "|    time_elapsed         | 3              |\n",
            "|    total_timesteps      | 8192           |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0071679      |\n",
            "|    clip_fraction        | 0.0436         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | 0.0234         |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0179        |\n",
            "|    n_updates            | 30             |\n",
            "|    policy_gradient_loss | -0.00354       |\n",
            "|    reward               | -0.00015616584 |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 0.000116       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2419          |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0035559146  |\n",
            "|    clip_fraction        | 0.0239        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.95         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.019        |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.00247      |\n",
            "|    reward               | -5.984597e-06 |\n",
            "|    std                  | 0.994         |\n",
            "|    value_loss           | 0.00109       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2405          |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 5             |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0027762665  |\n",
            "|    clip_fraction        | 0.0283        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.41         |\n",
            "|    explained_variance   | 0.272         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0265       |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00344      |\n",
            "|    reward               | -0.0065036225 |\n",
            "|    std                  | 0.996         |\n",
            "|    value_loss           | 0.00307       |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2388        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008312696 |\n",
            "|    clip_fraction        | 0.0875      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.584       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | -0.0287     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00868    |\n",
            "|    reward               | 0.029328678 |\n",
            "|    std                  | 0.984       |\n",
            "|    value_loss           | 0.00451     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038998886 |\n",
            "|    clip_fraction        | 0.028        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.199        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0232       |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0041      |\n",
            "|    reward               | 0.17092964   |\n",
            "|    std                  | 0.983        |\n",
            "|    value_loss           | 0.0595       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2378        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004505284 |\n",
            "|    clip_fraction        | 0.0383      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | -0.12       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | -0.0314     |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00615    |\n",
            "|    reward               | 0.021880936 |\n",
            "|    std                  | 0.982       |\n",
            "|    value_loss           | 0.0229      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020079836 |\n",
            "|    clip_fraction        | 0.0103       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.0521       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0312       |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    reward               | 0.005456524  |\n",
            "|    std                  | 0.988        |\n",
            "|    value_loss           | 0.128        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2389        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003003456 |\n",
            "|    clip_fraction        | 0.0221      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.0392      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.162       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00378    |\n",
            "|    reward               | 0.0585009   |\n",
            "|    std                  | 0.989       |\n",
            "|    value_loss           | 0.334       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003643889  |\n",
            "|    clip_fraction        | 0.033        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.232        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0129      |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00268     |\n",
            "|    reward               | 0.0024152014 |\n",
            "|    std                  | 0.989        |\n",
            "|    value_loss           | 0.015        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004939596 |\n",
            "|    clip_fraction        | 0.0259      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0948      |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00341    |\n",
            "|    reward               | -0.02866055 |\n",
            "|    std                  | 0.987       |\n",
            "|    value_loss           | 0.257       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032601478 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | -0.0578      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.00992      |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    reward               | -0.009538958 |\n",
            "|    std                  | 0.978        |\n",
            "|    value_loss           | 0.0537       |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2384           |\n",
            "|    iterations           | 15             |\n",
            "|    time_elapsed         | 12             |\n",
            "|    total_timesteps      | 30720          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.002567035    |\n",
            "|    clip_fraction        | 0.0155         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.4           |\n",
            "|    explained_variance   | 0.065          |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 0.0533         |\n",
            "|    n_updates            | 140            |\n",
            "|    policy_gradient_loss | -0.00374       |\n",
            "|    reward               | -0.00056177704 |\n",
            "|    std                  | 0.977          |\n",
            "|    value_loss           | 0.168          |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027338504 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.0381       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.138        |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    reward               | -0.08291085  |\n",
            "|    std                  | 0.979        |\n",
            "|    value_loss           | 0.291        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 20\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 176330.12\n",
            "total_reward: 166330.12\n",
            "total_cost: 764.04\n",
            "total_trades: 3150\n",
            "Sharpe: 0.945\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2387         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033034524 |\n",
            "|    clip_fraction        | 0.0189       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.13         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00793     |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00336     |\n",
            "|    reward               | -0.017476993 |\n",
            "|    std                  | 0.974        |\n",
            "|    value_loss           | 0.0157       |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2391           |\n",
            "|    iterations           | 18             |\n",
            "|    time_elapsed         | 15             |\n",
            "|    total_timesteps      | 36864          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0020663026   |\n",
            "|    clip_fraction        | 0.0153         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.39          |\n",
            "|    explained_variance   | 0.0786         |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 0.114          |\n",
            "|    n_updates            | 170            |\n",
            "|    policy_gradient_loss | -0.0036        |\n",
            "|    reward               | -5.3411644e-05 |\n",
            "|    std                  | 0.969          |\n",
            "|    value_loss           | 0.286          |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2392         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003273891  |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.0327       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0272       |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    reward               | -0.029642792 |\n",
            "|    std                  | 0.974        |\n",
            "|    value_loss           | 0.112        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2393          |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0060546375  |\n",
            "|    clip_fraction        | 0.0556        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.0884        |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.0952        |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -0.00641      |\n",
            "|    reward               | -0.0013553398 |\n",
            "|    std                  | 0.971         |\n",
            "|    value_loss           | 0.272         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2394         |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037046913 |\n",
            "|    clip_fraction        | 0.0228       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.108        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.195        |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00453     |\n",
            "|    reward               | 0.08818028   |\n",
            "|    std                  | 0.97         |\n",
            "|    value_loss           | 0.453        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2392          |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 45056         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0036336463  |\n",
            "|    clip_fraction        | 0.0237        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.072         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.00431       |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -0.00314      |\n",
            "|    reward               | -0.0025142445 |\n",
            "|    std                  | 0.966         |\n",
            "|    value_loss           | 0.0265        |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2391         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036371415 |\n",
            "|    clip_fraction        | 0.0246       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.129        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.153        |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    reward               | -0.25216508  |\n",
            "|    std                  | 0.967        |\n",
            "|    value_loss           | 0.404        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2387         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036475854 |\n",
            "|    clip_fraction        | 0.0405       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.0722       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.115        |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.00583     |\n",
            "|    reward               | -0.10216621  |\n",
            "|    std                  | 0.968        |\n",
            "|    value_loss           | 0.236        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2385         |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049593663 |\n",
            "|    clip_fraction        | 0.0406       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.173        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.155        |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00665     |\n",
            "|    reward               | 0.0052041365 |\n",
            "|    std                  | 0.963        |\n",
            "|    value_loss           | 0.296        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023827879 |\n",
            "|    clip_fraction        | 0.0173       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.239        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.288        |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.00366     |\n",
            "|    reward               | 0.23864456   |\n",
            "|    std                  | 0.963        |\n",
            "|    value_loss           | 0.431        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 23           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048498986 |\n",
            "|    clip_fraction        | 0.0479       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.232       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.025       |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    reward               | -0.042849302 |\n",
            "|    std                  | 0.969        |\n",
            "|    value_loss           | 0.0268       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026143715 |\n",
            "|    clip_fraction        | 0.0183       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.251        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.187        |\n",
            "|    n_updates            | 270          |\n",
            "|    policy_gradient_loss | -0.00349     |\n",
            "|    reward               | 0.108991206  |\n",
            "|    std                  | 0.967        |\n",
            "|    value_loss           | 0.4          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2380        |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.00520762  |\n",
            "|    clip_fraction        | 0.0427      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.141       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0721      |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    reward               | 0.048421245 |\n",
            "|    std                  | 0.96        |\n",
            "|    value_loss           | 0.266       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042847637 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.194        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.149        |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.00518     |\n",
            "|    reward               | 0.03109121   |\n",
            "|    std                  | 0.957        |\n",
            "|    value_loss           | 0.293        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2374        |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002283438 |\n",
            "|    clip_fraction        | 0.0126      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.29        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.211       |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0038     |\n",
            "|    reward               | -0.09485814 |\n",
            "|    std                  | 0.954       |\n",
            "|    value_loss           | 0.535       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.005739495  |\n",
            "|    clip_fraction        | 0.0557       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.146       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00448     |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.00562     |\n",
            "|    reward               | -0.013649777 |\n",
            "|    std                  | 0.961        |\n",
            "|    value_loss           | 0.0318       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 28           |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037111884 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.296        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.217        |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.00462     |\n",
            "|    reward               | 0.15608002   |\n",
            "|    std                  | 0.963        |\n",
            "|    value_loss           | 0.53         |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 30\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 253755.81\n",
            "total_reward: 243755.81\n",
            "total_cost: 807.49\n",
            "total_trades: 3135\n",
            "Sharpe: 1.022\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024173697 |\n",
            "|    clip_fraction        | 0.0196       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.209        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.122        |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    reward               | 0.040274356  |\n",
            "|    std                  | 0.964        |\n",
            "|    value_loss           | 0.286        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054188287 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.21         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0757       |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    reward               | -0.023343544 |\n",
            "|    std                  | 0.961        |\n",
            "|    value_loss           | 0.275        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061452943 |\n",
            "|    clip_fraction        | 0.0472       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.362        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.237        |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.00479     |\n",
            "|    reward               | -0.14197187  |\n",
            "|    std                  | 0.96         |\n",
            "|    value_loss           | 0.583        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 75776        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003938235  |\n",
            "|    clip_fraction        | 0.0254       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.108       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0129      |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    reward               | -0.045578614 |\n",
            "|    std                  | 0.959        |\n",
            "|    value_loss           | 0.0359       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2377        |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 32          |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003028945 |\n",
            "|    clip_fraction        | 0.014       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.439       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.231       |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00126    |\n",
            "|    reward               | -0.4329266  |\n",
            "|    std                  | 0.96        |\n",
            "|    value_loss           | 0.538       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049317484 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.437        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0686       |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.00372     |\n",
            "|    reward               | -0.045902118 |\n",
            "|    std                  | 0.96         |\n",
            "|    value_loss           | 0.32         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2378          |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0071796943  |\n",
            "|    clip_fraction        | 0.0472        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.491         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.126         |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -0.00427      |\n",
            "|    reward               | -0.0038444898 |\n",
            "|    std                  | 0.957         |\n",
            "|    value_loss           | 0.277         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042553768 |\n",
            "|    clip_fraction        | 0.0449       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.481        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.345        |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00437     |\n",
            "|    reward               | -0.12762655  |\n",
            "|    std                  | 0.959        |\n",
            "|    value_loss           | 0.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067870514 |\n",
            "|    clip_fraction        | 0.0672       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.0654      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00609     |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.00218     |\n",
            "|    reward               | 0.061554443  |\n",
            "|    std                  | 0.948        |\n",
            "|    value_loss           | 0.0407       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2378        |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003917687 |\n",
            "|    clip_fraction        | 0.0349      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.493       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.207       |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00403    |\n",
            "|    reward               | -0.05102773 |\n",
            "|    std                  | 0.945       |\n",
            "|    value_loss           | 0.597       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2379        |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 37          |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.00258938  |\n",
            "|    clip_fraction        | 0.0355      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.533       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.111       |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0042     |\n",
            "|    reward               | 0.012504807 |\n",
            "|    std                  | 0.945       |\n",
            "|    value_loss           | 0.319       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.002979846  |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0.297        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0933       |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    reward               | -0.022172049 |\n",
            "|    std                  | 0.946        |\n",
            "|    value_loss           | 0.296        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024894578 |\n",
            "|    clip_fraction        | 0.0213       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0.528        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.214        |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    reward               | -0.39901328  |\n",
            "|    std                  | 0.947        |\n",
            "|    value_loss           | 0.539        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028948882 |\n",
            "|    clip_fraction        | 0.0518       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.203       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.00697      |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.000395    |\n",
            "|    reward               | 0.01652417   |\n",
            "|    std                  | 0.958        |\n",
            "|    value_loss           | 0.0568       |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 2377       |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 41         |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0040577  |\n",
            "|    clip_fraction        | 0.0257     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.38      |\n",
            "|    explained_variance   | 0.558      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.182      |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.0054    |\n",
            "|    reward               | 0.12906277 |\n",
            "|    std                  | 0.957      |\n",
            "|    value_loss           | 0.435      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 42           |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003045065  |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.503        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.148        |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    reward               | -0.025775155 |\n",
            "|    std                  | 0.966        |\n",
            "|    value_loss           | 0.343        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 40\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 254609.23\n",
            "total_reward: 244609.23\n",
            "total_cost: 679.56\n",
            "total_trades: 3159\n",
            "Sharpe: 1.021\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2378        |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 43          |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004141118 |\n",
            "|    clip_fraction        | 0.0522      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.575       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0902      |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.00412    |\n",
            "|    reward               | 0.006700715 |\n",
            "|    std                  | 0.969       |\n",
            "|    value_loss           | 0.178       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 104448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048685363 |\n",
            "|    clip_fraction        | 0.0217       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.641        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.216        |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    reward               | 0.22693492   |\n",
            "|    std                  | 0.971        |\n",
            "|    value_loss           | 0.463        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 106496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.009739159  |\n",
            "|    clip_fraction        | 0.0693       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | -0.0154      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0441       |\n",
            "|    n_updates            | 510          |\n",
            "|    policy_gradient_loss | -0.00189     |\n",
            "|    reward               | -0.009791301 |\n",
            "|    std                  | 0.966        |\n",
            "|    value_loss           | 0.0719       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 108544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059926845 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.521        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.143        |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00439     |\n",
            "|    reward               | -0.57339436  |\n",
            "|    std                  | 0.967        |\n",
            "|    value_loss           | 0.517        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004231839  |\n",
            "|    clip_fraction        | 0.0355       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.583        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.259        |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -0.00448     |\n",
            "|    reward               | -0.040342458 |\n",
            "|    std                  | 0.967        |\n",
            "|    value_loss           | 0.408        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 47           |\n",
            "|    total_timesteps      | 112640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023237534 |\n",
            "|    clip_fraction        | 0.0358       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.492        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0815       |\n",
            "|    n_updates            | 540          |\n",
            "|    policy_gradient_loss | -0.00349     |\n",
            "|    reward               | 0.024166662  |\n",
            "|    std                  | 0.97         |\n",
            "|    value_loss           | 0.153        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034182596 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.627        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.085        |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    reward               | 0.017099738  |\n",
            "|    std                  | 0.971        |\n",
            "|    value_loss           | 0.464        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2379        |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007907632 |\n",
            "|    clip_fraction        | 0.0651      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | -0.0209     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0414      |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.000525   |\n",
            "|    reward               | 0.08790465  |\n",
            "|    std                  | 0.967       |\n",
            "|    value_loss           | 0.0748      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2377        |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002703562 |\n",
            "|    clip_fraction        | 0.015       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.649       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.199       |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00201    |\n",
            "|    reward               | -0.381563   |\n",
            "|    std                  | 0.966       |\n",
            "|    value_loss           | 0.444       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 120832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019036441 |\n",
            "|    clip_fraction        | 0.00454      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.58         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.165        |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00202     |\n",
            "|    reward               | 0.110471986  |\n",
            "|    std                  | 0.965        |\n",
            "|    value_loss           | 0.446        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048098275 |\n",
            "|    clip_fraction        | 0.031        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.668        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0651       |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    reward               | 0.0077452147 |\n",
            "|    std                  | 0.969        |\n",
            "|    value_loss           | 0.106        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 52           |\n",
            "|    total_timesteps      | 124928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038182666 |\n",
            "|    clip_fraction        | 0.0178       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.578        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.229        |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    reward               | -0.13511652  |\n",
            "|    std                  | 0.97         |\n",
            "|    value_loss           | 0.544        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2375        |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 53          |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005779258 |\n",
            "|    clip_fraction        | 0.0575      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | -0.2        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0409      |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.0039     |\n",
            "|    reward               | 0.05213114  |\n",
            "|    std                  | 0.987       |\n",
            "|    value_loss           | 0.0938      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 54           |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027081913 |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.623        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.248        |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00266     |\n",
            "|    reward               | -0.057645105 |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 0.463        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2371         |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 55           |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020883903 |\n",
            "|    clip_fraction        | 0.0131       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.534        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.173        |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    reward               | 0.07389905   |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 0.458        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2369         |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 56           |\n",
            "|    total_timesteps      | 133120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054178275 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.464        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00765     |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.00311     |\n",
            "|    reward               | -0.00148209  |\n",
            "|    std                  | 0.984        |\n",
            "|    value_loss           | 0.0591       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2368         |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 57           |\n",
            "|    total_timesteps      | 135168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051096696 |\n",
            "|    clip_fraction        | 0.0481       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.637        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.259        |\n",
            "|    n_updates            | 650          |\n",
            "|    policy_gradient_loss | -0.00448     |\n",
            "|    reward               | 0.16479358   |\n",
            "|    std                  | 0.982        |\n",
            "|    value_loss           | 0.507        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 50\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 262190.64\n",
            "total_reward: 252190.64\n",
            "total_cost: 584.78\n",
            "total_trades: 3150\n",
            "Sharpe: 1.026\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2367        |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005345472 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.0222      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0202      |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.00174    |\n",
            "|    reward               | 0.025319792 |\n",
            "|    std                  | 0.98        |\n",
            "|    value_loss           | 0.0807      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2366         |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040551545 |\n",
            "|    clip_fraction        | 0.0374       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.599        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.177        |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.00418     |\n",
            "|    reward               | -1.0027527   |\n",
            "|    std                  | 0.979        |\n",
            "|    value_loss           | 0.481        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2367         |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 59           |\n",
            "|    total_timesteps      | 141312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035090037 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.614        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.246        |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00397     |\n",
            "|    reward               | -0.01017875  |\n",
            "|    std                  | 0.981        |\n",
            "|    value_loss           | 0.545        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2365         |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 143360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006275383  |\n",
            "|    clip_fraction        | 0.0543       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | -0.0842      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.00594      |\n",
            "|    n_updates            | 690          |\n",
            "|    policy_gradient_loss | -0.00143     |\n",
            "|    reward               | -0.060359955 |\n",
            "|    std                  | 0.978        |\n",
            "|    value_loss           | 0.0312       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2366         |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.001643894  |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.59         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.211        |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    reward               | -0.011301721 |\n",
            "|    std                  | 0.984        |\n",
            "|    value_loss           | 0.55         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2366        |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 62          |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003932791 |\n",
            "|    clip_fraction        | 0.0316      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.306       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0646      |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | -0.00245    |\n",
            "|    reward               | 0.08496973  |\n",
            "|    std                  | 0.982       |\n",
            "|    value_loss           | 0.119       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2367          |\n",
            "|    iterations           | 73            |\n",
            "|    time_elapsed         | 63            |\n",
            "|    total_timesteps      | 149504        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0023196177  |\n",
            "|    clip_fraction        | 0.0178        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.4          |\n",
            "|    explained_variance   | 0.662         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.284         |\n",
            "|    n_updates            | 720           |\n",
            "|    policy_gradient_loss | -0.00283      |\n",
            "|    reward               | 0.00048702434 |\n",
            "|    std                  | 0.984         |\n",
            "|    value_loss           | 0.449         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2367         |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 64           |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023630094 |\n",
            "|    clip_fraction        | 0.0227       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.627        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.243        |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    reward               | -0.035418447 |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.588        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2368        |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006503785 |\n",
            "|    clip_fraction        | 0.0715      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.00264     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0154      |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00335    |\n",
            "|    reward               | 0.03641664  |\n",
            "|    std                  | 0.986       |\n",
            "|    value_loss           | 0.028       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2369         |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 65           |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006102302  |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.622        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.291        |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00355     |\n",
            "|    reward               | -0.047151126 |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.647        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2370         |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 157696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013194637 |\n",
            "|    clip_fraction        | 0.0121       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.602        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0299       |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00174     |\n",
            "|    reward               | -0.00141397  |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.177        |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2371           |\n",
            "|    iterations           | 78             |\n",
            "|    time_elapsed         | 67             |\n",
            "|    total_timesteps      | 159744         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0016403737   |\n",
            "|    clip_fraction        | 0.0185         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.41          |\n",
            "|    explained_variance   | 0.605          |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 0.226          |\n",
            "|    n_updates            | 770            |\n",
            "|    policy_gradient_loss | -0.00246       |\n",
            "|    reward               | -0.00040328017 |\n",
            "|    std                  | 0.987          |\n",
            "|    value_loss           | 0.567          |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2371         |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 68           |\n",
            "|    total_timesteps      | 161792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018282766 |\n",
            "|    clip_fraction        | 0.0116       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.61         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.247        |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    reward               | 0.03218794   |\n",
            "|    std                  | 0.988        |\n",
            "|    value_loss           | 0.685        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2372        |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008690625 |\n",
            "|    clip_fraction        | 0.0864      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.0579      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0087      |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00307    |\n",
            "|    reward               | 0.057509582 |\n",
            "|    std                  | 0.984       |\n",
            "|    value_loss           | 0.035       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2373        |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004367846 |\n",
            "|    clip_fraction        | 0.0265      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.601       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.316       |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00374    |\n",
            "|    reward               | -0.1764577  |\n",
            "|    std                  | 0.983       |\n",
            "|    value_loss           | 0.68        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004102479  |\n",
            "|    clip_fraction        | 0.0184       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.489        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.102        |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    reward               | 0.0071056867 |\n",
            "|    std                  | 0.982        |\n",
            "|    value_loss           | 0.23         |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 60\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 267639.37\n",
            "total_reward: 257639.37\n",
            "total_cost: 624.51\n",
            "total_trades: 3173\n",
            "Sharpe: 1.030\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 71           |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033299252 |\n",
            "|    clip_fraction        | 0.0169       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.629        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.236        |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.00199     |\n",
            "|    reward               | 0.0018072503 |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.409        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.005493282  |\n",
            "|    clip_fraction        | 0.0366       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.61         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.308        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00343     |\n",
            "|    reward               | 0.0024256487 |\n",
            "|    std                  | 0.989        |\n",
            "|    value_loss           | 0.623        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041161305 |\n",
            "|    clip_fraction        | 0.058        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.0168       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0047      |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | 0.000106     |\n",
            "|    reward               | -0.07202906  |\n",
            "|    std                  | 0.995        |\n",
            "|    value_loss           | 0.0348       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2377        |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005776549 |\n",
            "|    clip_fraction        | 0.0467      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.642       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.27        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00408    |\n",
            "|    reward               | 0.21256118  |\n",
            "|    std                  | 0.993       |\n",
            "|    value_loss           | 0.618       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 178176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027722279 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.668        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0774       |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -0.00298     |\n",
            "|    reward               | 0.042393185  |\n",
            "|    std                  | 0.997        |\n",
            "|    value_loss           | 0.225        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2378          |\n",
            "|    iterations           | 88            |\n",
            "|    time_elapsed         | 75            |\n",
            "|    total_timesteps      | 180224        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0059453305  |\n",
            "|    clip_fraction        | 0.0433        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0.629         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.34          |\n",
            "|    n_updates            | 870           |\n",
            "|    policy_gradient_loss | -0.00404      |\n",
            "|    reward               | -0.0072948392 |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 0.439         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 76           |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038739908 |\n",
            "|    clip_fraction        | 0.0174       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.681        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.324        |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    reward               | -0.029259475 |\n",
            "|    std                  | 0.998        |\n",
            "|    value_loss           | 0.579        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073558353 |\n",
            "|    clip_fraction        | 0.0758       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -0.0391      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0327       |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.000955    |\n",
            "|    reward               | 0.042983044  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.0404       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2380        |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002584586 |\n",
            "|    clip_fraction        | 0.0412      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.64        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.325       |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.000654   |\n",
            "|    reward               | 0.2969974   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 0.646       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 79           |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021481486 |\n",
            "|    clip_fraction        | 0.0194       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.741        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0747       |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    reward               | -0.028828986 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.286        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 79          |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004032265 |\n",
            "|    clip_fraction        | 0.0233      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.469       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.255       |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | -0.00366    |\n",
            "|    reward               | 0.001270913 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 0.345       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 80          |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006060167 |\n",
            "|    clip_fraction        | 0.0291      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.696       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.342       |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00357    |\n",
            "|    reward               | -0.13706543 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 0.611       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073759896 |\n",
            "|    clip_fraction        | 0.0861       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | -0.0219      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0156      |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    reward               | 0.056915477  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.036        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037668408 |\n",
            "|    clip_fraction        | 0.0281       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.69         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.35         |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    reward               | -0.01803132  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.625        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 83           |\n",
            "|    total_timesteps      | 198656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035159995 |\n",
            "|    clip_fraction        | 0.0257       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.76         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.169        |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    reward               | -0.028218927 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.289        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 84           |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041896068 |\n",
            "|    clip_fraction        | 0.027        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.136        |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.00356     |\n",
            "|    reward               | 0.018256614  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.346        |\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 282         |\n",
            "|    time_elapsed    | 48          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 19.8        |\n",
            "|    critic_loss     | 0.233       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 95          |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 16          |\n",
            "|    critic_loss     | 0.104       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 170\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 143         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 11          |\n",
            "|    critic_loss     | 0.229       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 191         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 7.44        |\n",
            "|    critic_loss     | 0.0701      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 180\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 20          |\n",
            "|    fps             | 282         |\n",
            "|    time_elapsed    | 240         |\n",
            "|    total_timesteps | 67940       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 4.96        |\n",
            "|    critic_loss     | 0.162       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 67839       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=80000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 216         |\n",
            "|    time_elapsed    | 62          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 165         |\n",
            "|    critic_loss     | 1.23        |\n",
            "|    ent_coef        | 0.385       |\n",
            "|    ent_coef_loss   | 9.04        |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 150\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 227         |\n",
            "|    time_elapsed    | 119         |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 565         |\n",
            "|    critic_loss     | 10.3        |\n",
            "|    ent_coef        | 1.5         |\n",
            "|    ent_coef_loss   | -3.79       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 225         |\n",
            "|    time_elapsed    | 180         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 2.11e+03    |\n",
            "|    critic_loss     | 124         |\n",
            "|    ent_coef        | 5.83        |\n",
            "|    ent_coef_loss   | -16.5       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 222         |\n",
            "|    time_elapsed    | 243         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 8.28e+03    |\n",
            "|    critic_loss     | 443         |\n",
            "|    ent_coef        | 22.7        |\n",
            "|    ent_coef_loss   | -29.5       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=60000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
