{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data_single.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "11\n",
            "Stock Dimension: 1, State Space: 11\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(len(INDICATORS))\n",
        "print(state_space)\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 10,\n",
        "    \"initial_amount\": 10000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1686         |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 0            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.5         |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | 0.000213     |\n",
            "|    reward             | 0.0001430151 |\n",
            "|    std                | 1.09         |\n",
            "|    value_loss         | 2.53e-08     |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1731           |\n",
            "|    iterations         | 200            |\n",
            "|    time_elapsed       | 0              |\n",
            "|    total_timesteps    | 1000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.56          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 199            |\n",
            "|    policy_loss        | 0.000287       |\n",
            "|    reward             | -5.4431828e-05 |\n",
            "|    std                | 1.15           |\n",
            "|    value_loss         | 8.93e-08       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1776           |\n",
            "|    iterations         | 300            |\n",
            "|    time_elapsed       | 0              |\n",
            "|    total_timesteps    | 1500           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.62          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 299            |\n",
            "|    policy_loss        | 0.000349       |\n",
            "|    reward             | -2.3858642e-05 |\n",
            "|    std                | 1.22           |\n",
            "|    value_loss         | 9.12e-08       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1798           |\n",
            "|    iterations         | 400            |\n",
            "|    time_elapsed       | 1              |\n",
            "|    total_timesteps    | 2000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -1.68          |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 399            |\n",
            "|    policy_loss        | -0.000308      |\n",
            "|    reward             | -0.00014200764 |\n",
            "|    std                | 1.3            |\n",
            "|    value_loss         | 7.35e-08       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1816          |\n",
            "|    iterations         | 500           |\n",
            "|    time_elapsed       | 1             |\n",
            "|    total_timesteps    | 2500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.75         |\n",
            "|    explained_variance | -2.38e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 499           |\n",
            "|    policy_loss        | 0.00878       |\n",
            "|    reward             | -0.0013436966 |\n",
            "|    std                | 1.39          |\n",
            "|    value_loss         | 2.54e-05      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1793         |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 1            |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.82        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | -0.0393      |\n",
            "|    reward             | 0.0012116415 |\n",
            "|    std                | 1.49         |\n",
            "|    value_loss         | 0.000337     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1800          |\n",
            "|    iterations         | 700           |\n",
            "|    time_elapsed       | 1             |\n",
            "|    total_timesteps    | 3500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.86         |\n",
            "|    explained_variance | -1.19e-07     |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 699           |\n",
            "|    policy_loss        | 0.000149      |\n",
            "|    reward             | 2.1924496e-09 |\n",
            "|    std                | 1.55          |\n",
            "|    value_loss         | 1.78e-08      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1795          |\n",
            "|    iterations         | 800           |\n",
            "|    time_elapsed       | 2             |\n",
            "|    total_timesteps    | 4000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.91         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 799           |\n",
            "|    policy_loss        | -0.000549     |\n",
            "|    reward             | 0.00051831576 |\n",
            "|    std                | 1.63          |\n",
            "|    value_loss         | 1.1e-06       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1793          |\n",
            "|    iterations         | 900           |\n",
            "|    time_elapsed       | 2             |\n",
            "|    total_timesteps    | 4500          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -1.97         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 899           |\n",
            "|    policy_loss        | -0.00173      |\n",
            "|    reward             | 3.8618527e-06 |\n",
            "|    std                | 1.73          |\n",
            "|    value_loss         | 7.06e-07      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1790          |\n",
            "|    iterations         | 1000          |\n",
            "|    time_elapsed       | 2             |\n",
            "|    total_timesteps    | 5000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.03         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 999           |\n",
            "|    policy_loss        | -0.00158      |\n",
            "|    reward             | 0.00029093763 |\n",
            "|    std                | 1.85          |\n",
            "|    value_loss         | 1.63e-06      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1784         |\n",
            "|    iterations         | 1100         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 5500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.1         |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1099         |\n",
            "|    policy_loss        | 0.00132      |\n",
            "|    reward             | 0.0031502142 |\n",
            "|    std                | 1.97         |\n",
            "|    value_loss         | 1.38e-05     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1787          |\n",
            "|    iterations         | 1200          |\n",
            "|    time_elapsed       | 3             |\n",
            "|    total_timesteps    | 6000          |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.13         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 1199          |\n",
            "|    policy_loss        | 0.0713        |\n",
            "|    reward             | 0.00018226866 |\n",
            "|    std                | 2.04          |\n",
            "|    value_loss         | 0.000838      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1792         |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.16        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | -0.00495     |\n",
            "|    reward             | 0.0010383851 |\n",
            "|    std                | 2.1          |\n",
            "|    value_loss         | 8.39e-06     |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                 |                |\n",
            "|    fps                | 1793           |\n",
            "|    iterations         | 1400           |\n",
            "|    time_elapsed       | 3              |\n",
            "|    total_timesteps    | 7000           |\n",
            "| train/                |                |\n",
            "|    entropy_loss       | -2.2           |\n",
            "|    explained_variance | 0              |\n",
            "|    learning_rate      | 0.0007         |\n",
            "|    n_updates          | 1399           |\n",
            "|    policy_loss        | 0.000599       |\n",
            "|    reward             | -0.00034603264 |\n",
            "|    std                | 2.18           |\n",
            "|    value_loss         | 1.16e-07       |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                 |                 |\n",
            "|    fps                | 1796            |\n",
            "|    iterations         | 1500            |\n",
            "|    time_elapsed       | 4               |\n",
            "|    total_timesteps    | 7500            |\n",
            "| train/                |                 |\n",
            "|    entropy_loss       | -2.24           |\n",
            "|    explained_variance | 0               |\n",
            "|    learning_rate      | 0.0007          |\n",
            "|    n_updates          | 1499            |\n",
            "|    policy_loss        | 0.000274        |\n",
            "|    reward             | -0.000112131136 |\n",
            "|    std                | 2.28            |\n",
            "|    value_loss         | 4.22e-08        |\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_a2c \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_a2c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma2c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_a2c \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[1;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:348\u001b[0m, in \u001b[0;36mStockTradingEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    343\u001b[0m end_total_asset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    344\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m    346\u001b[0m )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_memory\u001b[38;5;241m.\u001b[39mappend(end_total_asset)\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_memory\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m end_total_asset \u001b[38;5;241m-\u001b[39m begin_total_asset\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards_memory\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:485\u001b[0m, in \u001b[0;36mStockTradingEnv._get_date\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_date\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    486\u001b[0m         date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/algorithms.py:436\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    435\u001b[0m original \u001b[38;5;241m=\u001b[39m values\n\u001b[0;32m--> 436\u001b[0m hashtable, values \u001b[38;5;241m=\u001b[39m \u001b[43m_get_hashtable_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/algorithms.py:275\u001b[0m, in \u001b[0;36m_get_hashtable_algo\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mvalues : ndarray\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m values \u001b[38;5;241m=\u001b[39m _ensure_data(values)\n\u001b[0;32m--> 275\u001b[0m ndtype \u001b[38;5;241m=\u001b[39m \u001b[43m_check_object_for_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m hashtable \u001b[38;5;241m=\u001b[39m _hashtables[ndtype]\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hashtable, values\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/algorithms.py:292\u001b[0m, in \u001b[0;36m_check_object_for_strings\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_object_for_strings\u001b[39m(values: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Check if we can use string hashtable instead of object hashtable.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    str\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     ndtype \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;66;03m# it's cheaper to use a String Hash Table than Object; we infer\u001b[39;00m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;66;03m# including nulls because that is the only difference between\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# StringHashTable and ObjectHashtable\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_string_array(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/numpy/core/_dtype.py:353\u001b[0m, in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39misbuiltin \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# user dtypes don't promise to do anything special\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    354\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(dtype)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mvoid):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# historically, void subclasses preserve their name, eg `record64`\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=60000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 242         |\n",
            "|    time_elapsed    | 55          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -28.9       |\n",
            "|    critic_loss     | 0.0756      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 242         |\n",
            "|    time_elapsed    | 111         |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -14.4       |\n",
            "|    critic_loss     | 0.056       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 110\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 236         |\n",
            "|    time_elapsed    | 172         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -8.23       |\n",
            "|    critic_loss     | 0.0772      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 240         |\n",
            "|    time_elapsed    | 225         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -4.83       |\n",
            "|    critic_loss     | 0.0471      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 120\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 20          |\n",
            "|    fps             | 244         |\n",
            "|    time_elapsed    | 278         |\n",
            "|    total_timesteps | 67940       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | -2.93       |\n",
            "|    critic_loss     | 0.029       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 67839       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=80000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 2988        |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 0           |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | 0.005603891 |\n",
            "------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2560          |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 1             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.004120747   |\n",
            "|    clip_fraction        | 0.0564        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -0.0863       |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0347       |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.00489      |\n",
            "|    reward               | 0.00079246575 |\n",
            "|    std                  | 0.993         |\n",
            "|    value_loss           | 0.00643       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2480          |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 2             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.003952629   |\n",
            "|    clip_fraction        | 0.033         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -0.145        |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0186       |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.00196      |\n",
            "|    reward               | -0.0017937456 |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 0.0064        |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2450           |\n",
            "|    iterations           | 4              |\n",
            "|    time_elapsed         | 3              |\n",
            "|    total_timesteps      | 8192           |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.00042891514  |\n",
            "|    clip_fraction        | 0.0146         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.43          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0144        |\n",
            "|    n_updates            | 30             |\n",
            "|    policy_gradient_loss | -0.000454      |\n",
            "|    reward               | -2.1521148e-06 |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 0.000843       |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2390           |\n",
            "|    iterations           | 5              |\n",
            "|    time_elapsed         | 4              |\n",
            "|    total_timesteps      | 10240          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0025094675   |\n",
            "|    clip_fraction        | 0.0169         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.44          |\n",
            "|    explained_variance   | -0.339         |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0141        |\n",
            "|    n_updates            | 40             |\n",
            "|    policy_gradient_loss | -0.00154       |\n",
            "|    reward               | -1.2152772e-05 |\n",
            "|    std                  | 1.03           |\n",
            "|    value_loss           | 0.00278        |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2381           |\n",
            "|    iterations           | 6              |\n",
            "|    time_elapsed         | 5              |\n",
            "|    total_timesteps      | 12288          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.00900709     |\n",
            "|    clip_fraction        | 0.0834         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.45          |\n",
            "|    explained_variance   | -0.563         |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0231        |\n",
            "|    n_updates            | 50             |\n",
            "|    policy_gradient_loss | -0.00305       |\n",
            "|    reward               | -0.00030453186 |\n",
            "|    std                  | 1.04           |\n",
            "|    value_loss           | 0.00165        |\n",
            "--------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2380        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002452828 |\n",
            "|    clip_fraction        | 0.025       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | -0.0158     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00167    |\n",
            "|    reward               | 0.001156289 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 0.000398    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007454967 |\n",
            "|    clip_fraction        | 0.0144       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | -0.672       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0153      |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00242     |\n",
            "|    reward               | 0.0017727182 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.00164      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2388          |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0012710853  |\n",
            "|    clip_fraction        | 0.00278       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.43         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0172       |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | 0.000129      |\n",
            "|    reward               | 0.00042656407 |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 0.000271      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 10\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 15822.46\n",
            "total_reward: 5822.46\n",
            "total_cost: 797.39\n",
            "total_trades: 2938\n",
            "Sharpe: 0.515\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2390         |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023318967 |\n",
            "|    clip_fraction        | 0.0641       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | -0.343       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.034       |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    reward               | 0.0021522713 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.000591     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2392          |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0027725913  |\n",
            "|    clip_fraction        | 0.000928      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.43         |\n",
            "|    explained_variance   | -0.471        |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0246       |\n",
            "|    n_updates            | 100           |\n",
            "|    policy_gradient_loss | -0.000209     |\n",
            "|    reward               | -3.073696e-05 |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 0.000837      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2393          |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.001866259   |\n",
            "|    clip_fraction        | 0.0084        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.44         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00738      |\n",
            "|    n_updates            | 110           |\n",
            "|    policy_gradient_loss | -7.98e-05     |\n",
            "|    reward               | 0.00022394895 |\n",
            "|    std                  | 1.03          |\n",
            "|    value_loss           | 4.87e-05      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2397         |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073384875 |\n",
            "|    clip_fraction        | 0.0801       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0247      |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    reward               | -0.000980425 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.000191     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2399          |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 11            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.004644719   |\n",
            "|    clip_fraction        | 0.0226        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.45         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00907      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -0.00188      |\n",
            "|    reward               | -0.0026231806 |\n",
            "|    std                  | 1.03          |\n",
            "|    value_loss           | 0.000115      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2400          |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0025737607  |\n",
            "|    clip_fraction        | 0.00332       |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.44         |\n",
            "|    explained_variance   | -0.731        |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.0181       |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.00102      |\n",
            "|    reward               | -6.054938e-05 |\n",
            "|    std                  | 1.01          |\n",
            "|    value_loss           | 0.000705      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2402         |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012055208 |\n",
            "|    clip_fraction        | 0.00728      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | -0.574       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.016       |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00135     |\n",
            "|    reward               | -0.012961983 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.00459      |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2404           |\n",
            "|    iterations           | 17             |\n",
            "|    time_elapsed         | 14             |\n",
            "|    total_timesteps      | 34816          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.00070836383  |\n",
            "|    clip_fraction        | 0.0042         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.44          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.00795       |\n",
            "|    n_updates            | 160            |\n",
            "|    policy_gradient_loss | -2.05e-05      |\n",
            "|    reward               | -0.00042043687 |\n",
            "|    std                  | 1.02           |\n",
            "|    value_loss           | 0.000109       |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2369           |\n",
            "|    iterations           | 18             |\n",
            "|    time_elapsed         | 15             |\n",
            "|    total_timesteps      | 36864          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0015930757   |\n",
            "|    clip_fraction        | 0.00625        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.44          |\n",
            "|    explained_variance   | -0.0994        |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0108        |\n",
            "|    n_updates            | 170            |\n",
            "|    policy_gradient_loss | -0.000725      |\n",
            "|    reward               | -8.9019406e-05 |\n",
            "|    std                  | 1.03           |\n",
            "|    value_loss           | 0.000575       |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2371           |\n",
            "|    iterations           | 19             |\n",
            "|    time_elapsed         | 16             |\n",
            "|    total_timesteps      | 38912          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0044856477   |\n",
            "|    clip_fraction        | 0.0146         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.45          |\n",
            "|    explained_variance   | -1.19e-07      |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.00139       |\n",
            "|    n_updates            | 180            |\n",
            "|    policy_gradient_loss | -0.000596      |\n",
            "|    reward               | -0.00045820625 |\n",
            "|    std                  | 1.03           |\n",
            "|    value_loss           | 5.68e-05       |\n",
            "--------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2373           |\n",
            "|    iterations           | 20             |\n",
            "|    time_elapsed         | 17             |\n",
            "|    total_timesteps      | 40960          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.00022886149  |\n",
            "|    clip_fraction        | 0.00239        |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.45          |\n",
            "|    explained_variance   | -3.14          |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | -0.0179        |\n",
            "|    n_updates            | 190            |\n",
            "|    policy_gradient_loss | -0.00204       |\n",
            "|    reward               | -0.00013615136 |\n",
            "|    std                  | 1.04           |\n",
            "|    value_loss           | 0.000298       |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017320254 |\n",
            "|    clip_fraction        | 0.0251       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | -0.833       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0152      |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    reward               | 0.0036771582 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.000747     |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2378           |\n",
            "|    iterations           | 22             |\n",
            "|    time_elapsed         | 18             |\n",
            "|    total_timesteps      | 45056          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.0064572347   |\n",
            "|    clip_fraction        | 0.0521         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.46          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 0.000186       |\n",
            "|    n_updates            | 210            |\n",
            "|    policy_gradient_loss | -0.00328       |\n",
            "|    reward               | -0.00045050945 |\n",
            "|    std                  | 1.04           |\n",
            "|    value_loss           | 3.66e-05       |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019464114 |\n",
            "|    clip_fraction        | 0.00557      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | -0.284       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0199      |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    reward               | -0.06900014  |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.00343      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031995706 |\n",
            "|    clip_fraction        | 0.0118       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | -0.267       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0121      |\n",
            "|    n_updates            | 230          |\n",
            "|    policy_gradient_loss | -0.00208     |\n",
            "|    reward               | -0.046655435 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.0194       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004374356  |\n",
            "|    clip_fraction        | 0.0515       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0758       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0233      |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00451     |\n",
            "|    reward               | 0.0028002928 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.023        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2376        |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 22          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003991144 |\n",
            "|    clip_fraction        | 0.0319      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | -0.0578     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0469      |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00548    |\n",
            "|    reward               | 0.19414905  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 0.113       |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 20\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 179557.60\n",
            "total_reward: 169557.60\n",
            "total_cost: 758.17\n",
            "total_trades: 3132\n",
            "Sharpe: 0.949\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 23           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024911123 |\n",
            "|    clip_fraction        | 0.0103       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | -0.268       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0071      |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    reward               | -0.030199233 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.017        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013766338 |\n",
            "|    clip_fraction        | 0.00688      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0276       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.126        |\n",
            "|    n_updates            | 270          |\n",
            "|    policy_gradient_loss | -0.000655    |\n",
            "|    reward               | 0.07679398   |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.29         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2376        |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002621593 |\n",
            "|    clip_fraction        | 0.0121      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.0403      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0636      |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.00357    |\n",
            "|    reward               | 0.042144813 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 0.132       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048177447 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.041        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0715       |\n",
            "|    n_updates            | 290          |\n",
            "|    policy_gradient_loss | -0.0048      |\n",
            "|    reward               | 0.027007166  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.18         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041727796 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0626       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.16         |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00576     |\n",
            "|    reward               | -0.08730235  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.407        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057397755 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | -0.192       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -6.84e-05    |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.00522     |\n",
            "|    reward               | -0.01151447  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.0271       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 28           |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031243495 |\n",
            "|    clip_fraction        | 0.00806      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.069        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.215        |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    reward               | 0.12449526   |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.43         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041043703 |\n",
            "|    clip_fraction        | 0.0232       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.014        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.107        |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.00223     |\n",
            "|    reward               | 0.030125612  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.204        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2386         |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025721367 |\n",
            "|    clip_fraction        | 0.0188       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.1          |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0843       |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.0024      |\n",
            "|    reward               | -0.02057071  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.219        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2386         |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024619238 |\n",
            "|    clip_fraction        | 0.0234       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0655       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.211        |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.00236     |\n",
            "|    reward               | -0.11863804  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.357        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2388        |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.00410283  |\n",
            "|    clip_fraction        | 0.0341      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | -0.108      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.00897     |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    reward               | -0.03599379 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 0.0284      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036628742 |\n",
            "|    clip_fraction        | 0.03         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0795       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.233        |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.00376     |\n",
            "|    reward               | -0.34804794  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.393        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006162543  |\n",
            "|    clip_fraction        | 0.0525       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.00526      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0837       |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.00181     |\n",
            "|    reward               | -0.034719743 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.217        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2389          |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0080731865  |\n",
            "|    clip_fraction        | 0.0696        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.44         |\n",
            "|    explained_variance   | 0.117         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.123         |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -0.00334      |\n",
            "|    reward               | -0.0030935905 |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 0.186         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2391         |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040498027 |\n",
            "|    clip_fraction        | 0.037        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.0872       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.163        |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00212     |\n",
            "|    reward               | -0.09918529  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.37         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074220235 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | -0.0173      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0254       |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | 0.00172      |\n",
            "|    reward               | 0.05247212   |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.0286       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2387         |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 88064        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041161804 |\n",
            "|    clip_fraction        | 0.0516       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.129        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.191        |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    reward               | -0.042455964 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.358        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 30\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 211268.83\n",
            "total_reward: 201268.83\n",
            "total_cost: 831.97\n",
            "total_trades: 3176\n",
            "Sharpe: 0.998\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067643123 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | -0.0484      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0916       |\n",
            "|    n_updates            | 430          |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    reward               | 0.010322104  |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.237        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2387         |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 38           |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026793731 |\n",
            "|    clip_fraction        | 0.063        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.102        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0667       |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    reward               | -0.017898688 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.177        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071438514 |\n",
            "|    clip_fraction        | 0.0651       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.172        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.146        |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.00302     |\n",
            "|    reward               | -0.32666332  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.368        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2388         |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052746665 |\n",
            "|    clip_fraction        | 0.0496       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | -0.0115      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0111       |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | 0.00125      |\n",
            "|    reward               | 0.01338394   |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.0362       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040575713 |\n",
            "|    clip_fraction        | 0.0342       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.161        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.113        |\n",
            "|    n_updates            | 470          |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    reward               | 0.101842254  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.329        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 42           |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044721747 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.0307       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0992       |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00195     |\n",
            "|    reward               | -0.021976952 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.24         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 42           |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062044766 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.339        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0577       |\n",
            "|    n_updates            | 490          |\n",
            "|    policy_gradient_loss | -0.00108     |\n",
            "|    reward               | 0.005528304  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.139        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2384         |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 104448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073244087 |\n",
            "|    clip_fraction        | 0.0918       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.202        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.157        |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -0.00457     |\n",
            "|    reward               | 0.18158929   |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.364        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 106496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067687756 |\n",
            "|    clip_fraction        | 0.0611       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | -0.0138      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.012        |\n",
            "|    n_updates            | 510          |\n",
            "|    policy_gradient_loss | -0.00122     |\n",
            "|    reward               | -0.007607796 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.0523       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009809488 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.337       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.173       |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.00195    |\n",
            "|    reward               | -0.45217237 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 0.353       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072648223 |\n",
            "|    clip_fraction        | 0.0456       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.158        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.13         |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    reward               | -0.031124225 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.278        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2381        |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 47          |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009017745 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.566       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0331      |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00624    |\n",
            "|    reward               | 0.019235954 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 0.13        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2381        |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 48          |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011070438 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.409       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.128       |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.00204    |\n",
            "|    reward               | 0.013546458 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 0.371       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053773783 |\n",
            "|    clip_fraction        | 0.0717       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | -0.00745     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0125       |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | 0.000924     |\n",
            "|    reward               | 0.07105535   |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.0519       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2383        |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007137137 |\n",
            "|    clip_fraction        | 0.0795      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.357       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.113       |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00224    |\n",
            "|    reward               | -0.31141335 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 0.369       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 120832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074404036 |\n",
            "|    clip_fraction        | 0.0882       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.108        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.123        |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00258     |\n",
            "|    reward               | 0.09386189   |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.294        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 40\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 217595.11\n",
            "total_reward: 207595.11\n",
            "total_cost: 898.21\n",
            "total_trades: 3138\n",
            "Sharpe: 1.017\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.010343857  |\n",
            "|    clip_fraction        | 0.0711       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.47        |\n",
            "|    explained_variance   | 0.613        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0194       |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.00681     |\n",
            "|    reward               | 0.0065074563 |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.0797       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2383        |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.00673755  |\n",
            "|    clip_fraction        | 0.0635      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.349       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.153       |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00113    |\n",
            "|    reward               | -0.11494312 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 0.38        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071640653 |\n",
            "|    clip_fraction        | 0.0836       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.47        |\n",
            "|    explained_variance   | -0.0208      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0125       |\n",
            "|    n_updates            | 610          |\n",
            "|    policy_gradient_loss | 0.00148      |\n",
            "|    reward               | 0.044215642  |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.0546       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 54           |\n",
            "|    total_timesteps      | 129024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008866714  |\n",
            "|    clip_fraction        | 0.0809       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.47        |\n",
            "|    explained_variance   | 0.406        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.151        |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00018     |\n",
            "|    reward               | -0.047366608 |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.33         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2383        |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 54          |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008795101 |\n",
            "|    clip_fraction        | 0.0781      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.182       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.248       |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.00101    |\n",
            "|    reward               | 0.058578525 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 0.346       |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2381          |\n",
            "|    iterations           | 65            |\n",
            "|    time_elapsed         | 55            |\n",
            "|    total_timesteps      | 133120        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.011189355   |\n",
            "|    clip_fraction        | 0.0837        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.48         |\n",
            "|    explained_variance   | 0.507         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.00669      |\n",
            "|    n_updates            | 640           |\n",
            "|    policy_gradient_loss | -0.00205      |\n",
            "|    reward               | -0.0011707873 |\n",
            "|    std                  | 1.07          |\n",
            "|    value_loss           | 0.0482        |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2381        |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011317916 |\n",
            "|    clip_fraction        | 0.0664      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.465       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.109       |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.000207   |\n",
            "|    reward               | 0.13064384  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 0.342       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2379        |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 57          |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006764221 |\n",
            "|    clip_fraction        | 0.0616      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | -0.0509     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.00913     |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | 0.000124    |\n",
            "|    reward               | 0.020906536 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 0.0508      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 2379       |\n",
            "|    iterations           | 68         |\n",
            "|    time_elapsed         | 58         |\n",
            "|    total_timesteps      | 139264     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00578692 |\n",
            "|    clip_fraction        | 0.0399     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.49      |\n",
            "|    explained_variance   | 0.376      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.131      |\n",
            "|    n_updates            | 670        |\n",
            "|    policy_gradient_loss | -0.00281   |\n",
            "|    reward               | -0.8089782 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 0.322      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 59           |\n",
            "|    total_timesteps      | 141312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008309982  |\n",
            "|    clip_fraction        | 0.0679       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.49        |\n",
            "|    explained_variance   | 0.355        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.202        |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00169     |\n",
            "|    reward               | -0.008323582 |\n",
            "|    std                  | 1.08         |\n",
            "|    value_loss           | 0.369        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 143360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.005885833  |\n",
            "|    clip_fraction        | 0.0326       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.48        |\n",
            "|    explained_variance   | 0.152        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0213      |\n",
            "|    n_updates            | 690          |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    reward               | -0.051807713 |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.0208       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.008661149  |\n",
            "|    clip_fraction        | 0.0599       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.47        |\n",
            "|    explained_variance   | 0.431        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.171        |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | 0.000403     |\n",
            "|    reward               | -0.009544751 |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.366        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 61           |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038025498 |\n",
            "|    clip_fraction        | 0.0435       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.159        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0247       |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -0.000278    |\n",
            "|    reward               | 0.07298432   |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.0654       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 62           |\n",
            "|    total_timesteps      | 149504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.007970949  |\n",
            "|    clip_fraction        | 0.0752       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.512        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0824       |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.00375     |\n",
            "|    reward               | 0.0005844156 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.356        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 63           |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.009788868  |\n",
            "|    clip_fraction        | 0.07         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.281        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.253        |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | 0.000685     |\n",
            "|    reward               | -0.028232299 |\n",
            "|    std                  | 1.05         |\n",
            "|    value_loss           | 0.427        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 64          |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007989257 |\n",
            "|    clip_fraction        | 0.0783      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.01        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.00683     |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | 0.000426    |\n",
            "|    reward               | 0.028098179 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 0.0217      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 65           |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.012792948  |\n",
            "|    clip_fraction        | 0.0611       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.468        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.14         |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.000753    |\n",
            "|    reward               | -0.040435266 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.345        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 50\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 243778.74\n",
            "total_reward: 233778.74\n",
            "total_cost: 322.17\n",
            "total_trades: 3248\n",
            "Sharpe: 1.024\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 157696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045940615 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.419        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.000296     |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    reward               | -0.00109736  |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.0693       |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 2382           |\n",
            "|    iterations           | 78             |\n",
            "|    time_elapsed         | 67             |\n",
            "|    total_timesteps      | 159744         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.007905509    |\n",
            "|    clip_fraction        | 0.0555         |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.43          |\n",
            "|    explained_variance   | 0.4            |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 0.168          |\n",
            "|    n_updates            | 770            |\n",
            "|    policy_gradient_loss | -0.00175       |\n",
            "|    reward               | -0.00040899304 |\n",
            "|    std                  | 1.01           |\n",
            "|    value_loss           | 0.398          |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 67           |\n",
            "|    total_timesteps      | 161792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067858845 |\n",
            "|    clip_fraction        | 0.0445       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.528        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.22         |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | -0.00296     |\n",
            "|    reward               | 0.030445127  |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 0.465        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2381        |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009203541 |\n",
            "|    clip_fraction        | 0.0632      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.684       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.00156     |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00351    |\n",
            "|    reward               | 0.056067746 |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 0.0268      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 69           |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030424686 |\n",
            "|    clip_fraction        | 0.0407       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.411        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.254        |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    reward               | -0.17319632  |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 0.603        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029948992 |\n",
            "|    clip_fraction        | 0.0348       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.204        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0737       |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    reward               | 0.007323828  |\n",
            "|    std                  | 0.999        |\n",
            "|    value_loss           | 0.174        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 71           |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048539797 |\n",
            "|    clip_fraction        | 0.0294       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.462        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.18         |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.00254     |\n",
            "|    reward               | 0.0022255182 |\n",
            "|    std                  | 0.993        |\n",
            "|    value_loss           | 0.362        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061796154 |\n",
            "|    clip_fraction        | 0.0373       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.51         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.352        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    reward               | 0.0024419308 |\n",
            "|    std                  | 0.993        |\n",
            "|    value_loss           | 0.612        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004877759  |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | -0.109       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00833     |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    reward               | -0.072517656 |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.0336       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 86           |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040771295 |\n",
            "|    clip_fraction        | 0.0155       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.53         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.242        |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | -0.00147     |\n",
            "|    reward               | 0.2150197    |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 0.584        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004243619 |\n",
            "|    clip_fraction        | 0.0198      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.364       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.071       |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.000589   |\n",
            "|    reward               | 0.04124713  |\n",
            "|    std                  | 0.99        |\n",
            "|    value_loss           | 0.21        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 75           |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043977844 |\n",
            "|    clip_fraction        | 0.0188       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.619        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.199        |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    reward               | -0.007501981 |\n",
            "|    std                  | 0.991        |\n",
            "|    value_loss           | 0.423        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 76           |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012500177 |\n",
            "|    clip_fraction        | 0.00752      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.59         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.351        |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.000863    |\n",
            "|    reward               | -0.028840369 |\n",
            "|    std                  | 0.992        |\n",
            "|    value_loss           | 0.605        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2383         |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062409975 |\n",
            "|    clip_fraction        | 0.0373       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | -0.0513      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.00735     |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.00128     |\n",
            "|    reward               | 0.042872384  |\n",
            "|    std                  | 0.974        |\n",
            "|    value_loss           | 0.0366       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 78           |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040946812 |\n",
            "|    clip_fraction        | 0.0255       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.543        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.345        |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    reward               | 0.29918334   |\n",
            "|    std                  | 0.975        |\n",
            "|    value_loss           | 0.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 79           |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019523589 |\n",
            "|    clip_fraction        | 0.0266       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.551        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0633       |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00119     |\n",
            "|    reward               | -0.027924476 |\n",
            "|    std                  | 0.981        |\n",
            "|    value_loss           | 0.263        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 60\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 274713.56\n",
            "total_reward: 264713.56\n",
            "total_cost: 304.40\n",
            "total_trades: 3254\n",
            "Sharpe: 1.044\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 79           |\n",
            "|    total_timesteps      | 190464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072455686 |\n",
            "|    clip_fraction        | 0.0408       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.533        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.09         |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -0.000783    |\n",
            "|    reward               | 0.0012202626 |\n",
            "|    std                  | 0.979        |\n",
            "|    value_loss           | 0.361        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2382         |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 192512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061441856 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.566        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.377        |\n",
            "|    n_updates            | 930          |\n",
            "|    policy_gradient_loss | -0.0043      |\n",
            "|    reward               | -0.13416483  |\n",
            "|    std                  | 0.981        |\n",
            "|    value_loss           | 0.572        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059515755 |\n",
            "|    clip_fraction        | 0.0493       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.265        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.00257      |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    reward               | 0.057464313  |\n",
            "|    std                  | 0.985        |\n",
            "|    value_loss           | 0.0357       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2373         |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.006709456  |\n",
            "|    clip_fraction        | 0.0409       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.595        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.199        |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    reward               | -0.018186381 |\n",
            "|    std                  | 0.986        |\n",
            "|    value_loss           | 0.527        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2372        |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003952969 |\n",
            "|    clip_fraction        | 0.0182      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.536       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.134       |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.00207    |\n",
            "|    reward               | -0.03033421 |\n",
            "|    std                  | 0.985       |\n",
            "|    value_loss           | 0.317       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2371         |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 84           |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021216415 |\n",
            "|    clip_fraction        | 0.0131       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.57         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.178        |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.000831    |\n",
            "|    reward               | 0.01801552   |\n",
            "|    std                  | 0.984        |\n",
            "|    value_loss           | 0.337        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2372         |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 85           |\n",
            "|    total_timesteps      | 202752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050965166 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.4         |\n",
            "|    explained_variance   | 0.576        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.347        |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.002       |\n",
            "|    reward               | 0.061075624  |\n",
            "|    std                  | 0.987        |\n",
            "|    value_loss           | 0.656        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2372         |\n",
            "|    iterations           | 100          |\n",
            "|    time_elapsed         | 86           |\n",
            "|    total_timesteps      | 204800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071174274 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | -0.164       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | -0.0064      |\n",
            "|    n_updates            | 990          |\n",
            "|    policy_gradient_loss | -0.00244     |\n",
            "|    reward               | 0.06505032   |\n",
            "|    std                  | 0.991        |\n",
            "|    value_loss           | 0.0391       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2373        |\n",
            "|    iterations           | 101         |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 206848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005843498 |\n",
            "|    clip_fraction        | 0.0289      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.574       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.34        |\n",
            "|    n_updates            | 1000        |\n",
            "|    policy_gradient_loss | -0.00293    |\n",
            "|    reward               | 0.44550923  |\n",
            "|    std                  | 0.991       |\n",
            "|    value_loss           | 0.632       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2373         |\n",
            "|    iterations           | 102          |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 208896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022784201 |\n",
            "|    clip_fraction        | 0.0247       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.601        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.129        |\n",
            "|    n_updates            | 1010         |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    reward               | -0.09885475  |\n",
            "|    std                  | 0.99         |\n",
            "|    value_loss           | 0.336        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2374        |\n",
            "|    iterations           | 103         |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 210944      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003937574 |\n",
            "|    clip_fraction        | 0.0382      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.373       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.162       |\n",
            "|    n_updates            | 1020        |\n",
            "|    policy_gradient_loss | -0.00245    |\n",
            "|    reward               | -0.04975763 |\n",
            "|    std                  | 0.988       |\n",
            "|    value_loss           | 0.349       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2374        |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 89          |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003641653 |\n",
            "|    clip_fraction        | 0.0208      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.608       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.369       |\n",
            "|    n_updates            | 1030        |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    reward               | 0.005243117 |\n",
            "|    std                  | 0.986       |\n",
            "|    value_loss           | 0.694       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 105          |\n",
            "|    time_elapsed         | 90           |\n",
            "|    total_timesteps      | 215040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053849607 |\n",
            "|    clip_fraction        | 0.0621       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.00478      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.024        |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | -0.00025     |\n",
            "|    reward               | 0.106566876  |\n",
            "|    std                  | 0.991        |\n",
            "|    value_loss           | 0.0435       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2374        |\n",
            "|    iterations           | 106         |\n",
            "|    time_elapsed         | 91          |\n",
            "|    total_timesteps      | 217088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006112077 |\n",
            "|    clip_fraction        | 0.0552      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.652       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.36        |\n",
            "|    n_updates            | 1050        |\n",
            "|    policy_gradient_loss | -0.00382    |\n",
            "|    reward               | 0.11057224  |\n",
            "|    std                  | 0.99        |\n",
            "|    value_loss           | 0.679       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 219136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013827186 |\n",
            "|    clip_fraction        | 0.0147       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.697        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.149        |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    reward               | -0.019830523 |\n",
            "|    std                  | 0.988        |\n",
            "|    value_loss           | 0.393        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2374         |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 93           |\n",
            "|    total_timesteps      | 221184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020567111 |\n",
            "|    clip_fraction        | 0.0175       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | 0.468        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.237        |\n",
            "|    n_updates            | 1070         |\n",
            "|    policy_gradient_loss | -0.00107     |\n",
            "|    reward               | 0.06939185   |\n",
            "|    std                  | 0.991        |\n",
            "|    value_loss           | 0.335        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2375        |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 93          |\n",
            "|    total_timesteps      | 223232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002450787 |\n",
            "|    clip_fraction        | 0.0119      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.692       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.302       |\n",
            "|    n_updates            | 1080        |\n",
            "|    policy_gradient_loss | -0.0019     |\n",
            "|    reward               | 0.023991426 |\n",
            "|    std                  | 0.993       |\n",
            "|    value_loss           | 0.672       |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 70\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 309040.76\n",
            "total_reward: 299040.76\n",
            "total_cost: 117.33\n",
            "total_trades: 3311\n",
            "Sharpe: 1.062\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2375        |\n",
            "|    iterations           | 110         |\n",
            "|    time_elapsed         | 94          |\n",
            "|    total_timesteps      | 225280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004402786 |\n",
            "|    clip_fraction        | 0.0369      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.0088      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0191      |\n",
            "|    n_updates            | 1090        |\n",
            "|    policy_gradient_loss | -0.000287   |\n",
            "|    reward               | -0.16253556 |\n",
            "|    std                  | 0.973       |\n",
            "|    value_loss           | 0.0644      |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 95           |\n",
            "|    total_timesteps      | 227328       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035107657 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.681        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.356        |\n",
            "|    n_updates            | 1100         |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    reward               | 0.5193385    |\n",
            "|    std                  | 0.972        |\n",
            "|    value_loss           | 0.734        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 96           |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038178444 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.647        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.234        |\n",
            "|    n_updates            | 1110         |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    reward               | 0.028279314  |\n",
            "|    std                  | 0.971        |\n",
            "|    value_loss           | 0.461        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 231424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039249803 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.502        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.148        |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -0.00173     |\n",
            "|    reward               | 0.014903851  |\n",
            "|    std                  | 0.972        |\n",
            "|    value_loss           | 0.283        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 98           |\n",
            "|    total_timesteps      | 233472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025393404 |\n",
            "|    clip_fraction        | 0.025        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.685        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.385        |\n",
            "|    n_updates            | 1130         |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    reward               | 0.18294951   |\n",
            "|    std                  | 0.972        |\n",
            "|    value_loss           | 0.66         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 99           |\n",
            "|    total_timesteps      | 235520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.003528926  |\n",
            "|    clip_fraction        | 0.032        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.124       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0252       |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.000223    |\n",
            "|    reward               | -0.032167107 |\n",
            "|    std                  | 0.951        |\n",
            "|    value_loss           | 0.0616       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2377        |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 237568      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002360573 |\n",
            "|    clip_fraction        | 0.0206      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.732       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.187       |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | -2.31e-05   |\n",
            "|    reward               | 0.047185898 |\n",
            "|    std                  | 0.951       |\n",
            "|    value_loss           | 0.578       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2377        |\n",
            "|    iterations           | 117         |\n",
            "|    time_elapsed         | 100         |\n",
            "|    total_timesteps      | 239616      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005160756 |\n",
            "|    clip_fraction        | 0.0165      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.711       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.236       |\n",
            "|    n_updates            | 1160        |\n",
            "|    policy_gradient_loss | -0.00155    |\n",
            "|    reward               | 0.0599228   |\n",
            "|    std                  | 0.951       |\n",
            "|    value_loss           | 0.463       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2375         |\n",
            "|    iterations           | 118          |\n",
            "|    time_elapsed         | 101          |\n",
            "|    total_timesteps      | 241664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062622563 |\n",
            "|    clip_fraction        | 0.0511       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | 0.392        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0904       |\n",
            "|    n_updates            | 1170         |\n",
            "|    policy_gradient_loss | -0.00137     |\n",
            "|    reward               | -0.032045063 |\n",
            "|    std                  | 0.954        |\n",
            "|    value_loss           | 0.256        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2375        |\n",
            "|    iterations           | 119         |\n",
            "|    time_elapsed         | 102         |\n",
            "|    total_timesteps      | 243712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005294096 |\n",
            "|    clip_fraction        | 0.0333      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.713       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.296       |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.00252    |\n",
            "|    reward               | 0.16825953  |\n",
            "|    std                  | 0.954       |\n",
            "|    value_loss           | 0.646       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 120          |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 245760       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004405032  |\n",
            "|    clip_fraction        | 0.0466       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.235       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0453       |\n",
            "|    n_updates            | 1190         |\n",
            "|    policy_gradient_loss | 0.000339     |\n",
            "|    reward               | -0.024293553 |\n",
            "|    std                  | 0.956        |\n",
            "|    value_loss           | 0.0957       |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2376        |\n",
            "|    iterations           | 121         |\n",
            "|    time_elapsed         | 104         |\n",
            "|    total_timesteps      | 247808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004253939 |\n",
            "|    clip_fraction        | 0.0214      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.709       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.318       |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | -0.000767   |\n",
            "|    reward               | -0.17791474 |\n",
            "|    std                  | 0.958       |\n",
            "|    value_loss           | 0.572       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2376         |\n",
            "|    iterations           | 122          |\n",
            "|    time_elapsed         | 105          |\n",
            "|    total_timesteps      | 249856       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004514492  |\n",
            "|    clip_fraction        | 0.027        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.662        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.12         |\n",
            "|    n_updates            | 1210         |\n",
            "|    policy_gradient_loss | -0.00127     |\n",
            "|    reward               | -0.016725553 |\n",
            "|    std                  | 0.957        |\n",
            "|    value_loss           | 0.43         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2377          |\n",
            "|    iterations           | 123           |\n",
            "|    time_elapsed         | 105           |\n",
            "|    total_timesteps      | 251904        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.0077385353  |\n",
            "|    clip_fraction        | 0.074         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.539         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.0534        |\n",
            "|    n_updates            | 1220          |\n",
            "|    policy_gradient_loss | -0.00567      |\n",
            "|    reward               | -0.0060738423 |\n",
            "|    std                  | 0.964         |\n",
            "|    value_loss           | 0.205         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2377         |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 106          |\n",
            "|    total_timesteps      | 253952       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019584782 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.714        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.297        |\n",
            "|    n_updates            | 1230         |\n",
            "|    policy_gradient_loss | -0.00354     |\n",
            "|    reward               | 0.09218839   |\n",
            "|    std                  | 0.968        |\n",
            "|    value_loss           | 0.59         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 107          |\n",
            "|    total_timesteps      | 256000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071079256 |\n",
            "|    clip_fraction        | 0.0623       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.41        |\n",
            "|    explained_variance   | -0.159       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0314       |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | 0.000405     |\n",
            "|    reward               | 0.0043929336 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.101        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 108          |\n",
            "|    total_timesteps      | 258048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028931866 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.666        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.175        |\n",
            "|    n_updates            | 1250         |\n",
            "|    policy_gradient_loss | -0.00329     |\n",
            "|    reward               | 0.9911029    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.562        |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 80\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 306709.19\n",
            "total_reward: 296709.19\n",
            "total_cost: 107.12\n",
            "total_trades: 3310\n",
            "Sharpe: 1.060\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2378         |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 109          |\n",
            "|    total_timesteps      | 260096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0074733817 |\n",
            "|    clip_fraction        | 0.0246       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.43        |\n",
            "|    explained_variance   | 0.625        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.305        |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    reward               | -0.042727668 |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 0.505        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 110          |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068964446 |\n",
            "|    clip_fraction        | 0.0606       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | -0.00827     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0278       |\n",
            "|    n_updates            | 1270         |\n",
            "|    policy_gradient_loss | -0.00816     |\n",
            "|    reward               | -0.025363412 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 0.151        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2379         |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 111          |\n",
            "|    total_timesteps      | 264192       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036425693 |\n",
            "|    clip_fraction        | 0.0212       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.44        |\n",
            "|    explained_variance   | 0.574        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.196        |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.000946    |\n",
            "|    reward               | 0.14620616   |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.664        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2379        |\n",
            "|    iterations           | 130         |\n",
            "|    time_elapsed         | 111         |\n",
            "|    total_timesteps      | 266240      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009915996 |\n",
            "|    clip_fraction        | 0.0601      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | -0.0789     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | -0.000845   |\n",
            "|    n_updates            | 1290        |\n",
            "|    policy_gradient_loss | -0.00392    |\n",
            "|    reward               | 0.031112717 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 0.11        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 131          |\n",
            "|    time_elapsed         | 112          |\n",
            "|    total_timesteps      | 268288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048104823 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.598        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.156        |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -0.000906    |\n",
            "|    reward               | 0.99394435   |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.597        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 270336       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048761927 |\n",
            "|    clip_fraction        | 0.0233       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.619        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.259        |\n",
            "|    n_updates            | 1310         |\n",
            "|    policy_gradient_loss | -0.00157     |\n",
            "|    reward               | 0.005135571  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.542        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 114          |\n",
            "|    total_timesteps      | 272384       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048099025 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.43         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0141       |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.00176     |\n",
            "|    reward               | -0.033170227 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.0896       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 115          |\n",
            "|    total_timesteps      | 274432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026407028 |\n",
            "|    clip_fraction        | 0.00703      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.675        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.183        |\n",
            "|    n_updates            | 1330         |\n",
            "|    policy_gradient_loss | -0.00123     |\n",
            "|    reward               | -0.05503926  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.585        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 116          |\n",
            "|    total_timesteps      | 276480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038797718 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | -0.0889      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0328       |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | 0.00165      |\n",
            "|    reward               | -0.058917854 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.104        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 116          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029684347 |\n",
            "|    clip_fraction        | 0.0191       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.707        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.192        |\n",
            "|    n_updates            | 1350         |\n",
            "|    policy_gradient_loss | -0.00246     |\n",
            "|    reward               | -0.6102122   |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.514        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2380          |\n",
            "|    iterations           | 137           |\n",
            "|    time_elapsed         | 117           |\n",
            "|    total_timesteps      | 280576        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.004984134   |\n",
            "|    clip_fraction        | 0.0474        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.45         |\n",
            "|    explained_variance   | 0.734         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 0.308         |\n",
            "|    n_updates            | 1360          |\n",
            "|    policy_gradient_loss | -0.00444      |\n",
            "|    reward               | -0.0110384505 |\n",
            "|    std                  | 1.03          |\n",
            "|    value_loss           | 0.585         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2380         |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 282624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.007327024  |\n",
            "|    clip_fraction        | 0.0683       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.383        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0136       |\n",
            "|    n_updates            | 1370         |\n",
            "|    policy_gradient_loss | 0.000861     |\n",
            "|    reward               | -0.047443617 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.0419       |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 119          |\n",
            "|    total_timesteps      | 284672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014446261 |\n",
            "|    clip_fraction        | 0.00981      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.757        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.262        |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00124     |\n",
            "|    reward               | 0.02176854   |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.582        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 120          |\n",
            "|    total_timesteps      | 286720       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.004288201  |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.284        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0846       |\n",
            "|    n_updates            | 1390         |\n",
            "|    policy_gradient_loss | -0.000284    |\n",
            "|    reward               | -0.005975414 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.161        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 288768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047660414 |\n",
            "|    clip_fraction        | 0.0241       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.763        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.227        |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.00239     |\n",
            "|    reward               | 0.0018096829 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.422        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 122         |\n",
            "|    total_timesteps      | 290816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004443655 |\n",
            "|    clip_fraction        | 0.0117      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.752       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.254       |\n",
            "|    n_updates            | 1410        |\n",
            "|    policy_gradient_loss | -0.00111    |\n",
            "|    reward               | 0.012583145 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 0.615       |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 90\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 303872.05\n",
            "total_reward: 293872.05\n",
            "total_cost: 107.02\n",
            "total_trades: 3314\n",
            "Sharpe: 1.057\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 2382          |\n",
            "|    iterations           | 143           |\n",
            "|    time_elapsed         | 122           |\n",
            "|    total_timesteps      | 292864        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.009363795   |\n",
            "|    clip_fraction        | 0.0634        |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.46         |\n",
            "|    explained_variance   | -0.0362       |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | -0.000439     |\n",
            "|    n_updates            | 1420          |\n",
            "|    policy_gradient_loss | 0.00129       |\n",
            "|    reward               | -0.0040794355 |\n",
            "|    std                  | 1.04          |\n",
            "|    value_loss           | 0.036         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 123          |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024632446 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.708        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.295        |\n",
            "|    n_updates            | 1430         |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    reward               | 0.12888557   |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 0.678        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 145          |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 296960       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014496037 |\n",
            "|    clip_fraction        | 0.0778       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.509        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.0722       |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    reward               | -0.032757256 |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.238        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 2381         |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 125          |\n",
            "|    total_timesteps      | 299008       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046254247 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.735        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.144        |\n",
            "|    n_updates            | 1450         |\n",
            "|    policy_gradient_loss | -0.00464     |\n",
            "|    reward               | 0.003981215  |\n",
            "|    std                  | 1.03         |\n",
            "|    value_loss           | 0.43         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 2382        |\n",
            "|    iterations           | 147         |\n",
            "|    time_elapsed         | 126         |\n",
            "|    total_timesteps      | 301056      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003638865 |\n",
            "|    clip_fraction        | 0.00825     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.747       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.277       |\n",
            "|    n_updates            | 1460        |\n",
            "|    policy_gradient_loss | -0.0017     |\n",
            "|    reward               | 0.027440408 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 0.604       |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=300000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 282         |\n",
            "|    time_elapsed    | 48          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 19.8        |\n",
            "|    critic_loss     | 0.233       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 95          |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 16          |\n",
            "|    critic_loss     | 0.104       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 170\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 143         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 11          |\n",
            "|    critic_loss     | 0.229       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 283         |\n",
            "|    time_elapsed    | 191         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 7.44        |\n",
            "|    critic_loss     | 0.0701      |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 180\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 20          |\n",
            "|    fps             | 282         |\n",
            "|    time_elapsed    | 240         |\n",
            "|    total_timesteps | 67940       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 4.96        |\n",
            "|    critic_loss     | 0.162       |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 67839       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=80000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 216         |\n",
            "|    time_elapsed    | 62          |\n",
            "|    total_timesteps | 13588       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 165         |\n",
            "|    critic_loss     | 1.23        |\n",
            "|    ent_coef        | 0.385       |\n",
            "|    ent_coef_loss   | 9.04        |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 13487       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 150\n",
            "begin_total_asset: 10000.00\n",
            "end_total_asset: 314204.97\n",
            "total_reward: 304204.97\n",
            "total_cost: 9.99\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 227         |\n",
            "|    time_elapsed    | 119         |\n",
            "|    total_timesteps | 27176       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 565         |\n",
            "|    critic_loss     | 10.3        |\n",
            "|    ent_coef        | 1.5         |\n",
            "|    ent_coef_loss   | -3.79       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 27075       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 225         |\n",
            "|    time_elapsed    | 180         |\n",
            "|    total_timesteps | 40764       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 2.11e+03    |\n",
            "|    critic_loss     | 124         |\n",
            "|    ent_coef        | 5.83        |\n",
            "|    ent_coef_loss   | -16.5       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 40663       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 222         |\n",
            "|    time_elapsed    | 243         |\n",
            "|    total_timesteps | 54352       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 8.28e+03    |\n",
            "|    critic_loss     | 443         |\n",
            "|    ent_coef        | 22.7        |\n",
            "|    ent_coef_loss   | -29.5       |\n",
            "|    learning_rate   | 0.0001      |\n",
            "|    n_updates       | 54251       |\n",
            "|    reward          | -0.57683593 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=60000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
