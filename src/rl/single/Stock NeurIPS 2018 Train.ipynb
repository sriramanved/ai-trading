{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from customenv import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data_single.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "11\n",
            "Stock Dimension: 1, State Space: 11\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(len(INDICATORS))\n",
        "print(state_space)\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            date   tic        open        high         low       close  \\\n",
            "                                                                         \n",
            "0     2009-01-02  AAPL    3.067143    3.251429    3.041429    2.743889   \n",
            "1     2009-01-05  AAPL    3.327500    3.435000    3.311071    2.859692   \n",
            "2     2009-01-06  AAPL    3.426786    3.470357    3.299643    2.812525   \n",
            "3     2009-01-07  AAPL    3.278929    3.303571    3.223571    2.751750   \n",
            "4     2009-01-08  AAPL    3.229643    3.326786    3.215714    2.802849   \n",
            "...          ...   ...         ...         ...         ...         ...   \n",
            "3392  2022-06-24  AAPL  139.899994  141.910004  139.770004  140.265213   \n",
            "3393  2022-06-27  AAPL  142.699997  143.490005  140.970001  140.265213   \n",
            "3394  2022-06-28  AAPL  142.130005  143.419998  137.320007  136.086792   \n",
            "3395  2022-06-29  AAPL  137.460007  140.669998  136.669998  137.859146   \n",
            "3396  2022-06-30  AAPL  137.250000  138.369995  133.770004  135.373856   \n",
            "\n",
            "            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
            "                                                                        \n",
            "0     7.460152e+08  4.0  0.000000    2.965560    2.638021  100.000000   \n",
            "1     1.181608e+09  0.0  0.002598    2.965560    2.638021  100.000000   \n",
            "2     1.289310e+09  1.0  0.001848    2.921832    2.688904   70.355597   \n",
            "3     7.530488e+08  2.0 -0.000733    2.901131    2.682796   50.429125   \n",
            "4     6.735008e+08  3.0 -0.000086    2.889183    2.699099   60.227046   \n",
            "...            ...  ...       ...         ...         ...         ...   \n",
            "3392  8.911680e+07  4.0 -3.592022  153.653134  125.865370   45.916192   \n",
            "3393  7.020790e+07  0.0 -3.035211  153.496937  125.811654   45.916192   \n",
            "3394  6.708340e+07  1.0 -2.897694  152.373494  125.727114   43.786092   \n",
            "3395  6.624240e+07  2.0 -2.615547  151.312869  125.836202   44.907562   \n",
            "3396  9.896450e+07  3.0 -2.562941  150.109767  125.852107   43.644504   \n",
            "\n",
            "         cci_30       dx_30  close_30_sma  close_60_sma        vix  turbulence  \n",
            "                                                                                \n",
            "0     66.666667  100.000000      2.743889      2.743889  39.189999    0.000000  \n",
            "1     66.666667  100.000000      2.801790      2.801790  39.080002    0.000000  \n",
            "2     46.873238  100.000000      2.805368      2.805368  38.560001    0.000000  \n",
            "3    -29.668948   43.607834      2.791964      2.791964  43.389999    0.000000  \n",
            "4     -9.105090   48.357918      2.794141      2.794141  42.560001    0.000000  \n",
            "...         ...         ...           ...           ...        ...         ...  \n",
            "3392  -4.563871    6.030935    140.178751    151.497016  27.230000    1.684843  \n",
            "3393   8.191339    1.515034    140.149046    150.905412  26.950001    0.000542  \n",
            "3394 -24.937968   10.887399    139.829887    150.296239  28.360001    2.615248  \n",
            "3395 -29.756641   12.473066    139.621624    149.721550  28.160000    0.460942  \n",
            "3396 -61.598580   19.319766    139.208399    149.037383  28.709999    0.950558  \n",
            "\n",
            "[3397 rows x 18 columns]\n"
          ]
        }
      ],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 100000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4,\n",
        "    \"print_verbosity\": 1\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1628       |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 0          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.51      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | 1.93       |\n",
            "|    reward             | 0.62805086 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 2.45       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1719      |\n",
            "|    iterations         | 200       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 1000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.51     |\n",
            "|    explained_variance | -0.00273  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 199       |\n",
            "|    policy_loss        | 12.3      |\n",
            "|    reward             | 3.4211624 |\n",
            "|    std                | 1.1       |\n",
            "|    value_loss         | 125       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1746     |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.52    |\n",
            "|    explained_variance | 0.000169 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 57.1     |\n",
            "|    reward             | 9.77627  |\n",
            "|    std                | 1.1      |\n",
            "|    value_loss         | 1.11e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1773      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.51     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | 37.7      |\n",
            "|    reward             | 10.701119 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 1.07e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1788      |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.5      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 73.4      |\n",
            "|    reward             | 16.177843 |\n",
            "|    std                | 1.09      |\n",
            "|    value_loss         | 2.7e+03   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1797      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.49     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | 83.9      |\n",
            "|    reward             | 16.423761 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 2.72e+03  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 2\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 261764.39\n",
            "total_reward: 161764.39\n",
            "beating_benchmark: 161858.49\n",
            "total_cost: 7198.04\n",
            "total_trades: 3080\n",
            "Sharpe: 0.599\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1803        |\n",
            "|    iterations         | 700         |\n",
            "|    time_elapsed       | 1           |\n",
            "|    total_timesteps    | 3500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 699         |\n",
            "|    policy_loss        | -1.72       |\n",
            "|    reward             | 0.007881946 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 1.57        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1806        |\n",
            "|    iterations         | 800         |\n",
            "|    time_elapsed       | 2           |\n",
            "|    total_timesteps    | 4000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.48       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 799         |\n",
            "|    policy_loss        | -1.15       |\n",
            "|    reward             | 0.029861355 |\n",
            "|    std                | 1.06        |\n",
            "|    value_loss         | 1.03        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1810       |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.48      |\n",
            "|    explained_variance | 5.01e-06   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 0.00109    |\n",
            "|    reward             | 0.08516443 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 9.7e-07    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1809       |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 2          |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.51      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -0.0625    |\n",
            "|    reward             | 0.07557534 |\n",
            "|    std                | 1.09       |\n",
            "|    value_loss         | 0.00153    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1807         |\n",
            "|    iterations         | 1100         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 5500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.54        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1099         |\n",
            "|    policy_loss        | 0.0108       |\n",
            "|    reward             | -0.041449223 |\n",
            "|    std                | 1.12         |\n",
            "|    value_loss         | 0.000106     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1765         |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.57        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | -0.0198      |\n",
            "|    reward             | -0.029544804 |\n",
            "|    std                | 1.17         |\n",
            "|    value_loss         | 8.11e-05     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1768         |\n",
            "|    iterations         | 1300         |\n",
            "|    time_elapsed       | 3            |\n",
            "|    total_timesteps    | 6500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.61        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1299         |\n",
            "|    policy_loss        | -0.00354     |\n",
            "|    reward             | -0.027127204 |\n",
            "|    std                | 1.21         |\n",
            "|    value_loss         | 7.81e-06     |\n",
            "----------------------------------------\n",
            "day: 3396, episode: 3\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 99668.53\n",
            "total_reward: -331.47\n",
            "beating_benchmark: -237.38\n",
            "total_cost: 2570.69\n",
            "total_trades: 1663\n",
            "Sharpe: -0.045\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1768       |\n",
            "|    iterations         | 1400       |\n",
            "|    time_elapsed       | 3          |\n",
            "|    total_timesteps    | 7000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.64      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1399       |\n",
            "|    policy_loss        | 0.0011     |\n",
            "|    reward             | 0.00537262 |\n",
            "|    std                | 1.25       |\n",
            "|    value_loss         | 7.15e-07   |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1770         |\n",
            "|    iterations         | 1500         |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 7500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.68        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1499         |\n",
            "|    policy_loss        | -0.00191     |\n",
            "|    reward             | -0.025189914 |\n",
            "|    std                | 1.3          |\n",
            "|    value_loss         | 1.62e-06     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1775         |\n",
            "|    iterations         | 1600         |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 8000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.73        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1599         |\n",
            "|    policy_loss        | -0.0206      |\n",
            "|    reward             | -0.052151177 |\n",
            "|    std                | 1.36         |\n",
            "|    value_loss         | 0.000122     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1779         |\n",
            "|    iterations         | 1700         |\n",
            "|    time_elapsed       | 4            |\n",
            "|    total_timesteps    | 8500         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.79        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1699         |\n",
            "|    policy_loss        | 0.0124       |\n",
            "|    reward             | -0.024768805 |\n",
            "|    std                | 1.45         |\n",
            "|    value_loss         | 5.32e-05     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1781         |\n",
            "|    iterations         | 1800         |\n",
            "|    time_elapsed       | 5            |\n",
            "|    total_timesteps    | 9000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -1.85        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1799         |\n",
            "|    policy_loss        | 0.046        |\n",
            "|    reward             | -0.014038541 |\n",
            "|    std                | 1.54         |\n",
            "|    value_loss         | 0.000356     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1712        |\n",
            "|    iterations         | 1900        |\n",
            "|    time_elapsed       | 5           |\n",
            "|    total_timesteps    | 9500        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.9        |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1899        |\n",
            "|    policy_loss        | 0.00321     |\n",
            "|    reward             | 0.022271575 |\n",
            "|    std                | 1.62        |\n",
            "|    value_loss         | 3.94e-06    |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1714      |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -1.92     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 7.93      |\n",
            "|    reward             | 1.7976487 |\n",
            "|    std                | 1.65      |\n",
            "|    value_loss         | 22.5      |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 4\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 114126.84\n",
            "total_reward: 14126.84\n",
            "beating_benchmark: 14220.94\n",
            "total_cost: 6482.58\n",
            "total_trades: 1558\n",
            "Sharpe: 0.224\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1719        |\n",
            "|    iterations         | 2100        |\n",
            "|    time_elapsed       | 6           |\n",
            "|    total_timesteps    | 10500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -1.93       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2099        |\n",
            "|    policy_loss        | -0.943      |\n",
            "|    reward             | 0.047412287 |\n",
            "|    std                | 1.67        |\n",
            "|    value_loss         | 0.339       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1724       |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -1.96      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -0.00122   |\n",
            "|    reward             | 0.04484694 |\n",
            "|    std                | 1.71       |\n",
            "|    value_loss         | 7.64e-07   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1731       |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2         |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | 0.00306    |\n",
            "|    reward             | 0.07967651 |\n",
            "|    std                | 1.79       |\n",
            "|    value_loss         | 4.23e-06   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1735       |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.04      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | -0.000392  |\n",
            "|    reward             | 0.06418434 |\n",
            "|    std                | 1.86       |\n",
            "|    value_loss         | 1.11e-06   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1739       |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.07      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -0.0305    |\n",
            "|    reward             | 0.12814456 |\n",
            "|    std                | 1.93       |\n",
            "|    value_loss         | 0.000203   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1743       |\n",
            "|    iterations         | 2600       |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 13000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.11      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2599       |\n",
            "|    policy_loss        | 2.15       |\n",
            "|    reward             | 0.69070816 |\n",
            "|    std                | 1.99       |\n",
            "|    value_loss         | 1.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1745      |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.1      |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 10.8      |\n",
            "|    reward             | 2.2648652 |\n",
            "|    std                | 1.97      |\n",
            "|    value_loss         | 40.5      |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 5\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 120028.46\n",
            "total_reward: 20028.46\n",
            "beating_benchmark: 20122.55\n",
            "total_cost: 6535.55\n",
            "total_trades: 1974\n",
            "Sharpe: 0.585\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1746         |\n",
            "|    iterations         | 2800         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 14000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.11        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 2799         |\n",
            "|    policy_loss        | -3.95        |\n",
            "|    reward             | 0.0075082406 |\n",
            "|    std                | 1.99         |\n",
            "|    value_loss         | 1.69         |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1746        |\n",
            "|    iterations         | 2900        |\n",
            "|    time_elapsed       | 8           |\n",
            "|    total_timesteps    | 14500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.13       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2899        |\n",
            "|    policy_loss        | -0.0166     |\n",
            "|    reward             | 0.034159184 |\n",
            "|    std                | 2.03        |\n",
            "|    value_loss         | 0.000104    |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1750          |\n",
            "|    iterations         | 3000          |\n",
            "|    time_elapsed       | 8             |\n",
            "|    total_timesteps    | 15000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.16         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 2999          |\n",
            "|    policy_loss        | -0.000662     |\n",
            "|    reward             | -0.0013625637 |\n",
            "|    std                | 2.1           |\n",
            "|    value_loss         | 2.57e-07      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1753         |\n",
            "|    iterations         | 3100         |\n",
            "|    time_elapsed       | 8            |\n",
            "|    total_timesteps    | 15500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.2         |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3099         |\n",
            "|    policy_loss        | -0.0812      |\n",
            "|    reward             | -0.089597985 |\n",
            "|    std                | 2.19         |\n",
            "|    value_loss         | 0.00251      |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1756        |\n",
            "|    iterations         | 3200        |\n",
            "|    time_elapsed       | 9           |\n",
            "|    total_timesteps    | 16000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.22       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3199        |\n",
            "|    policy_loss        | -0.305      |\n",
            "|    reward             | -0.10523504 |\n",
            "|    std                | 2.22        |\n",
            "|    value_loss         | 0.0121      |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1757       |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 9          |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.23      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 2.63       |\n",
            "|    reward             | 0.32962695 |\n",
            "|    std                | 2.24       |\n",
            "|    value_loss         | 2.2        |\n",
            "--------------------------------------\n",
            "day: 3396, episode: 6\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 97059.10\n",
            "total_reward: -2940.90\n",
            "beating_benchmark: -2846.80\n",
            "total_cost: 6711.03\n",
            "total_trades: 1852\n",
            "Sharpe: -0.080\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1759          |\n",
            "|    iterations         | 3400          |\n",
            "|    time_elapsed       | 9             |\n",
            "|    total_timesteps    | 17000         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.22         |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 3399          |\n",
            "|    policy_loss        | -0.0758       |\n",
            "|    reward             | -0.0039406423 |\n",
            "|    std                | 2.23          |\n",
            "|    value_loss         | 0.00181       |\n",
            "-----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1760      |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.25     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 0.0529    |\n",
            "|    reward             | 0.0990356 |\n",
            "|    std                | 2.31      |\n",
            "|    value_loss         | 0.000712  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1762      |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.29     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -0.0534   |\n",
            "|    reward             | 0.1814761 |\n",
            "|    std                | 2.38      |\n",
            "|    value_loss         | 0.000446  |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1764       |\n",
            "|    iterations         | 3700       |\n",
            "|    time_elapsed       | 10         |\n",
            "|    total_timesteps    | 18500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.33      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3699       |\n",
            "|    policy_loss        | -0.0691    |\n",
            "|    reward             | 0.29783118 |\n",
            "|    std                | 2.49       |\n",
            "|    value_loss         | 0.00118    |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1766         |\n",
            "|    iterations         | 3800         |\n",
            "|    time_elapsed       | 10           |\n",
            "|    total_timesteps    | 19000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.35        |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3799         |\n",
            "|    policy_loss        | 0.0108       |\n",
            "|    reward             | -0.005956837 |\n",
            "|    std                | 2.54         |\n",
            "|    value_loss         | 3.16e-05     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1769        |\n",
            "|    iterations         | 3900        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 19500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.39       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3899        |\n",
            "|    policy_loss        | -1.38       |\n",
            "|    reward             | -0.18294074 |\n",
            "|    std                | 2.63        |\n",
            "|    value_loss         | 0.445       |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1771         |\n",
            "|    iterations         | 4000         |\n",
            "|    time_elapsed       | 11           |\n",
            "|    total_timesteps    | 20000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.4         |\n",
            "|    explained_variance | 1.19e-07     |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 3999         |\n",
            "|    policy_loss        | -0.0535      |\n",
            "|    reward             | -0.068880595 |\n",
            "|    std                | 2.66         |\n",
            "|    value_loss         | 0.00108      |\n",
            "----------------------------------------\n",
            "day: 3396, episode: 7\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 97114.77\n",
            "total_reward: -2885.23\n",
            "beating_benchmark: -2791.13\n",
            "total_cost: 7569.32\n",
            "total_trades: 2525\n",
            "Sharpe: -0.083\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                 |               |\n",
            "|    fps                | 1773          |\n",
            "|    iterations         | 4100          |\n",
            "|    time_elapsed       | 11            |\n",
            "|    total_timesteps    | 20500         |\n",
            "| train/                |               |\n",
            "|    entropy_loss       | -2.4          |\n",
            "|    explained_variance | 0             |\n",
            "|    learning_rate      | 0.0007        |\n",
            "|    n_updates          | 4099          |\n",
            "|    policy_loss        | -0.0467       |\n",
            "|    reward             | -0.0018592711 |\n",
            "|    std                | 2.67          |\n",
            "|    value_loss         | 0.000624      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1776        |\n",
            "|    iterations         | 4200        |\n",
            "|    time_elapsed       | 11          |\n",
            "|    total_timesteps    | 21000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.42       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4199        |\n",
            "|    policy_loss        | -0.0224     |\n",
            "|    reward             | 0.017652946 |\n",
            "|    std                | 2.73        |\n",
            "|    value_loss         | 7.98e-05    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1778        |\n",
            "|    iterations         | 4300        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 21500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.46       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4299        |\n",
            "|    policy_loss        | -0.0249     |\n",
            "|    reward             | -0.04107194 |\n",
            "|    std                | 2.84        |\n",
            "|    value_loss         | 8.92e-05    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1777         |\n",
            "|    iterations         | 4400         |\n",
            "|    time_elapsed       | 12           |\n",
            "|    total_timesteps    | 22000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.51        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4399         |\n",
            "|    policy_loss        | -0.0324      |\n",
            "|    reward             | -0.053789914 |\n",
            "|    std                | 2.98         |\n",
            "|    value_loss         | 0.000242     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1778        |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.55       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | -0.00262    |\n",
            "|    reward             | -0.12603389 |\n",
            "|    std                | 3.1         |\n",
            "|    value_loss         | 4.09e-06    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1777        |\n",
            "|    iterations         | 4600        |\n",
            "|    time_elapsed       | 12          |\n",
            "|    total_timesteps    | 23000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.59       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4599        |\n",
            "|    policy_loss        | -0.0164     |\n",
            "|    reward             | -0.18531437 |\n",
            "|    std                | 3.21        |\n",
            "|    value_loss         | 0.000107    |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1778        |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.6        |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -0.105      |\n",
            "|    reward             | -0.07504627 |\n",
            "|    std                | 3.27        |\n",
            "|    value_loss         | 0.00335     |\n",
            "---------------------------------------\n",
            "day: 3396, episode: 8\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 97435.98\n",
            "total_reward: -2564.02\n",
            "beating_benchmark: -2469.93\n",
            "total_cost: 5185.61\n",
            "total_trades: 1794\n",
            "Sharpe: -0.134\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1779         |\n",
            "|    iterations         | 4800         |\n",
            "|    time_elapsed       | 13           |\n",
            "|    total_timesteps    | 24000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.62        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 4799         |\n",
            "|    policy_loss        | 0.00567      |\n",
            "|    reward             | 0.0015799153 |\n",
            "|    std                | 3.34         |\n",
            "|    value_loss         | 5.31e-06     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1779        |\n",
            "|    iterations         | 4900        |\n",
            "|    time_elapsed       | 13          |\n",
            "|    total_timesteps    | 24500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.66       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4899        |\n",
            "|    policy_loss        | -0.0161     |\n",
            "|    reward             | 0.013058719 |\n",
            "|    std                | 3.46        |\n",
            "|    value_loss         | 5.64e-05    |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1779       |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.7       |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -0.0508    |\n",
            "|    reward             | 0.08090488 |\n",
            "|    std                | 3.59       |\n",
            "|    value_loss         | 0.000363   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1778       |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.74      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | 0.0774     |\n",
            "|    reward             | 0.08520222 |\n",
            "|    std                | 3.74       |\n",
            "|    value_loss         | 0.00152    |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1778      |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.78     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | 0.0585    |\n",
            "|    reward             | 0.1718913 |\n",
            "|    std                | 3.89      |\n",
            "|    value_loss         | 0.000652  |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1779         |\n",
            "|    iterations         | 5300         |\n",
            "|    time_elapsed       | 14           |\n",
            "|    total_timesteps    | 26500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.82        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5299         |\n",
            "|    policy_loss        | 0.205        |\n",
            "|    reward             | -0.012501862 |\n",
            "|    std                | 4.05         |\n",
            "|    value_loss         | 0.00701      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1780         |\n",
            "|    iterations         | 5400         |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 27000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.85        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5399         |\n",
            "|    policy_loss        | -0.301       |\n",
            "|    reward             | -0.103763014 |\n",
            "|    std                | 4.18         |\n",
            "|    value_loss         | 0.0145       |\n",
            "----------------------------------------\n",
            "day: 3396, episode: 9\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 95008.01\n",
            "total_reward: -4991.99\n",
            "beating_benchmark: -4897.90\n",
            "total_cost: 7502.40\n",
            "total_trades: 2003\n",
            "Sharpe: -0.177\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1782         |\n",
            "|    iterations         | 5500         |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 27500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.86        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5499         |\n",
            "|    policy_loss        | -0.067       |\n",
            "|    reward             | 0.0038734921 |\n",
            "|    std                | 4.22         |\n",
            "|    value_loss         | 0.000638     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1783        |\n",
            "|    iterations         | 5600        |\n",
            "|    time_elapsed       | 15          |\n",
            "|    total_timesteps    | 28000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.9        |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5599        |\n",
            "|    policy_loss        | 0.0531      |\n",
            "|    reward             | 0.117186345 |\n",
            "|    std                | 4.38        |\n",
            "|    value_loss         | 0.00046     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1785         |\n",
            "|    iterations         | 5700         |\n",
            "|    time_elapsed       | 15           |\n",
            "|    total_timesteps    | 28500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.94        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5699         |\n",
            "|    policy_loss        | 0.0132       |\n",
            "|    reward             | -0.039529737 |\n",
            "|    std                | 4.56         |\n",
            "|    value_loss         | 2.64e-05     |\n",
            "----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1785        |\n",
            "|    iterations         | 5800        |\n",
            "|    time_elapsed       | 16          |\n",
            "|    total_timesteps    | 29000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.97       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5799        |\n",
            "|    policy_loss        | -0.225      |\n",
            "|    reward             | -0.18704046 |\n",
            "|    std                | 4.72        |\n",
            "|    value_loss         | 0.00647     |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1786         |\n",
            "|    iterations         | 5900         |\n",
            "|    time_elapsed       | 16           |\n",
            "|    total_timesteps    | 29500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.98        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5899         |\n",
            "|    policy_loss        | -0.391       |\n",
            "|    reward             | 0.0015378884 |\n",
            "|    std                | 4.77         |\n",
            "|    value_loss         | 0.0209       |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1787       |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 16         |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.99      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | -0.876     |\n",
            "|    reward             | 0.17655952 |\n",
            "|    std                | 4.81       |\n",
            "|    value_loss         | 0.0986     |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1787      |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | 44.2      |\n",
            "|    reward             | 5.9380546 |\n",
            "|    std                | 4.77      |\n",
            "|    value_loss         | 291       |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 10\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 150345.01\n",
            "total_reward: 50345.01\n",
            "beating_benchmark: 50439.10\n",
            "total_cost: 11394.40\n",
            "total_trades: 2796\n",
            "Sharpe: 0.386\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1787       |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 17         |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.98      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 0.509      |\n",
            "|    reward             | 0.27377138 |\n",
            "|    std                | 4.78       |\n",
            "|    value_loss         | 0.0163     |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1788     |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.98    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 22.1     |\n",
            "|    reward             | 3.211871 |\n",
            "|    std                | 4.78     |\n",
            "|    value_loss         | 74.4     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1789     |\n",
            "|    iterations         | 6400     |\n",
            "|    time_elapsed       | 17       |\n",
            "|    total_timesteps    | 32000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.98    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6399     |\n",
            "|    policy_loss        | 19.3     |\n",
            "|    reward             | 2.620117 |\n",
            "|    std                | 4.76     |\n",
            "|    value_loss         | 44.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1790      |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 17.2      |\n",
            "|    reward             | 2.6373324 |\n",
            "|    std                | 4.76      |\n",
            "|    value_loss         | 47.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1792      |\n",
            "|    iterations         | 6600      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 33000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | -8.69e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6599      |\n",
            "|    policy_loss        | 83.9      |\n",
            "|    reward             | 10.562838 |\n",
            "|    std                | 4.77      |\n",
            "|    value_loss         | 958       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1793      |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.97     |\n",
            "|    explained_variance | -5.28e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | 207       |\n",
            "|    reward             | 27.278942 |\n",
            "|    std                | 4.71      |\n",
            "|    value_loss         | 6.91e+03  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 11\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 456793.60\n",
            "total_reward: 356793.60\n",
            "beating_benchmark: 356887.69\n",
            "total_cost: 12060.09\n",
            "total_trades: 3334\n",
            "Sharpe: 0.720\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1795         |\n",
            "|    iterations         | 6800         |\n",
            "|    time_elapsed       | 18           |\n",
            "|    total_timesteps    | 34000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.96        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 6799         |\n",
            "|    policy_loss        | -5.9         |\n",
            "|    reward             | 0.0034870661 |\n",
            "|    std                | 4.68         |\n",
            "|    value_loss         | 6.03         |\n",
            "----------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1796     |\n",
            "|    iterations         | 6900     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 34500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.97    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6899     |\n",
            "|    policy_loss        | 4.37     |\n",
            "|    reward             | 1.272109 |\n",
            "|    std                | 4.71     |\n",
            "|    value_loss         | 2.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1797     |\n",
            "|    iterations         | 7000     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 35000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.96    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6999     |\n",
            "|    policy_loss        | 8.3      |\n",
            "|    reward             | 1.849513 |\n",
            "|    std                | 4.68     |\n",
            "|    value_loss         | 10.9     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1797     |\n",
            "|    iterations         | 7100     |\n",
            "|    time_elapsed       | 19       |\n",
            "|    total_timesteps    | 35500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.97    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7099     |\n",
            "|    policy_loss        | 12.1     |\n",
            "|    reward             | 2.383386 |\n",
            "|    std                | 4.73     |\n",
            "|    value_loss         | 25.7     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1798      |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | 23.2      |\n",
            "|    reward             | 3.7221587 |\n",
            "|    std                | 4.78      |\n",
            "|    value_loss         | 74.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1799      |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | -1.39e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 49.7      |\n",
            "|    reward             | 6.611477  |\n",
            "|    std                | 4.76      |\n",
            "|    value_loss         | 328       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1801      |\n",
            "|    iterations         | 7400      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 37000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | 1.53e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7399      |\n",
            "|    policy_loss        | 283       |\n",
            "|    reward             | 32.068565 |\n",
            "|    std                | 4.8       |\n",
            "|    value_loss         | 1.1e+04   |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 12\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 505563.68\n",
            "total_reward: 405563.68\n",
            "beating_benchmark: 405657.77\n",
            "total_cost: 12888.08\n",
            "total_trades: 3362\n",
            "Sharpe: 0.822\n",
            "=================================\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 1802        |\n",
            "|    iterations         | 7500        |\n",
            "|    time_elapsed       | 20          |\n",
            "|    total_timesteps    | 37500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -2.98       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7499        |\n",
            "|    policy_loss        | -10.9       |\n",
            "|    reward             | 0.017613744 |\n",
            "|    std                | 4.77        |\n",
            "|    value_loss         | 10          |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1803       |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 21         |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.98      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | -4.35      |\n",
            "|    reward             | 0.43474135 |\n",
            "|    std                | 4.77       |\n",
            "|    value_loss         | 3.02       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 1804       |\n",
            "|    iterations         | 7700       |\n",
            "|    time_elapsed       | 21         |\n",
            "|    total_timesteps    | 38500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -2.98      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7699       |\n",
            "|    policy_loss        | -4.23      |\n",
            "|    reward             | 0.45045954 |\n",
            "|    std                | 4.78       |\n",
            "|    value_loss         | 2.37       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1806      |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | 4.82e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 48.4      |\n",
            "|    reward             | 5.8845124 |\n",
            "|    std                | 4.8       |\n",
            "|    value_loss         | 277       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1807      |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | -5.27e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 48.7      |\n",
            "|    reward             | 7.1882396 |\n",
            "|    std                | 4.78      |\n",
            "|    value_loss         | 378       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1808     |\n",
            "|    iterations         | 8000     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.99    |\n",
            "|    explained_variance | -0.00076 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7999     |\n",
            "|    policy_loss        | 83.3     |\n",
            "|    reward             | 12.77593 |\n",
            "|    std                | 4.82     |\n",
            "|    value_loss         | 1.3e+03  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1809     |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 22       |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.99    |\n",
            "|    explained_variance | 8.82e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 389      |\n",
            "|    reward             | 46.2668  |\n",
            "|    std                | 4.81     |\n",
            "|    value_loss         | 2.02e+04 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 13\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 592582.52\n",
            "total_reward: 492582.52\n",
            "beating_benchmark: 492676.62\n",
            "total_cost: 11811.99\n",
            "total_trades: 3362\n",
            "Sharpe: 0.746\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1809      |\n",
            "|    iterations         | 8200      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 41000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8199      |\n",
            "|    policy_loss        | 4.71      |\n",
            "|    reward             | 1.6269019 |\n",
            "|    std                | 4.82      |\n",
            "|    value_loss         | 2.9       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1810      |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 22        |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3        |\n",
            "|    explained_variance | 1.63e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | 67.3      |\n",
            "|    reward             | 10.251359 |\n",
            "|    std                | 4.86      |\n",
            "|    value_loss         | 691       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1811      |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3        |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | 161       |\n",
            "|    reward             | 19.526442 |\n",
            "|    std                | 4.88      |\n",
            "|    value_loss         | 2.87e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1812     |\n",
            "|    iterations         | 8500     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 42500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -3.01    |\n",
            "|    explained_variance | 2.09e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8499     |\n",
            "|    policy_loss        | 319      |\n",
            "|    reward             | 35.52446 |\n",
            "|    std                | 4.9      |\n",
            "|    value_loss         | 1.12e+04 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1813      |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.02     |\n",
            "|    explained_variance | 1.85e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 528       |\n",
            "|    reward             | 57.196327 |\n",
            "|    std                | 4.94      |\n",
            "|    value_loss         | 3.55e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1815      |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 23        |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | -4.29e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 796       |\n",
            "|    reward             | 95.5829   |\n",
            "|    std                | 4.9       |\n",
            "|    value_loss         | 9.04e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1816      |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | 2.01e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | 2.17e+03  |\n",
            "|    reward             | 232.96208 |\n",
            "|    std                | 4.9       |\n",
            "|    value_loss         | 5.64e+05  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 14\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 2204469.76\n",
            "total_reward: 2104469.76\n",
            "beating_benchmark: 2104563.86\n",
            "total_cost: 11986.36\n",
            "total_trades: 3390\n",
            "Sharpe: 0.998\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1816      |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | -1.01     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 19.8      |\n",
            "|    reward             | 2.6747377 |\n",
            "|    std                | 4.9       |\n",
            "|    value_loss         | 60.5      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1814     |\n",
            "|    iterations         | 9000     |\n",
            "|    time_elapsed       | 24       |\n",
            "|    total_timesteps    | 45000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -3.01    |\n",
            "|    explained_variance | 5.25e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8999     |\n",
            "|    policy_loss        | 144      |\n",
            "|    reward             | 17.27536 |\n",
            "|    std                | 4.92     |\n",
            "|    value_loss         | 2.58e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1814      |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | -3.58e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 129       |\n",
            "|    reward             | 18.429955 |\n",
            "|    std                | 4.93      |\n",
            "|    value_loss         | 2.07e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1815      |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | 0.00232   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 287       |\n",
            "|    reward             | 26.623875 |\n",
            "|    std                | 4.93      |\n",
            "|    value_loss         | 7.55e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1816      |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.02     |\n",
            "|    explained_variance | -4.77e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | 456       |\n",
            "|    reward             | 52.543465 |\n",
            "|    std                | 4.94      |\n",
            "|    value_loss         | 2.75e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1816     |\n",
            "|    iterations         | 9400     |\n",
            "|    time_elapsed       | 25       |\n",
            "|    total_timesteps    | 47000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -3.01    |\n",
            "|    explained_variance | -3.1e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9399     |\n",
            "|    policy_loss        | 745      |\n",
            "|    reward             | 95.07584 |\n",
            "|    std                | 4.93     |\n",
            "|    value_loss         | 7.09e+04 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1817     |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -3.01    |\n",
            "|    explained_variance | 5.04e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | 2.14e+03 |\n",
            "|    reward             | 245.2445 |\n",
            "|    std                | 4.91     |\n",
            "|    value_loss         | 6.67e+05 |\n",
            "------------------------------------\n",
            "day: 3396, episode: 15\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 2035299.44\n",
            "total_reward: 1935299.44\n",
            "beating_benchmark: 1935393.54\n",
            "total_cost: 11291.86\n",
            "total_trades: 3392\n",
            "Sharpe: 0.978\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1818     |\n",
            "|    iterations         | 9600     |\n",
            "|    time_elapsed       | 26       |\n",
            "|    total_timesteps    | 48000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -3.02    |\n",
            "|    explained_variance | 1.79e-06 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9599     |\n",
            "|    policy_loss        | 31.8     |\n",
            "|    reward             | 5.513188 |\n",
            "|    std                | 4.93     |\n",
            "|    value_loss         | 155      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1819      |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | 5.13e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 201       |\n",
            "|    reward             | 26.061481 |\n",
            "|    std                | 4.9       |\n",
            "|    value_loss         | 6.8e+03   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1819      |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | -6.79e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 259       |\n",
            "|    reward             | 27.755709 |\n",
            "|    std                | 4.93      |\n",
            "|    value_loss         | 7.97e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1820      |\n",
            "|    iterations         | 9900      |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 49500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3.01     |\n",
            "|    explained_variance | -4.77e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9899      |\n",
            "|    policy_loss        | 252       |\n",
            "|    reward             | 35.244682 |\n",
            "|    std                | 4.93      |\n",
            "|    value_loss         | 9.83e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1821      |\n",
            "|    iterations         | 10000     |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3        |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9999      |\n",
            "|    policy_loss        | 700       |\n",
            "|    reward             | 81.687805 |\n",
            "|    std                | 4.86      |\n",
            "|    value_loss         | 6.85e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1822      |\n",
            "|    iterations         | 10100     |\n",
            "|    time_elapsed       | 27        |\n",
            "|    total_timesteps    | 50500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -3        |\n",
            "|    explained_variance | -6.79e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10099     |\n",
            "|    policy_loss        | 1.78e+03  |\n",
            "|    reward             | 179.88708 |\n",
            "|    std                | 4.84      |\n",
            "|    value_loss         | 4.49e+05  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 16\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 2302163.90\n",
            "total_reward: 2202163.90\n",
            "beating_benchmark: 2202257.99\n",
            "total_cost: 10810.86\n",
            "total_trades: 3395\n",
            "Sharpe: 1.001\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 1822         |\n",
            "|    iterations         | 10200        |\n",
            "|    time_elapsed       | 27           |\n",
            "|    total_timesteps    | 51000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -2.99        |\n",
            "|    explained_variance | 0.241        |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 10199        |\n",
            "|    policy_loss        | -9.33        |\n",
            "|    reward             | -0.016100798 |\n",
            "|    std                | 4.82         |\n",
            "|    value_loss         | 11.9         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1823      |\n",
            "|    iterations         | 10300     |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 51500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10299     |\n",
            "|    policy_loss        | 74.8      |\n",
            "|    reward             | 10.662792 |\n",
            "|    std                | 4.8       |\n",
            "|    value_loss         | 645       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1824      |\n",
            "|    iterations         | 10400     |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 52000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10399     |\n",
            "|    policy_loss        | 140       |\n",
            "|    reward             | 15.609535 |\n",
            "|    std                | 4.83      |\n",
            "|    value_loss         | 2.17e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1824      |\n",
            "|    iterations         | 10500     |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 52500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10499     |\n",
            "|    policy_loss        | 345       |\n",
            "|    reward             | 45.67491  |\n",
            "|    std                | 4.82      |\n",
            "|    value_loss         | 1.8e+04   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1825      |\n",
            "|    iterations         | 10600     |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 53000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.99     |\n",
            "|    explained_variance | -1.19e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10599     |\n",
            "|    policy_loss        | 429       |\n",
            "|    reward             | 49.148712 |\n",
            "|    std                | 4.79      |\n",
            "|    value_loss         | 2.17e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1826      |\n",
            "|    iterations         | 10700     |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 53500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.98     |\n",
            "|    explained_variance | -1.18e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10699     |\n",
            "|    policy_loss        | 535       |\n",
            "|    reward             | 66.37033  |\n",
            "|    std                | 4.77      |\n",
            "|    value_loss         | 4.43e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1827      |\n",
            "|    iterations         | 10800     |\n",
            "|    time_elapsed       | 29        |\n",
            "|    total_timesteps    | 54000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.96     |\n",
            "|    explained_variance | 6.08e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10799     |\n",
            "|    policy_loss        | 2.07e+03  |\n",
            "|    reward             | 238.50925 |\n",
            "|    std                | 4.69      |\n",
            "|    value_loss         | 5.88e+05  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 17\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 2494706.11\n",
            "total_reward: 2394706.11\n",
            "beating_benchmark: 2394800.21\n",
            "total_cost: 9908.38\n",
            "total_trades: 3391\n",
            "Sharpe: 1.018\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1827     |\n",
            "|    iterations         | 10900    |\n",
            "|    time_elapsed       | 29       |\n",
            "|    total_timesteps    | 54500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.95    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 10899    |\n",
            "|    policy_loss        | -3.05    |\n",
            "|    reward             | 0.961077 |\n",
            "|    std                | 4.64     |\n",
            "|    value_loss         | 1.37     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1826      |\n",
            "|    iterations         | 11000     |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 55000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.94     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 10999     |\n",
            "|    policy_loss        | 82.7      |\n",
            "|    reward             | 11.385433 |\n",
            "|    std                | 4.6       |\n",
            "|    value_loss         | 828       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1827      |\n",
            "|    iterations         | 11100     |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 55500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.94     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11099     |\n",
            "|    policy_loss        | 89.6      |\n",
            "|    reward             | 14.508707 |\n",
            "|    std                | 4.58      |\n",
            "|    value_loss         | 1.4e+03   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1827      |\n",
            "|    iterations         | 11200     |\n",
            "|    time_elapsed       | 30        |\n",
            "|    total_timesteps    | 56000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.92     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11199     |\n",
            "|    policy_loss        | 353       |\n",
            "|    reward             | 40.40385  |\n",
            "|    std                | 4.5       |\n",
            "|    value_loss         | 1.53e+04  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1827     |\n",
            "|    iterations         | 11300    |\n",
            "|    time_elapsed       | 30       |\n",
            "|    total_timesteps    | 56500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.93    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11299    |\n",
            "|    policy_loss        | 364      |\n",
            "|    reward             | 52.49185 |\n",
            "|    std                | 4.52     |\n",
            "|    value_loss         | 2.43e+04 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1828      |\n",
            "|    iterations         | 11400     |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 57000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.93     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11399     |\n",
            "|    policy_loss        | 729       |\n",
            "|    reward             | 76.93509  |\n",
            "|    std                | 4.56      |\n",
            "|    value_loss         | 5.88e+04  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1828      |\n",
            "|    iterations         | 11500     |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 57500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.94     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11499     |\n",
            "|    policy_loss        | 2.5e+03   |\n",
            "|    reward             | 240.36922 |\n",
            "|    std                | 4.57      |\n",
            "|    value_loss         | 5.18e+05  |\n",
            "-------------------------------------\n",
            "day: 3396, episode: 18\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 2382444.81\n",
            "total_reward: 2282444.81\n",
            "beating_benchmark: 2282538.91\n",
            "total_cost: 10572.77\n",
            "total_trades: 3387\n",
            "Sharpe: 1.010\n",
            "=================================\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1828      |\n",
            "|    iterations         | 11600     |\n",
            "|    time_elapsed       | 31        |\n",
            "|    total_timesteps    | 58000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.94     |\n",
            "|    explained_variance | -25.6     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11599     |\n",
            "|    policy_loss        | 10.4      |\n",
            "|    reward             | 2.8262968 |\n",
            "|    std                | 4.56      |\n",
            "|    value_loss         | 20.2      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1828     |\n",
            "|    iterations         | 11700    |\n",
            "|    time_elapsed       | 31       |\n",
            "|    total_timesteps    | 58500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.92    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11699    |\n",
            "|    policy_loss        | 83       |\n",
            "|    reward             | 13.87311 |\n",
            "|    std                | 4.5      |\n",
            "|    value_loss         | 1.07e+03 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 1828      |\n",
            "|    iterations         | 11800     |\n",
            "|    time_elapsed       | 32        |\n",
            "|    total_timesteps    | 59000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -2.91     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 11799     |\n",
            "|    policy_loss        | 210       |\n",
            "|    reward             | 23.472563 |\n",
            "|    std                | 4.47      |\n",
            "|    value_loss         | 4.92e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1828     |\n",
            "|    iterations         | 11900    |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 59500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.91    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11899    |\n",
            "|    policy_loss        | 336      |\n",
            "|    reward             | 38.99826 |\n",
            "|    std                | 4.46     |\n",
            "|    value_loss         | 1.61e+04 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 1829     |\n",
            "|    iterations         | 12000    |\n",
            "|    time_elapsed       | 32       |\n",
            "|    total_timesteps    | 60000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -2.91    |\n",
            "|    explained_variance | 3.58e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 11999    |\n",
            "|    policy_loss        | 559      |\n",
            "|    reward             | 68.70722 |\n",
            "|    std                | 4.45     |\n",
            "|    value_loss         | 4.52e+04 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=60000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 3396, episode: 20\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 100173.33\n",
            "total_reward: 173.33\n",
            "total_cost: 15.07\n",
            "total_trades: 97\n",
            "Sharpe: 0.260\n",
            "=================================\n",
            "day: 3396, episode: 21\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 100000.00\n",
            "total_reward: 0.00\n",
            "total_cost: 0.00\n",
            "total_trades: 0\n",
            "=================================\n",
            "day: 3396, episode: 22\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 100000.00\n",
            "total_reward: 0.00\n",
            "total_cost: 0.00\n",
            "total_trades: 0\n",
            "=================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_ddpg \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ddpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_ddpg \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/td3/td3.py:194\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Delayed policy updates\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_delay \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Compute actor loss\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mq1_forward(replay_data\u001b[38;5;241m.\u001b[39mobservations, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    195\u001b[0m     actor_losses\u001b[38;5;241m.\u001b[39mappend(actor_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/td3/policies.py:78\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# assert deterministic, 'The TD3 actor only outputs deterministic actions'\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=100000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trained_ddpg' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrained_ddpg\u001b[49m\u001b[38;5;241m.\u001b[39msave(TRAINED_MODEL_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/agent_ddpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m if_using_ddpg \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trained_ddpg' is not defined"
          ]
        }
      ],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 1654      |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 1         |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | -65462.78 |\n",
            "----------------------------------\n",
            "day: 3396, episode: 24\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 316349.41\n",
            "total_reward: 216349.41\n",
            "total_cost: 8724.29\n",
            "total_trades: 3317\n",
            "Sharpe: 0.673\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1555          |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 2             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.1240128e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.89e+11      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -6.34e-06     |\n",
            "|    reward               | -24117.105    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.74e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1533          |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 4             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2573235e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.43e+12      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -4.63e-07     |\n",
            "|    reward               | -133404.3     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.76e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 25\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 270993.64\n",
            "total_reward: 170993.64\n",
            "total_cost: 8640.14\n",
            "total_trades: 3332\n",
            "Sharpe: 0.622\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1523         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.411007e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.3e+11      |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -1.58e-07    |\n",
            "|    reward               | -44160.13    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.12e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 26\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 302486.74\n",
            "total_reward: 202486.74\n",
            "total_cost: 8809.03\n",
            "total_trades: 3270\n",
            "Sharpe: 0.645\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1519          |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6717316e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -3.58e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.53e+12      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -6.26e-06     |\n",
            "|    reward               | -5943.05      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.04e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1518          |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 8             |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.1618478e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.95e+12      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -4e-07        |\n",
            "|    reward               | -70635.86     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.99e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 27\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 539392.57\n",
            "total_reward: 439392.57\n",
            "total_cost: 8863.45\n",
            "total_trades: 3373\n",
            "Sharpe: 0.878\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1518          |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 9             |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.5175194e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.15e+11      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -9.97e-06     |\n",
            "|    reward               | -23707.186    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.03e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1517         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.802132e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.93e+12     |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -9.75e-08    |\n",
            "|    reward               | -159911.34   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.73e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 28\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 546442.84\n",
            "total_reward: 446442.84\n",
            "total_cost: 8880.11\n",
            "total_trades: 3369\n",
            "Sharpe: 0.809\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1517          |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5401976e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 6.6e+11       |\n",
            "|    n_updates            | 80            |\n",
            "|    policy_gradient_loss | -3.82e-07     |\n",
            "|    reward               | -45287.06     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.26e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 29\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 601756.56\n",
            "total_reward: 501756.56\n",
            "total_cost: 8680.42\n",
            "total_trades: 3358\n",
            "Sharpe: 0.755\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7439244e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.47e+12      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -3.06e-07     |\n",
            "|    reward               | -7591.85      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.9e+12       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1514         |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.452886e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.69e+12     |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -6.61e-07    |\n",
            "|    reward               | -70079.6     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.97e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 30\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 375462.48\n",
            "total_reward: 275462.48\n",
            "total_cost: 9089.17\n",
            "total_trades: 3352\n",
            "Sharpe: 0.747\n",
            "=================================\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 1514           |\n",
            "|    iterations           | 12             |\n",
            "|    time_elapsed         | 16             |\n",
            "|    total_timesteps      | 24576          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.07771484e-07 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | 0              |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 2.04e+11       |\n",
            "|    n_updates            | 110            |\n",
            "|    policy_gradient_loss | -9.49e-06      |\n",
            "|    reward               | -33775.56      |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 4.37e+11       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6397174e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.46e+12      |\n",
            "|    n_updates            | 120           |\n",
            "|    policy_gradient_loss | -1.34e-06     |\n",
            "|    reward               | -138148.77    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.7e+12       |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 31\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 376981.10\n",
            "total_reward: 276981.10\n",
            "total_cost: 8800.03\n",
            "total_trades: 3318\n",
            "Sharpe: 0.696\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 18            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.9971277e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 6.77e+11      |\n",
            "|    n_updates            | 130           |\n",
            "|    policy_gradient_loss | -7.53e-06     |\n",
            "|    reward               | -51336.758    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.39e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 32\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 718304.96\n",
            "total_reward: 618304.96\n",
            "total_cost: 8084.20\n",
            "total_trades: 3376\n",
            "Sharpe: 0.788\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.897608e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.93e+12     |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -1.26e-06    |\n",
            "|    reward               | -10317.718   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.79e+12     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.947651e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.9e+12      |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -1.82e-06    |\n",
            "|    reward               | -75341.055   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.95e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 33\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 488965.98\n",
            "total_reward: 388965.98\n",
            "total_cost: 8866.37\n",
            "total_trades: 3352\n",
            "Sharpe: 0.763\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1190423e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.61e+11      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -1.01e-05     |\n",
            "|    reward               | -35393.047    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.74e+11      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1515        |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 4.48199e-09 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.15e+12    |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -1.33e-06   |\n",
            "|    reward               | -182665.44  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.67e+12    |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 34\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 595007.95\n",
            "total_reward: 495007.95\n",
            "total_cost: 8844.03\n",
            "total_trades: 3360\n",
            "Sharpe: 0.863\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 25            |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.7625584e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 7.29e+11      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -1.18e-06     |\n",
            "|    reward               | -61234.37     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.54e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 35\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 470545.52\n",
            "total_reward: 370545.52\n",
            "total_cost: 8909.36\n",
            "total_trades: 3362\n",
            "Sharpe: 0.781\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 27            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2892997e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.75e+12      |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -2.77e-06     |\n",
            "|    reward               | -11830.434    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.59e+12      |\n",
            "-------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 1517      |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 28        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.42     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 3.01e+12  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -2.16e-07 |\n",
            "|    reward               | -82108.11 |\n",
            "|    std                  | 1         |\n",
            "|    value_loss           | 6.93e+12  |\n",
            "---------------------------------------\n",
            "day: 3396, episode: 36\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 209065.94\n",
            "total_reward: 109065.94\n",
            "total_cost: 9100.69\n",
            "total_trades: 3353\n",
            "Sharpe: 0.645\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 29            |\n",
            "|    total_timesteps      | 45056         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0954758e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.66e+11      |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -1.87e-06     |\n",
            "|    reward               | -37489.715    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.15e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1517         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.265488e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.35e+12     |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -7.37e-07    |\n",
            "|    reward               | -228178.5    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.63e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 37\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 236311.51\n",
            "total_reward: 136311.51\n",
            "total_cost: 8815.29\n",
            "total_trades: 3306\n",
            "Sharpe: 0.675\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 24            |\n",
            "|    time_elapsed         | 32            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5716068e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 9.73e+11      |\n",
            "|    n_updates            | 230           |\n",
            "|    policy_gradient_loss | -1.27e-06     |\n",
            "|    reward               | -59258.543    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.83e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 38\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 236228.06\n",
            "total_reward: 136228.06\n",
            "total_cost: 8350.66\n",
            "total_trades: 3317\n",
            "Sharpe: 0.621\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 25            |\n",
            "|    time_elapsed         | 33            |\n",
            "|    total_timesteps      | 51200         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8044375e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.08e+12      |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | 9.41e-08      |\n",
            "|    reward               | -12291.349    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.26e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 53248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6647391e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.5e+12       |\n",
            "|    n_updates            | 250           |\n",
            "|    policy_gradient_loss | -6.97e-06     |\n",
            "|    reward               | -76087.74     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.91e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 39\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 386724.86\n",
            "total_reward: 286724.86\n",
            "total_cost: 8810.24\n",
            "total_trades: 3343\n",
            "Sharpe: 0.683\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.452886e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.05e+11     |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -4.39e-07    |\n",
            "|    reward               | -41563.87    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.6e+11      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 28            |\n",
            "|    time_elapsed         | 37            |\n",
            "|    total_timesteps      | 57344         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.1409082e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.91e+12      |\n",
            "|    n_updates            | 270           |\n",
            "|    policy_gradient_loss | -1.84e-07     |\n",
            "|    reward               | -237724.11    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.6e+12       |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 40\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 175472.76\n",
            "total_reward: 75472.76\n",
            "total_cost: 8491.03\n",
            "total_trades: 3306\n",
            "Sharpe: 0.494\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.537854e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.05e+12     |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -1.76e-06    |\n",
            "|    reward               | -58072.133   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.18e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 41\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 578257.15\n",
            "total_reward: 478257.15\n",
            "total_cost: 8907.65\n",
            "total_trades: 3372\n",
            "Sharpe: 0.835\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 30            |\n",
            "|    time_elapsed         | 40            |\n",
            "|    total_timesteps      | 61440         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2014214e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.12e+12      |\n",
            "|    n_updates            | 290           |\n",
            "|    policy_gradient_loss | -2.88e-07     |\n",
            "|    reward               | -13065.538    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.9e+12       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 31            |\n",
            "|    time_elapsed         | 41            |\n",
            "|    total_timesteps      | 63488         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5693014e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.52e+12      |\n",
            "|    n_updates            | 300           |\n",
            "|    policy_gradient_loss | -2.26e-07     |\n",
            "|    reward               | -80385.48     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.89e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 42\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 239859.00\n",
            "total_reward: 139859.00\n",
            "total_cost: 9028.56\n",
            "total_trades: 3357\n",
            "Sharpe: 0.563\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 32            |\n",
            "|    time_elapsed         | 43            |\n",
            "|    total_timesteps      | 65536         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.5011908e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.93e+11      |\n",
            "|    n_updates            | 310           |\n",
            "|    policy_gradient_loss | -8.22e-06     |\n",
            "|    reward               | -33883.88     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.06e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 33            |\n",
            "|    time_elapsed         | 44            |\n",
            "|    total_timesteps      | 67584         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3416866e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.31e+12      |\n",
            "|    n_updates            | 320           |\n",
            "|    policy_gradient_loss | -1.07e-06     |\n",
            "|    reward               | -270169.12    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.56e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 43\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 214661.86\n",
            "total_reward: 114661.86\n",
            "total_cost: 8642.14\n",
            "total_trades: 3298\n",
            "Sharpe: 0.695\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 34            |\n",
            "|    time_elapsed         | 46            |\n",
            "|    total_timesteps      | 69632         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.2864274e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.52e+12      |\n",
            "|    n_updates            | 330           |\n",
            "|    policy_gradient_loss | -1.72e-06     |\n",
            "|    reward               | -52666.08     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.61e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 44\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 554162.85\n",
            "total_reward: 454162.85\n",
            "total_cost: 8785.68\n",
            "total_trades: 3359\n",
            "Sharpe: 0.784\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 35            |\n",
            "|    time_elapsed         | 47            |\n",
            "|    total_timesteps      | 71680         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.4237822e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.38e+12      |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | -1.31e-05     |\n",
            "|    reward               | -16022.509    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.46e+12      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1511         |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.556526e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.73e+12     |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -1.31e-07    |\n",
            "|    reward               | -91410.336   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.87e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 45\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 367123.55\n",
            "total_reward: 267123.55\n",
            "total_cost: 8620.19\n",
            "total_trades: 3341\n",
            "Sharpe: 0.687\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 37            |\n",
            "|    time_elapsed         | 50            |\n",
            "|    total_timesteps      | 75776         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8958311e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.12e+11      |\n",
            "|    n_updates            | 360           |\n",
            "|    policy_gradient_loss | -6.2e-06      |\n",
            "|    reward               | -28377.982    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.58e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.947651e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.97e+12     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -1.91e-06    |\n",
            "|    reward               | -271150.47   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.51e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 46\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 263196.63\n",
            "total_reward: 163196.63\n",
            "total_cost: 8887.97\n",
            "total_trades: 3353\n",
            "Sharpe: 0.639\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 39            |\n",
            "|    time_elapsed         | 52            |\n",
            "|    total_timesteps      | 79872         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4260877e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.11e+12      |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -5.02e-06     |\n",
            "|    reward               | -54730.035    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.05e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 47\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 179705.94\n",
            "total_reward: 79705.94\n",
            "total_cost: 8396.72\n",
            "total_trades: 3308\n",
            "Sharpe: 0.600\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8894435e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.87e+12      |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -6.95e-07     |\n",
            "|    reward               | -16120.698    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.03e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 41            |\n",
            "|    time_elapsed         | 55            |\n",
            "|    total_timesteps      | 83968         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.7916623e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 4.14e+12      |\n",
            "|    n_updates            | 400           |\n",
            "|    policy_gradient_loss | -3.22e-07     |\n",
            "|    reward               | -109700.51    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.86e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 48\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 434704.50\n",
            "total_reward: 334704.50\n",
            "total_cost: 8458.56\n",
            "total_trades: 3358\n",
            "Sharpe: 0.655\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 56           |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.542503e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.73e+11     |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -5.66e-09    |\n",
            "|    reward               | -28326.146   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.22e+11     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 43            |\n",
            "|    time_elapsed         | 58            |\n",
            "|    total_timesteps      | 88064         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.6379788e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.43e+12      |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | -7.01e-07     |\n",
            "|    reward               | -270686.8     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.44e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 49\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 433314.76\n",
            "total_reward: 333314.76\n",
            "total_cost: 8888.71\n",
            "total_trades: 3366\n",
            "Sharpe: 0.777\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 44            |\n",
            "|    time_elapsed         | 59            |\n",
            "|    total_timesteps      | 90112         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8708095e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.2e+12       |\n",
            "|    n_updates            | 430           |\n",
            "|    policy_gradient_loss | -5.51e-07     |\n",
            "|    reward               | -43940.098    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.5e+12       |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 50\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 353626.38\n",
            "total_reward: 253626.38\n",
            "total_cost: 8536.77\n",
            "total_trades: 3325\n",
            "Sharpe: 0.677\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1511         |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.315531e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.47e+12     |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -9.03e-07    |\n",
            "|    reward               | -17521.922   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.57e+12     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1511         |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 62           |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.675247e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.2e+12      |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -1.44e-07    |\n",
            "|    reward               | -91398.8     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.84e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 51\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 519282.97\n",
            "total_reward: 419282.97\n",
            "total_cost: 8851.16\n",
            "total_trades: 3313\n",
            "Sharpe: 0.834\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1511         |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 63           |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.014023e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.67e+11     |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -3.76e-07    |\n",
            "|    reward               | -27083.852   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 7.96e+11     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 48            |\n",
            "|    time_elapsed         | 65            |\n",
            "|    total_timesteps      | 98304         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.7386704e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.36e+12      |\n",
            "|    n_updates            | 470           |\n",
            "|    policy_gradient_loss | -7.52e-06     |\n",
            "|    reward               | -307178.22    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.38e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 52\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 758680.14\n",
            "total_reward: 658680.14\n",
            "total_cost: 8899.17\n",
            "total_trades: 3368\n",
            "Sharpe: 0.855\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 49            |\n",
            "|    time_elapsed         | 66            |\n",
            "|    total_timesteps      | 100352        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.9849193e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.92e+12      |\n",
            "|    n_updates            | 480           |\n",
            "|    policy_gradient_loss | -1.96e-06     |\n",
            "|    reward               | -49554.785    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.05e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 53\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 444230.12\n",
            "total_reward: 344230.12\n",
            "total_cost: 8889.62\n",
            "total_trades: 3361\n",
            "Sharpe: 0.809\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 50            |\n",
            "|    time_elapsed         | 67            |\n",
            "|    total_timesteps      | 102400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9837475e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.25e+12      |\n",
            "|    n_updates            | 490           |\n",
            "|    policy_gradient_loss | -7.46e-06     |\n",
            "|    reward               | -19882.521    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.98e+12      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1513        |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 5.47152e-09 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.05e+12    |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -6.35e-07   |\n",
            "|    reward               | -81885.31   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.83e+12    |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 54\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 367812.87\n",
            "total_reward: 267812.87\n",
            "total_cost: 8856.50\n",
            "total_trades: 3347\n",
            "Sharpe: 0.751\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 52            |\n",
            "|    time_elapsed         | 70            |\n",
            "|    total_timesteps      | 106496        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5110937e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 4.69e+11      |\n",
            "|    n_updates            | 510           |\n",
            "|    policy_gradient_loss | -8.17e-07     |\n",
            "|    reward               | -31295.879    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.4e+11       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 53            |\n",
            "|    time_elapsed         | 71            |\n",
            "|    total_timesteps      | 108544        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.4633558e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.95e+12      |\n",
            "|    n_updates            | 520           |\n",
            "|    policy_gradient_loss | -4.61e-07     |\n",
            "|    reward               | -305601.47    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.34e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 55\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 364181.45\n",
            "total_reward: 264181.45\n",
            "total_cost: 8688.96\n",
            "total_trades: 3340\n",
            "Sharpe: 0.688\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1514         |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.452886e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.4e+12      |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -5.52e-07    |\n",
            "|    reward               | -45240.895   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.64e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 56\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 541248.79\n",
            "total_reward: 441248.79\n",
            "total_cost: 8526.13\n",
            "total_trades: 3345\n",
            "Sharpe: 0.822\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 55            |\n",
            "|    time_elapsed         | 74            |\n",
            "|    total_timesteps      | 112640        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.0594175e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.88e+12      |\n",
            "|    n_updates            | 540           |\n",
            "|    policy_gradient_loss | -4.31e-06     |\n",
            "|    reward               | -21002.375    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.36e+12      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1514         |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 75           |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.887138e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.72e+12     |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -7.73e-07    |\n",
            "|    reward               | -98221.31    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.82e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 57\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 294256.83\n",
            "total_reward: 194256.83\n",
            "total_cost: 8637.09\n",
            "total_trades: 3308\n",
            "Sharpe: 0.697\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1514         |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 116736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.344635e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.55e+11     |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -1.42e-06    |\n",
            "|    reward               | -35366.902   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.89e+11     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 58            |\n",
            "|    time_elapsed         | 78            |\n",
            "|    total_timesteps      | 118784        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1612428e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.79e+12      |\n",
            "|    n_updates            | 570           |\n",
            "|    policy_gradient_loss | -6.99e-07     |\n",
            "|    reward               | -333369.44    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.29e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 58\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 328190.70\n",
            "total_reward: 228190.70\n",
            "total_cost: 8806.18\n",
            "total_trades: 3282\n",
            "Sharpe: 0.691\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 59            |\n",
            "|    time_elapsed         | 79            |\n",
            "|    total_timesteps      | 120832        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.9744495e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.55e+12      |\n",
            "|    n_updates            | 580           |\n",
            "|    policy_gradient_loss | -1.63e-06     |\n",
            "|    reward               | -50015.938    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.39e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 59\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 290228.01\n",
            "total_reward: 190228.01\n",
            "total_cost: 8558.40\n",
            "total_trades: 3332\n",
            "Sharpe: 0.623\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1514        |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 4.48199e-09 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.2e+12     |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -1.89e-06   |\n",
            "|    reward               | -21597.586  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 1.57e+12    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1514        |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 82          |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 4.10364e-09 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.62e+12    |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -1.25e-06   |\n",
            "|    reward               | -98442.086  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.8e+12     |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 60\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 691614.60\n",
            "total_reward: 591614.60\n",
            "total_cost: 8503.78\n",
            "total_trades: 3345\n",
            "Sharpe: 0.830\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 62            |\n",
            "|    time_elapsed         | 83            |\n",
            "|    total_timesteps      | 126976        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5308615e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 4.98e+11      |\n",
            "|    n_updates            | 610           |\n",
            "|    policy_gradient_loss | -2.72e-06     |\n",
            "|    reward               | -34589.81     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 9.48e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1514          |\n",
            "|    iterations           | 63            |\n",
            "|    time_elapsed         | 85            |\n",
            "|    total_timesteps      | 129024        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.2759576e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.05e+12      |\n",
            "|    n_updates            | 620           |\n",
            "|    policy_gradient_loss | -1.72e-06     |\n",
            "|    reward               | -354298.44    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.23e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 61\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 242958.41\n",
            "total_reward: 142958.41\n",
            "total_cost: 8804.93\n",
            "total_trades: 3348\n",
            "Sharpe: 0.766\n",
            "=================================\n",
            "--------------------------------------------\n",
            "| time/                   |                |\n",
            "|    fps                  | 1514           |\n",
            "|    iterations           | 64             |\n",
            "|    time_elapsed         | 86             |\n",
            "|    total_timesteps      | 131072         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.15833245e-08 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.2            |\n",
            "|    entropy_loss         | -1.42          |\n",
            "|    explained_variance   | -1.19e-07      |\n",
            "|    learning_rate        | 0.00025        |\n",
            "|    loss                 | 2.56e+12       |\n",
            "|    n_updates            | 630            |\n",
            "|    policy_gradient_loss | -4.11e-06      |\n",
            "|    reward               | -52473.098     |\n",
            "|    std                  | 1              |\n",
            "|    value_loss           | 6.16e+12       |\n",
            "--------------------------------------------\n",
            "day: 3396, episode: 62\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 605430.27\n",
            "total_reward: 505430.27\n",
            "total_cost: 8704.14\n",
            "total_trades: 3362\n",
            "Sharpe: 0.880\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 1514       |\n",
            "|    iterations           | 65         |\n",
            "|    time_elapsed         | 87         |\n",
            "|    total_timesteps      | 133120     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0        |\n",
            "|    clip_fraction        | 0          |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.42      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 2.99e+11   |\n",
            "|    n_updates            | 640        |\n",
            "|    policy_gradient_loss | -1.15e-06  |\n",
            "|    reward               | -22206.434 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 8.36e+11   |\n",
            "----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 66            |\n",
            "|    time_elapsed         | 89            |\n",
            "|    total_timesteps      | 135168        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.7520887e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.81e+12      |\n",
            "|    n_updates            | 650           |\n",
            "|    policy_gradient_loss | -2.75e-06     |\n",
            "|    reward               | -102360.29    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.78e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 63\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 544700.64\n",
            "total_reward: 444700.64\n",
            "total_cost: 8988.72\n",
            "total_trades: 3364\n",
            "Sharpe: 0.810\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 90           |\n",
            "|    total_timesteps      | 137216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.467476e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.55e+11     |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | -1.31e-05    |\n",
            "|    reward               | -36647.1     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.01e+12     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 68            |\n",
            "|    time_elapsed         | 91            |\n",
            "|    total_timesteps      | 139264        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0622898e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.51e+12      |\n",
            "|    n_updates            | 670           |\n",
            "|    policy_gradient_loss | -2.87e-06     |\n",
            "|    reward               | -278634.38    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.17e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 64\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 437364.14\n",
            "total_reward: 337364.14\n",
            "total_cost: 8903.15\n",
            "total_trades: 3317\n",
            "Sharpe: 0.741\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 69            |\n",
            "|    time_elapsed         | 93            |\n",
            "|    total_timesteps      | 141312        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5320332e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.16e+12      |\n",
            "|    n_updates            | 680           |\n",
            "|    policy_gradient_loss | -1.36e-07     |\n",
            "|    reward               | -61385.113    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.85e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 65\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 503623.78\n",
            "total_reward: 403623.78\n",
            "total_cost: 9018.87\n",
            "total_trades: 3375\n",
            "Sharpe: 0.837\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 70            |\n",
            "|    time_elapsed         | 94            |\n",
            "|    total_timesteps      | 143360        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8708095e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.99e+11      |\n",
            "|    n_updates            | 690           |\n",
            "|    policy_gradient_loss | -2.84e-06     |\n",
            "|    reward               | -25564.672    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.86e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 95           |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.929017e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.86e+12     |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -4.08e-07    |\n",
            "|    reward               | -130885.32   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.77e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 66\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 279335.95\n",
            "total_reward: 179335.95\n",
            "total_cost: 8864.14\n",
            "total_trades: 3328\n",
            "Sharpe: 0.617\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 97           |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.043127e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 5.26e+11     |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -3.02e-06    |\n",
            "|    reward               | -42722.42    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.09e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 67\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 330201.03\n",
            "total_reward: 230201.03\n",
            "total_cost: 8573.95\n",
            "total_trades: 3342\n",
            "Sharpe: 0.588\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 73            |\n",
            "|    time_elapsed         | 98            |\n",
            "|    total_timesteps      | 149504        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0739313e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.33e+12      |\n",
            "|    n_updates            | 720           |\n",
            "|    policy_gradient_loss | -3.53e-06     |\n",
            "|    reward               | -5590.584     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.07e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 74            |\n",
            "|    time_elapsed         | 99            |\n",
            "|    total_timesteps      | 151552        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0745363e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.1e+12       |\n",
            "|    n_updates            | 730           |\n",
            "|    policy_gradient_loss | -6.58e-07     |\n",
            "|    reward               | -67904.31     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.99e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 68\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 527026.42\n",
            "total_reward: 427026.42\n",
            "total_cost: 8814.78\n",
            "total_trades: 3345\n",
            "Sharpe: 0.789\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 75            |\n",
            "|    time_elapsed         | 101           |\n",
            "|    total_timesteps      | 153600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.5402356e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.12e+11      |\n",
            "|    n_updates            | 740           |\n",
            "|    policy_gradient_loss | -9.24e-06     |\n",
            "|    reward               | -23706.777    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.95e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 76            |\n",
            "|    time_elapsed         | 102           |\n",
            "|    total_timesteps      | 155648        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2677933e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.87e+12      |\n",
            "|    n_updates            | 750           |\n",
            "|    policy_gradient_loss | -9.35e-07     |\n",
            "|    reward               | -159334.28    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.74e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 69\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 683634.79\n",
            "total_reward: 583634.79\n",
            "total_cost: 8703.24\n",
            "total_trades: 3359\n",
            "Sharpe: 0.820\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 77            |\n",
            "|    time_elapsed         | 104           |\n",
            "|    total_timesteps      | 157696        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8812792e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 6.01e+11      |\n",
            "|    n_updates            | 760           |\n",
            "|    policy_gradient_loss | -6.56e-07     |\n",
            "|    reward               | -46420.547    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.22e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 70\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 554723.27\n",
            "total_reward: 454723.27\n",
            "total_cost: 8604.35\n",
            "total_trades: 3339\n",
            "Sharpe: 0.752\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 105          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.683411e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.81e+12     |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -8e-07       |\n",
            "|    reward               | -8225.49     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.94e+12     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 79            |\n",
            "|    time_elapsed         | 106           |\n",
            "|    total_timesteps      | 161792        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6857167e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.58e+12      |\n",
            "|    n_updates            | 780           |\n",
            "|    policy_gradient_loss | -5.64e-07     |\n",
            "|    reward               | -70026.35     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.97e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 71\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 407736.66\n",
            "total_reward: 307736.66\n",
            "total_cost: 8852.61\n",
            "total_trades: 3349\n",
            "Sharpe: 0.730\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 80            |\n",
            "|    time_elapsed         | 108           |\n",
            "|    total_timesteps      | 163840        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2014214e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.06e+11      |\n",
            "|    n_updates            | 790           |\n",
            "|    policy_gradient_loss | -3.34e-07     |\n",
            "|    reward               | -30609.096    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.29e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 81            |\n",
            "|    time_elapsed         | 109           |\n",
            "|    total_timesteps      | 165888        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0768417e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.43e+12      |\n",
            "|    n_updates            | 800           |\n",
            "|    policy_gradient_loss | -3.27e-06     |\n",
            "|    reward               | -122552.69    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.71e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 72\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 348841.88\n",
            "total_reward: 248841.88\n",
            "total_cost: 8774.56\n",
            "total_trades: 3342\n",
            "Sharpe: 0.630\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 82            |\n",
            "|    time_elapsed         | 110           |\n",
            "|    total_timesteps      | 167936        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.1118044e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 6.89e+11      |\n",
            "|    n_updates            | 810           |\n",
            "|    policy_gradient_loss | -1.6e-06      |\n",
            "|    reward               | -53335.45     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.36e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 73\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 454437.13\n",
            "total_reward: 354437.13\n",
            "total_cost: 8976.35\n",
            "total_trades: 3353\n",
            "Sharpe: 0.869\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 112          |\n",
            "|    total_timesteps      | 169984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.654307e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.36e+12     |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -1.59e-07    |\n",
            "|    reward               | -9104.774    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.82e+12     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 113          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.731149e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.26e+12     |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -2.84e-06    |\n",
            "|    reward               | -79175.08    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.96e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 74\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 176831.87\n",
            "total_reward: 76831.87\n",
            "total_cost: 8695.26\n",
            "total_trades: 3304\n",
            "Sharpe: 0.526\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 85            |\n",
            "|    time_elapsed         | 114           |\n",
            "|    total_timesteps      | 174080        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2700912e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.25e+11      |\n",
            "|    n_updates            | 840           |\n",
            "|    policy_gradient_loss | -1.14e-05     |\n",
            "|    reward               | -35463.875    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.64e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 116           |\n",
            "|    total_timesteps      | 176128        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4185283e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.89e+12      |\n",
            "|    n_updates            | 850           |\n",
            "|    policy_gradient_loss | -7.08e-06     |\n",
            "|    reward               | -169983.47    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.68e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 75\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 304312.95\n",
            "total_reward: 204312.95\n",
            "total_cost: 8718.29\n",
            "total_trades: 3329\n",
            "Sharpe: 0.712\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 117          |\n",
            "|    total_timesteps      | 178176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.150773e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 7.86e+11     |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -4.54e-06    |\n",
            "|    reward               | -54409.707   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.49e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 76\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 640242.21\n",
            "total_reward: 540242.21\n",
            "total_cost: 9004.06\n",
            "total_trades: 3366\n",
            "Sharpe: 0.797\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.947651e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.11e+12     |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | -4.97e-07    |\n",
            "|    reward               | -11501.426   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.65e+12     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 120          |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.897608e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.73e+12     |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -7.7e-08     |\n",
            "|    reward               | -84904.78    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.94e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 77\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 396814.60\n",
            "total_reward: 296814.60\n",
            "total_cost: 9199.26\n",
            "total_trades: 3347\n",
            "Sharpe: 0.736\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.278263e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.28e+11     |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -9.7e-08     |\n",
            "|    reward               | -36757.438   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.03e+11     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 122          |\n",
            "|    total_timesteps      | 186368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.151378e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.77e+12     |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -1.38e-06    |\n",
            "|    reward               | -250022.03   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.64e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 78\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 639501.77\n",
            "total_reward: 539501.77\n",
            "total_cost: 8791.39\n",
            "total_trades: 3346\n",
            "Sharpe: 0.790\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 92            |\n",
            "|    time_elapsed         | 124           |\n",
            "|    total_timesteps      | 188416        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2386895e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 8.91e+11      |\n",
            "|    n_updates            | 910           |\n",
            "|    policy_gradient_loss | -1.26e-06     |\n",
            "|    reward               | -58374.754    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.73e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 79\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 293710.06\n",
            "total_reward: 193710.06\n",
            "total_cost: 8982.08\n",
            "total_trades: 3343\n",
            "Sharpe: 0.637\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 125          |\n",
            "|    total_timesteps      | 190464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.876668e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.1e+12      |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -1.45e-06    |\n",
            "|    reward               | -12167.216   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.35e+12     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 94            |\n",
            "|    time_elapsed         | 126           |\n",
            "|    total_timesteps      | 192512        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2992026e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.85e+12      |\n",
            "|    n_updates            | 930           |\n",
            "|    policy_gradient_loss | -4.57e-07     |\n",
            "|    reward               | -85772.94     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.92e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 80\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 451372.66\n",
            "total_reward: 351372.66\n",
            "total_cost: 8885.40\n",
            "total_trades: 3358\n",
            "Sharpe: 0.755\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 95            |\n",
            "|    time_elapsed         | 128           |\n",
            "|    total_timesteps      | 194560        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1234079e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.63e+11      |\n",
            "|    n_updates            | 940           |\n",
            "|    policy_gradient_loss | -2.32e-06     |\n",
            "|    reward               | -41159.117    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.48e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 129          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.895302e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.71e+12     |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -1.78e-06    |\n",
            "|    reward               | -218868.27   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.61e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 81\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 296964.29\n",
            "total_reward: 196964.29\n",
            "total_cost: 8512.86\n",
            "total_trades: 3282\n",
            "Sharpe: 0.649\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 97            |\n",
            "|    time_elapsed         | 130           |\n",
            "|    total_timesteps      | 198656        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0983862e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.07e+12      |\n",
            "|    n_updates            | 960           |\n",
            "|    policy_gradient_loss | -5.02e-06     |\n",
            "|    reward               | -59229.332    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.09e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 82\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 603571.51\n",
            "total_reward: 503571.51\n",
            "total_cost: 8781.51\n",
            "total_trades: 3358\n",
            "Sharpe: 0.822\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 98            |\n",
            "|    time_elapsed         | 132           |\n",
            "|    total_timesteps      | 200704        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.6670826e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.33e+12      |\n",
            "|    n_updates            | 970           |\n",
            "|    policy_gradient_loss | -4.29e-07     |\n",
            "|    reward               | -12424.701    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5e+12         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 99            |\n",
            "|    time_elapsed         | 133           |\n",
            "|    total_timesteps      | 202752        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.0640665e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.47e+12      |\n",
            "|    n_updates            | 980           |\n",
            "|    policy_gradient_loss | -1.8e-06      |\n",
            "|    reward               | -83479.93     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.89e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 83\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 326614.03\n",
            "total_reward: 226614.03\n",
            "total_cost: 8799.34\n",
            "total_trades: 3341\n",
            "Sharpe: 0.720\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 100           |\n",
            "|    time_elapsed         | 135           |\n",
            "|    total_timesteps      | 204800        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.8178557e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.73e+11      |\n",
            "|    n_updates            | 990           |\n",
            "|    policy_gradient_loss | -8.93e-06     |\n",
            "|    reward               | -35155.31     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.94e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 101           |\n",
            "|    time_elapsed         | 136           |\n",
            "|    total_timesteps      | 206848        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4348188e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.08e+12      |\n",
            "|    n_updates            | 1000          |\n",
            "|    policy_gradient_loss | -5.65e-06     |\n",
            "|    reward               | -259389.11    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.57e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 84\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 466769.41\n",
            "total_reward: 366769.41\n",
            "total_cost: 8402.94\n",
            "total_trades: 3348\n",
            "Sharpe: 0.764\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 102           |\n",
            "|    time_elapsed         | 137           |\n",
            "|    total_timesteps      | 208896        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.1304385e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -2.38e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.06e+12      |\n",
            "|    n_updates            | 1010          |\n",
            "|    policy_gradient_loss | -1.53e-06     |\n",
            "|    reward               | -52168.438    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.48e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 85\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 863852.97\n",
            "total_reward: 763852.97\n",
            "total_cost: 8918.71\n",
            "total_trades: 3370\n",
            "Sharpe: 0.930\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1516          |\n",
            "|    iterations           | 103           |\n",
            "|    time_elapsed         | 139           |\n",
            "|    total_timesteps      | 210944        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2014214e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.12e+12      |\n",
            "|    n_updates            | 1020          |\n",
            "|    policy_gradient_loss | -8.4e-07      |\n",
            "|    reward               | -16713.41     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.58e+12      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1516         |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.566996e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.91e+12     |\n",
            "|    n_updates            | 1030         |\n",
            "|    policy_gradient_loss | -2.81e-06    |\n",
            "|    reward               | -93127.4     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.87e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 86\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 204570.28\n",
            "total_reward: 104570.28\n",
            "total_cost: 8558.33\n",
            "total_trades: 3314\n",
            "Sharpe: 0.793\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 105           |\n",
            "|    time_elapsed         | 141           |\n",
            "|    total_timesteps      | 215040        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.9546437e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.46e+11      |\n",
            "|    n_updates            | 1040          |\n",
            "|    policy_gradient_loss | -6.02e-06     |\n",
            "|    reward               | -27682.271    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.44e+11      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1515          |\n",
            "|    iterations           | 106           |\n",
            "|    time_elapsed         | 143           |\n",
            "|    total_timesteps      | 217088        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1723175e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.8e+12       |\n",
            "|    n_updates            | 1050          |\n",
            "|    policy_gradient_loss | -8.5e-07      |\n",
            "|    reward               | -242227.05    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.52e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 87\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 251482.39\n",
            "total_reward: 151482.39\n",
            "total_cost: 8733.60\n",
            "total_trades: 3317\n",
            "Sharpe: 0.611\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1515         |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 144          |\n",
            "|    total_timesteps      | 219136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.731149e-11 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.24e+12     |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | -3.2e-08     |\n",
            "|    reward               | -54156.87    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 2.93e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 88\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 443673.15\n",
            "total_reward: 343673.15\n",
            "total_cost: 8690.92\n",
            "total_trades: 3333\n",
            "Sharpe: 0.692\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1513         |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 146          |\n",
            "|    total_timesteps      | 221184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.646143e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.86e+12     |\n",
            "|    n_updates            | 1070         |\n",
            "|    policy_gradient_loss | -4.55e-07    |\n",
            "|    reward               | -15418.567   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 4.15e+12     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1513          |\n",
            "|    iterations           | 109           |\n",
            "|    time_elapsed         | 147           |\n",
            "|    total_timesteps      | 223232        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.2200554e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.59e+12      |\n",
            "|    n_updates            | 1080          |\n",
            "|    policy_gradient_loss | -3.69e-07     |\n",
            "|    reward               | -105378.81    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.86e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 89\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 539928.07\n",
            "total_reward: 439928.07\n",
            "total_cost: 8897.79\n",
            "total_trades: 3351\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 110           |\n",
            "|    time_elapsed         | 148           |\n",
            "|    total_timesteps      | 225280        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.2677933e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.27e+11      |\n",
            "|    n_updates            | 1090          |\n",
            "|    policy_gradient_loss | -2.17e-06     |\n",
            "|    reward               | -26831.27     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.02e+11      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1512        |\n",
            "|    iterations           | 111         |\n",
            "|    time_elapsed         | 150         |\n",
            "|    total_timesteps      | 227328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 8.58563e-09 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.32e+12    |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | -2.32e-06   |\n",
            "|    reward               | -249598.86  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 6.46e+12    |\n",
            "-----------------------------------------\n",
            "day: 3396, episode: 90\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 391542.14\n",
            "total_reward: 291542.14\n",
            "total_cost: 8757.73\n",
            "total_trades: 3343\n",
            "Sharpe: 0.659\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 151          |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.754394e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.66e+12     |\n",
            "|    n_updates            | 1110         |\n",
            "|    policy_gradient_loss | 9.83e-09     |\n",
            "|    reward               | -46380.57    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.38e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 91\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 411728.79\n",
            "total_reward: 311728.79\n",
            "total_cost: 8537.12\n",
            "total_trades: 3321\n",
            "Sharpe: 0.692\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 153          |\n",
            "|    total_timesteps      | 231424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.812602e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.08e+12     |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -3.27e-07    |\n",
            "|    reward               | -16624.582   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 3.7e+12      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 114           |\n",
            "|    time_elapsed         | 154           |\n",
            "|    total_timesteps      | 233472        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6123522e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.15e+12      |\n",
            "|    n_updates            | 1130          |\n",
            "|    policy_gradient_loss | -4.28e-06     |\n",
            "|    reward               | -107256.03    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.84e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 92\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 408639.45\n",
            "total_reward: 308639.45\n",
            "total_cost: 8937.23\n",
            "total_trades: 3344\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1511          |\n",
            "|    iterations           | 115           |\n",
            "|    time_elapsed         | 155           |\n",
            "|    total_timesteps      | 235520        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3242243e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 4.24e+11      |\n",
            "|    n_updates            | 1140          |\n",
            "|    policy_gradient_loss | -1.79e-06     |\n",
            "|    reward               | -25511.742    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 7.78e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1512         |\n",
            "|    iterations           | 116          |\n",
            "|    time_elapsed         | 157          |\n",
            "|    total_timesteps      | 237568       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.818461e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 3.09e+12     |\n",
            "|    n_updates            | 1150         |\n",
            "|    policy_gradient_loss | -6.41e-07    |\n",
            "|    reward               | -301288.7    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.39e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 93\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 176935.58\n",
            "total_reward: 76935.58\n",
            "total_cost: 8527.12\n",
            "total_trades: 3330\n",
            "Sharpe: 0.408\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1512          |\n",
            "|    iterations           | 117           |\n",
            "|    time_elapsed         | 158           |\n",
            "|    total_timesteps      | 239616        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3911631e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.84e+12      |\n",
            "|    n_updates            | 1160          |\n",
            "|    policy_gradient_loss | -3.34e-06     |\n",
            "|    reward               | -51495.58     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.89e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 94\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 502580.85\n",
            "total_reward: 402580.85\n",
            "total_cost: 8905.40\n",
            "total_trades: 3368\n",
            "Sharpe: 0.757\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1511          |\n",
            "|    iterations           | 118           |\n",
            "|    time_elapsed         | 159           |\n",
            "|    total_timesteps      | 241664        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0454324e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.19e+12      |\n",
            "|    n_updates            | 1170          |\n",
            "|    policy_gradient_loss | -1.54e-06     |\n",
            "|    reward               | -19435.486    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 3.14e+12      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1510         |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 161          |\n",
            "|    total_timesteps      | 243712       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.947651e-10 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.98e+12     |\n",
            "|    n_updates            | 1180         |\n",
            "|    policy_gradient_loss | 3.37e-08     |\n",
            "|    reward               | -73756.58    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 6.83e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 95\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 652537.25\n",
            "total_reward: 552537.25\n",
            "total_cost: 8819.49\n",
            "total_trades: 3359\n",
            "Sharpe: 0.803\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1510          |\n",
            "|    iterations           | 120           |\n",
            "|    time_elapsed         | 162           |\n",
            "|    total_timesteps      | 245760        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7491402e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 4.51e+11      |\n",
            "|    n_updates            | 1190          |\n",
            "|    policy_gradient_loss | -2.43e-06     |\n",
            "|    reward               | -31809.02     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 8.3e+11       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1510          |\n",
            "|    iterations           | 121           |\n",
            "|    time_elapsed         | 164           |\n",
            "|    total_timesteps      | 247808        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5110937e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.28e+12      |\n",
            "|    n_updates            | 1200          |\n",
            "|    policy_gradient_loss | 3e-08         |\n",
            "|    reward               | -302469.78    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.35e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 96\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 297868.40\n",
            "total_reward: 197868.40\n",
            "total_cost: 8954.68\n",
            "total_trades: 3336\n",
            "Sharpe: 0.647\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1510          |\n",
            "|    iterations           | 122           |\n",
            "|    time_elapsed         | 165           |\n",
            "|    total_timesteps      | 249856        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1787051e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.95e+12      |\n",
            "|    n_updates            | 1210          |\n",
            "|    policy_gradient_loss | -1.14e-06     |\n",
            "|    reward               | -45975.9      |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 4.48e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 97\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 705683.03\n",
            "total_reward: 605683.03\n",
            "total_cost: 8830.53\n",
            "total_trades: 3350\n",
            "Sharpe: 0.901\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1509          |\n",
            "|    iterations           | 123           |\n",
            "|    time_elapsed         | 166           |\n",
            "|    total_timesteps      | 251904        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.0349627e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 1.14e+12      |\n",
            "|    n_updates            | 1220          |\n",
            "|    policy_gradient_loss | -2.46e-07     |\n",
            "|    reward               | -21356.475    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 2.54e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1509          |\n",
            "|    iterations           | 124           |\n",
            "|    time_elapsed         | 168           |\n",
            "|    total_timesteps      | 253952        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5693014e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.22e+12      |\n",
            "|    n_updates            | 1230          |\n",
            "|    policy_gradient_loss | -5.13e-07     |\n",
            "|    reward               | -92238.57     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.82e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 98\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 804999.84\n",
            "total_reward: 704999.84\n",
            "total_cost: 8526.48\n",
            "total_trades: 3347\n",
            "Sharpe: 0.835\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1508         |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 169          |\n",
            "|    total_timesteps      | 256000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.858034e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.56e+11     |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -1.33e-06    |\n",
            "|    reward               | -33363.88    |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 8.74e+11     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1507          |\n",
            "|    iterations           | 126           |\n",
            "|    time_elapsed         | 171           |\n",
            "|    total_timesteps      | 258048        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7520506e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.39e+12      |\n",
            "|    n_updates            | 1250          |\n",
            "|    policy_gradient_loss | -5.41e-06     |\n",
            "|    reward               | -359814.75    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.3e+12       |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 99\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 457223.12\n",
            "total_reward: 357223.12\n",
            "total_cost: 8505.20\n",
            "total_trades: 3305\n",
            "Sharpe: 0.801\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1507         |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 172          |\n",
            "|    total_timesteps      | 260096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.313226e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 2.73e+12     |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -1.7e-06     |\n",
            "|    reward               | -51631.1     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 5.17e+12     |\n",
            "------------------------------------------\n",
            "day: 3396, episode: 100\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 633181.98\n",
            "total_reward: 533181.98\n",
            "total_cost: 8824.78\n",
            "total_trades: 3360\n",
            "Sharpe: 0.796\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1507          |\n",
            "|    iterations           | 128           |\n",
            "|    time_elapsed         | 173           |\n",
            "|    total_timesteps      | 262144        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.0931703e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 9.78e+11      |\n",
            "|    n_updates            | 1270          |\n",
            "|    policy_gradient_loss | -4.02e-07     |\n",
            "|    reward               | -20848.568    |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 1.77e+12      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1507          |\n",
            "|    iterations           | 129           |\n",
            "|    time_elapsed         | 175           |\n",
            "|    total_timesteps      | 264192        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0454324e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 3.34e+12      |\n",
            "|    n_updates            | 1280          |\n",
            "|    policy_gradient_loss | -3.36e-07     |\n",
            "|    reward               | -90498.88     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.81e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 101\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 214037.23\n",
            "total_reward: 114037.23\n",
            "total_cost: 8687.23\n",
            "total_trades: 3315\n",
            "Sharpe: 0.546\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1507         |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 176          |\n",
            "|    total_timesteps      | 266240       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.198511e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 4.4e+11      |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | -4.51e-06    |\n",
            "|    reward               | -35035.164   |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 9.33e+11     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1506          |\n",
            "|    iterations           | 131           |\n",
            "|    time_elapsed         | 178           |\n",
            "|    total_timesteps      | 268288        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.3766112e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.57e+12      |\n",
            "|    n_updates            | 1300          |\n",
            "|    policy_gradient_loss | -3.8e-06      |\n",
            "|    reward               | -305612.6     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 6.25e+12      |\n",
            "-------------------------------------------\n",
            "day: 3396, episode: 102\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 271392.55\n",
            "total_reward: 171392.55\n",
            "total_cost: 8888.05\n",
            "total_trades: 3324\n",
            "Sharpe: 0.593\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 1506          |\n",
            "|    iterations           | 132           |\n",
            "|    time_elapsed         | 179           |\n",
            "|    total_timesteps      | 270336        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2165401e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 2.85e+12      |\n",
            "|    n_updates            | 1310          |\n",
            "|    policy_gradient_loss | -3.65e-06     |\n",
            "|    reward               | -52885.36     |\n",
            "|    std                  | 1             |\n",
            "|    value_loss           | 5.96e+12      |\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_ppo \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_ppo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/policies.py:655\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 655\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    657\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:183\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Reparametrization trick to pass gradients\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/normal.py:73\u001b[0m, in \u001b[0;36mNormal.rsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrsample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mSize()):\n\u001b[0;32m---> 73\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     eps \u001b[38;5;241m=\u001b[39m _standard_normal(shape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/distribution.py:268\u001b[0m, in \u001b[0;36mDistribution._extended_shape\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample_shape, torch\u001b[38;5;241m.\u001b[39mSize):\n\u001b[1;32m    267\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_shape\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=300000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 221       |\n",
            "|    time_elapsed    | 61        |\n",
            "|    total_timesteps | 13588     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 157       |\n",
            "|    critic_loss     | 6.18      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 13487     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "day: 3396, episode: 140\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 3142438.00\n",
            "total_reward: 3042438.00\n",
            "total_cost: 99.90\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 216       |\n",
            "|    time_elapsed    | 125       |\n",
            "|    total_timesteps | 27176     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 107       |\n",
            "|    critic_loss     | 40        |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 27075     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 217       |\n",
            "|    time_elapsed    | 187       |\n",
            "|    total_timesteps | 40764     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 76.5      |\n",
            "|    critic_loss     | 16.1      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 40663     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 214       |\n",
            "|    time_elapsed    | 253       |\n",
            "|    total_timesteps | 54352     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 52.5      |\n",
            "|    critic_loss     | 15.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 54251     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "day: 3396, episode: 150\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 3142438.00\n",
            "total_reward: 3042438.00\n",
            "total_cost: 99.90\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 218       |\n",
            "|    time_elapsed    | 311       |\n",
            "|    total_timesteps | 67940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 36.8      |\n",
            "|    critic_loss     | 14.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 67839     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=80000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 3396, episode: 160\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 3142438.00\n",
            "total_reward: 3042438.00\n",
            "total_cost: 99.90\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 191       |\n",
            "|    time_elapsed    | 70        |\n",
            "|    total_timesteps | 13588     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 482       |\n",
            "|    critic_loss     | 912       |\n",
            "|    ent_coef        | 0.385     |\n",
            "|    ent_coef_loss   | 30.4      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 13487     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 197       |\n",
            "|    time_elapsed    | 137       |\n",
            "|    total_timesteps | 27176     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.74e+03  |\n",
            "|    critic_loss     | 195       |\n",
            "|    ent_coef        | 1.5       |\n",
            "|    ent_coef_loss   | -12.9     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 27075     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 198       |\n",
            "|    time_elapsed    | 205       |\n",
            "|    total_timesteps | 40764     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 6.63e+03  |\n",
            "|    critic_loss     | 3.21e+05  |\n",
            "|    ent_coef        | 5.83      |\n",
            "|    ent_coef_loss   | -56.3     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 40663     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n",
            "day: 3396, episode: 170\n",
            "begin_total_asset: 100000.00\n",
            "end_total_asset: 3142438.00\n",
            "total_reward: 3042438.00\n",
            "total_cost: 99.90\n",
            "total_trades: 3396\n",
            "Sharpe: 1.064\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 199       |\n",
            "|    time_elapsed    | 272       |\n",
            "|    total_timesteps | 54352     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 2.59e+04  |\n",
            "|    critic_loss     | 2.17e+03  |\n",
            "|    ent_coef        | 22.7      |\n",
            "|    ent_coef_loss   | -99.6     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 54251     |\n",
            "|    reward          | -5.769105 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=60000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
